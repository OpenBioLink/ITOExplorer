<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="275e569f-5b47-4a62-bcc4-1fe4e2c19a0b" class="plotly-graph-div" style="height:750.0px; width:1500px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("275e569f-5b47-4a62-bcc4-1fe4e2c19a0b")) {                    Plotly.newPlot(                        "275e569f-5b47-4a62-bcc4-1fe4e2c19a0b",                        [{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Computer code processing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Computer code processing","orientation":"v","showlegend":true,"x":["2017-08","2018-03","2018-04","2018-10","2019-10"],"xaxis":"x","y":["Computer code processing","Computer code processing","Computer code processing","Computer code processing","Computer code processing"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Dialog process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Dialog process","orientation":"v","showlegend":true,"x":["2017-09","2017-11","2018-05","2018-09","2018-10","2019-02","2019-04"],"xaxis":"x","y":["Dialog process","Dialog process","Dialog process","Dialog process","Dialog process","Dialog process","Dialog process"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Inference and reasoning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Inference and reasoning","orientation":"v","showlegend":true,"x":["2014-08","2015-08","2016-09","2017-02","2017-09","2017-11","2017-12","2018-04","2018-05","2018-06","2018-09","2018-10","2019-01","2019-02","2019-06","2019-07","2019-09"],"xaxis":"x","y":["Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Information extraction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Information extraction","orientation":"v","showlegend":true,"x":["2016-01","2017-07","2017-09","2018-07","2018-08","2018-09","2018-10","2018-12","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-11","2020-03","2020-04"],"xaxis":"x","y":["Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Information retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Information retrieval","orientation":"v","showlegend":true,"x":["2018-03","2019-01","2019-04","2019-11"],"xaxis":"x","y":["Information retrieval","Information retrieval","Information retrieval","Information retrieval"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Machine translation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Machine translation","orientation":"v","showlegend":true,"x":["2018-10","2019-01","2019-02","2019-05"],"xaxis":"x","y":["Machine translation","Machine translation","Machine translation","Machine translation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Natural language generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Natural language generation","orientation":"v","showlegend":true,"x":["2014-09","2014-10","2014-12","2015-09","2016-02","2016-03","2016-06","2016-07","2016-08","2016-09","2016-10","2016-11","2016-12","2017-01","2017-05","2017-06","2017-09","2017-11","2018-02","2018-03","2018-06","2018-07","2018-08","2018-09","2018-10","2019-01","2019-04","2019-05","2019-06","2019-09","2019-10"],"xaxis":"x","y":["Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other NLP task","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other NLP task","orientation":"v","showlegend":true,"x":["2016-12","2017-10","2017-11","2019-01","2019-03","2019-04","2019-09"],"xaxis":"x","y":["Other NLP task","Other NLP task","Other NLP task","Other NLP task","Other NLP task","Other NLP task","Other NLP task"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pragmatics analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pragmatics analysis","orientation":"v","showlegend":true,"x":["2013-10","2014-06","2014-08","2015-02","2015-06","2015-11","2016-02","2016-06","2016-07","2016-09","2017-02","2017-04","2017-07","2017-08","2017-09","2017-12","2018-01","2018-02","2018-04","2018-05","2018-06","2018-07","2018-10","2018-11","2019-01","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-12"],"xaxis":"x","y":["Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Question answering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Question answering","orientation":"v","showlegend":true,"x":["2014-06","2014-12","2015-06","2015-11","2016-02","2016-03","2016-05","2016-06","2016-08","2016-09","2016-10","2016-11","2016-12","2017-03","2017-04","2017-05","2017-06","2017-07","2017-08","2017-10","2017-12","2018-01","2018-03","2018-04","2018-05","2018-06","2018-07","2018-08","2018-09","2018-10","2018-11","2019-01","2019-02","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2020-02"],"xaxis":"x","y":["Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic analysis","orientation":"v","showlegend":true,"x":["2016-03","2016-06","2017-04","2017-05","2017-09","2017-12","2018-02","2018-03","2018-05","2018-10","2018-11","2019-05","2019-06","2019-07","2019-09"],"xaxis":"x","y":["Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Sentence embedding","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Sentence embedding","orientation":"v","showlegend":true,"x":["2017-07","2018-07"],"xaxis":"x","y":["Sentence embedding","Sentence embedding"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Syntactic analysis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Syntactic analysis","orientation":"v","showlegend":true,"x":["2016-03","2016-07","2016-11","2017-04","2017-07","2018-05","2018-07","2018-08","2018-10","2018-11","2019-03","2019-04","2019-06","2019-09"],"xaxis":"x","y":["Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Text classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Text classification","orientation":"v","showlegend":true,"x":["2015-11","2016-03","2016-06","2016-09","2016-11","2017-02","2017-10","2018-01","2018-02","2018-03","2018-05","2018-07","2018-08","2018-09","2018-10","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-08","2019-09","2020-02"],"xaxis":"x","y":["Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Text summarization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Text summarization","orientation":"v","showlegend":true,"x":["2017-05","2018-08","2019-03","2019-05","2019-08","2019-10"],"xaxis":"x","y":["Text summarization","Text summarization","Text summarization","Text summarization","Text summarization","Text summarization"],"yaxis":"y","type":"scatter"},{"hovertemplate":["<BR>task: Computer code processing<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: 100 sleep nights of 8 caregivers - Code Generation benchmarking - 14 gestures accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: Android Repos - Code Generation benchmarking - Perplexity<BR>","<BR>task: Computer code processing<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Code Generation: CoNaLa - Code Generation benchmarking - BLEU<BR>  Code Generation: CoNaLa-Ext - Code Generation benchmarking - BLEU<BR>","<BR>task: Dialog process<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Dialog process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Area<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Food<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Price<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>","<BR>task: Dialog process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - Parameters<BR>","<BR>task: Inference and reasoning<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>","<BR>task: Inference and reasoning<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind dev - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>","<BR>task: Inference and reasoning<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>","<BR>task: Inference and reasoning<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - EM<BR>  Common Sense Reasoning: ReCoRD - Common Sense Reasoning benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: Quora Question Pairs - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A2<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A3<BR>  Natural Language Inference: WNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - 1 in 10 R-at-5<BR>  Common Sense Reasoning: Visual Dialog  v0.9 - Common Sense Reasoning benchmarking - Recall-at-10<BR>","<BR>task: Information extraction<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Information extraction<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Information extraction<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1 (surface form)<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Wikipedia-Wikidata relations - Relation Extraction benchmarking - Error rate<BR>","<BR>task: Information extraction<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>","<BR>task: Information extraction<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: OntoNotes 4 - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Weibo NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: CoNLL 2000 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>","<BR>task: Information extraction<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: SighanNER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: JNLPBA - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: JNLPBA - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WLPC - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: WetLab - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: WLPC - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - F1<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Precision<BR>  Relation Extraction: FewRel - Relation Extraction benchmarking - Recall<BR>","<BR>task: Information extraction<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>","<BR>task: Information extraction<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: LINNAEUS - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: Species-800 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: Code-Switching English-Spanish NER - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ontontoes chinese v5 - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Relation Extraction: NYT24 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT29 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Precision<BR>  Named Entity Recognition: SoSciSoCi - Named Entity Recognition benchmarking - Recall<BR>","<BR>task: Information retrieval<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Information retrieval<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Information retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-10<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R-at-1<BR>  Conversational Response Selection: Advising Corpus - Conversational Response Selection benchmarking - R@50<BR>","<BR>task: Information retrieval<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI OpenSubtitles - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Machine translation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Machine translation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>","<BR>task: Natural language generation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - Accuracy<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2015 English-Russian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Natural language generation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: Text8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Natural language generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 Thai-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Czech-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Russian - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Russian-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: Hutter Prize - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Natural language generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Validation perplexity<BR>","<BR>task: Natural language generation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>","<BR>task: Natural language generation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - KL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - NLL<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Natural language generation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>  Text Summarization: Pubmed - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: arXiv - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Natural language generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: 20NEWS - Machine Translation benchmarking - 1-of-100 Accuracy<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Estonian-Russian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: ACCURAT balanced test corpus for under resourced languages Russian-Estonian - Machine Translation benchmarking - BLEU<BR>  Text Generation: LDC2016E25 - Text Generation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: LAMBADA - Language Modelling benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - SacreBLEU<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - SacreBLEU<BR>  Question Generation: Visual Question Generation - Question Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-1<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-2<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-3<BR>  Text Generation: DailyDialog - Text Generation benchmarking - BLEU-4<BR>","<BR>task: Natural language generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 French-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT 2017 English-Latvian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Estonian - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 English-Finnish - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Estonian-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: The Pile - Language Modelling benchmarking - Bits per byte<BR>","<BR>task: Natural language generation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2016 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2017 Finnish-English - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2019 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2019 English-German - Machine Translation benchmarking - SacreBLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>","<BR>task: Natural language generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Machine Translation: IWSLT2017 Arabic-English - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-Arabic - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 English-French - Machine Translation benchmarking - Cased sacreBLEU<BR>  Machine Translation: IWSLT2017 French-English - Machine Translation benchmarking - Cased sacreBLEU<BR>","<BR>task: Natural language generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Language Modelling: PTB - Language Modelling benchmarking - PPL<BR>  Machine Translation: IWSLT2015 Chinese-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Other NLP task<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Other NLP task<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Other NLP task<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Other NLP task<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - LPIPS<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Real<BR>","<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Average<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Yelp Binary classification - Sentiment Analysis benchmarking - Error<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Sogou News - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: SemEval - Sentiment Analysis benchmarking - F1-score<BR>  Sentiment Analysis: SemEval 2017 Task 4-A - Sentiment Analysis benchmarking - Average Recall<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: CR - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: 2017_test set - Paraphrase Identification benchmarking - 10 fold Cross validation<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Intent Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Sentiment Analysis: Twitter - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - F1<BR>  Sentiment Analysis: ChnSentiCorp - Sentiment Analysis benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Bias (F/M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Feminine F1 (F)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Masculine F1 (M)<BR>  Coreference Resolution: GAP - Coreference Resolution benchmarking - Overall F1<BR>  Sentiment Analysis: ASTD - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: ArSAS - Sentiment Analysis benchmarking - Average Recall<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - MSE<BR>  Sentiment Analysis: FiQA - Sentiment Analysis benchmarking - R^2<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Financial PhraseBank - Sentiment Analysis benchmarking - F1 score<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Reverb - Question Answering benchmarking - Accuracy<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Question answering<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>","<BR>task: Question answering<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>","<BR>task: Question answering<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: SimpleQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>","<BR>task: Question answering<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: MCTest-160 - Question Answering benchmarking - Accuracy<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Question answering<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>","<BR>task: Question answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Question answering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>","<BR>task: Question answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: COMPLEXQUESTIONS - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>","<BR>task: Question answering<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>","<BR>task: Question answering<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Question answering<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>","<BR>task: Question answering<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Question answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: QuAC - Question Answering benchmarking - F1<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQD<BR>  Question Answering: QuAC - Question Answering benchmarking - HEQQ<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: JD Product Question Answer - Question Answering benchmarking - BLEU<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Long)<BR>  Question Answering: Natural Questions - Question Answering benchmarking - F1 (Short)<BR>","<BR>task: Question answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: CODAH - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Question answering<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: HotpotQA - Question Answering benchmarking - JOINT-F1<BR>","<BR>task: Question answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: NaturalQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>","<BR>task: Question answering<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Question Answering: SCDE - Question Answering benchmarking - BA<BR>  Question Answering: SCDE - Question Answering benchmarking - DE<BR>  Question Answering: SCDE - Question Answering benchmarking - PA<BR>","<BR>task: Semantic analysis<BR>date: 2013-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>","<BR>task: Semantic analysis<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>","<BR>task: Semantic analysis<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Semantic analysis<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - All<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Knowledge-based: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>","<BR>task: Semantic analysis<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>","<BR>task: Semantic analysis<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>","<BR>task: Semantic analysis<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: Geo - Semantic Parsing benchmarking - Accuracy<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: spider - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>","<BR>task: Semantic analysis<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Semantic Parsing: WikiSQL - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Sentence embedding<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Syntactic analysis<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Syntactic analysis<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Syntactic analysis<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>","<BR>task: Syntactic analysis<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: CoNLL-2009 - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Syntactic analysis<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>","<BR>task: Syntactic analysis<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Dependency Parsing: GENIA - LAS - Dependency Parsing benchmarking - F1<BR>  Dependency Parsing: GENIA - UAS - Dependency Parsing benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>","<BR>task: Syntactic analysis<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Grammatical Error Detection: JFLEG - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Syntactic analysis<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2014-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>","<BR>task: Text classification<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters De-En - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters En-De - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>  Text Classification: DBpedia - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: RCV1 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: TREC-50 - Text Classification benchmarking - Error<BR>","<BR>task: Text classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: WOS-11967 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-46985 - Document Classification benchmarking - Accuracy<BR>  Document Classification: WOS-5736 - Document Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: LOCAL DATASET - Text Classification benchmarking - Accuracy (%)<BR>","<BR>task: Text classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>","<BR>task: Text classification<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Sentence Classification: PubMed 20k RCT - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-5 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Citation Intent Classification: SciCite - Citation Intent Classification benchmarking - F1<BR>  Sentence Classification: Paper Field - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: AAPD - Document Classification benchmarking - F1<BR>  Document Classification: Amazon - Document Classification benchmarking - Accuracy<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Classic - Document Classification benchmarking - Accuracy<BR>  Document Classification: Recipe - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>  Document Classification: Twitter - Document Classification benchmarking - Accuracy<BR>  Document Classification: Yelp-14 - Document Classification benchmarking - Accuracy<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (10 classes)<BR>","<BR>task: Text classification<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: IMDb-M - Document Classification benchmarking - Accuracy<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - P-at-5<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-1<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-3<BR>  Text Classification: RCV1 - Text Classification benchmarking - nDCG-at-5<BR>","<BR>task: Text classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Document Classification: MPQA - Document Classification benchmarking - Accuracy<BR>  Text Classification: RCV1 - Text Classification benchmarking - Macro F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>","<BR>task: Text classification<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Precision<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Recall<BR>","<BR>task: Text summarization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Text summarization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Text summarization<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>"],"marker":{"line":{"width":1,"color":"black"},"size":21,"symbol":42},"mode":"markers","x":["2016-03","2017-05","2017-08","2018-05","2018-10","2016-03","2016-05","2016-06","2017-04","2014-04","2015-08","2017-12","2018-03","2018-05","2018-06","2018-08","2018-09","2018-10","2018-11","2018-12","2019-04","2019-05","2019-06","2019-09","2014-06","2014-09","2014-10","2016-08","2017-06","2017-09","2018-04","2018-05","2018-06","2018-08","2018-09","2018-10","2019-01","2019-03","2019-04","2019-05","2019-06","2019-08","2019-09","2019-11","2020-03","2018-02","2018-12","2019-01","2019-04","2018-04","2019-01","2013-12","2014-06","2014-09","2015-08","2015-09","2015-12","2016-02","2016-06","2016-07","2016-09","2016-11","2016-12","2017-02","2017-04","2017-09","2017-11","2018-03","2018-05","2018-07","2018-08","2018-09","2018-10","2019-01","2019-02","2019-06","2019-07","2019-08","2019-10","2019-11","2016-10","2016-12","2017-10","2017-11","2013-10","2014-12","2015-05","2015-09","2016-04","2016-07","2017-02","2017-04","2017-07","2017-12","2018-03","2018-06","2018-12","2019-01","2019-02","2019-03","2019-06","2019-08","2019-09","2019-12","2014-04","2014-05","2014-12","2015-03","2015-06","2015-11","2016-02","2016-03","2016-06","2016-08","2016-11","2016-12","2017-03","2017-04","2017-05","2017-07","2017-08","2017-10","2017-11","2018-03","2018-05","2018-08","2018-10","2019-01","2019-02","2019-04","2019-05","2019-07","2019-08","2020-04","2013-10","2014-11","2015-05","2016-01","2016-03","2017-04","2017-05","2017-07","2017-09","2018-01","2018-02","2018-03","2018-04","2018-05","2018-09","2019-09","2019-10","2015-09","2014-12","2015-06","2016-07","2016-08","2016-11","2017-11","2018-08","2018-10","2018-11","2019-01","2013-06","2014-03","2014-05","2014-10","2015-04","2015-09","2016-02","2016-07","2016-12","2017-07","2017-09","2018-01","2018-05","2018-06","2018-08","2018-09","2018-10","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-08","2019-11","2016-02","2017-05","2017-09"],"y":["Computer code processing","Computer code processing","Computer code processing","Computer code processing","Computer code processing","Dialog process","Dialog process","Dialog process","Dialog process","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Inference and reasoning","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information extraction","Information retrieval","Information retrieval","Information retrieval","Information retrieval","Machine translation","Machine translation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Natural language generation","Other NLP task","Other NLP task","Other NLP task","Other NLP task","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Pragmatics analysis","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Question answering","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Semantic analysis","Sentence embedding","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Syntactic analysis","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text classification","Text summarization","Text summarization","Text summarization"],"type":"scatter","line":{"color":"black","width":0}},{"hovertemplate":["<BR>task: Pragmatics analysis<BR>date: 2013-10<BR>ratio: 0.5551<BR>benchmarks:<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: MSRP - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2014-06<BR>ratio: 0.76<BR>benchmarks:<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-06<BR>ratio: 0.3305<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2014-08<BR>ratio: 0.2549<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2014-08<BR>ratio: 0.2308<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2014-09<BR>ratio: 0.0886<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2014-10<BR>ratio: 0.0904<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2014-12<BR>ratio: 0.3032<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Natural language generation<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - PPL<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-02<BR>ratio: 0.1186<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-06<BR>ratio: 0.0427<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2015-06<BR>ratio: 0.232<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: WebQuestions - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2015-08<BR>ratio: 0.5<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Train Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2015-09<BR>ratio: 0.1435<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Question answering<BR>date: 2015-11<BR>ratio: 0.2159<BR>benchmarks:<BR>  Question Answering: QASent - Question Answering benchmarking - MAP<BR>  Question Answering: QASent - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>","<BR>task: Pragmatics analysis<BR>date: 2015-11<BR>ratio: 0.4025<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Text classification<BR>date: 2015-11<BR>ratio: 0.25<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Information extraction<BR>date: 2016-01<BR>ratio: 0.483<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-02<BR>ratio: 0.3491<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2016-02<BR>ratio: 0.3173<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Question answering<BR>date: 2016-02<BR>ratio: 0.0718<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>","<BR>task: Natural language generation<BR>date: 2016-03<BR>ratio: 0.2<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-03<BR>ratio: 0.40459999999999996<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: MCTest-500 - Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-03<BR>ratio: 0.5215<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2016-03<BR>ratio: 0.1323<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2016-03<BR>ratio: 0.1966<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>","<BR>task: Question answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-06<BR>ratio: 0.7941<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2016-06<BR>ratio: 0.3097<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - Daily Mail<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 10k)<BR>  Question Answering: bAbi - Question Answering benchmarking - Accuracy (trained on 1k)<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-06<BR>ratio: 0.0702<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2016-06<BR>ratio: 0.0678<BR>benchmarks:<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2016-06<BR>ratio: 0.30955<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-07<BR>ratio: 0.094<BR>benchmarks:<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2016-07<BR>ratio: 0.9481<BR>benchmarks:<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Natural language generation<BR>date: 2016-07<BR>ratio: 0.218<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Natural language generation<BR>date: 2016-08<BR>ratio: 0.1882<BR>benchmarks:<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-08<BR>ratio: 0.3589<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2016-09<BR>ratio: 0.36160000000000003<BR>benchmarks:<BR>  Language Modelling: enwik8 - Language Modelling benchmarking - Bit per Character (BPC)<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Inference and reasoning<BR>date: 2016-09<BR>ratio: 0.098<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Text classification<BR>date: 2016-09<BR>ratio: 0.3558<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2016-09<BR>ratio: 0.101<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2016-09<BR>ratio: 0.0286<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2016-10<BR>ratio: 0.8<BR>benchmarks:<BR>  Machine Translation: WMT2015 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-10<BR>ratio: 0.6069<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: bAbi - Question Answering benchmarking - Mean Error Rate<BR>","<BR>task: Natural language generation<BR>date: 2016-11<BR>ratio: 0.0632<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2016-11<BR>ratio: 0.29685<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Syntactic analysis<BR>date: 2016-11<BR>ratio: 0.385075<BR>benchmarks:<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - LAS<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - UAS<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Text classification<BR>date: 2016-11<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2016-12<BR>ratio: 0.6667<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Test perplexity<BR>  Language Modelling: WikiText-2 - Language Modelling benchmarking - Test perplexity<BR>","<BR>task: Question answering<BR>date: 2016-12<BR>ratio: 0.7801<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Other NLP task<BR>date: 2016-12<BR>ratio: 0.0708<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Natural language generation<BR>date: 2017-01<BR>ratio: 0.0597<BR>benchmarks:<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-02<BR>ratio: 0.1937<BR>benchmarks:<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>","<BR>task: Inference and reasoning<BR>date: 2017-02<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Text classification<BR>date: 2017-02<BR>ratio: 0.2857<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Question answering<BR>date: 2017-03<BR>ratio: 0.033<BR>benchmarks:<BR>  Question Answering: CNN / Daily Mail - Question Answering benchmarking - CNN<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2017-04<BR>ratio: 0.55<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-04<BR>ratio: 0.06<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Syntactic analysis<BR>date: 2017-04<BR>ratio: 0.6016<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-04<BR>ratio: 0.14375<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2017-05<BR>ratio: 0.29195<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: Chinese Poems - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Text summarization<BR>date: 2017-05<BR>ratio: 0.2511<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Question answering<BR>date: 2017-05<BR>ratio: 0.2379<BR>benchmarks:<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Semantic analysis<BR>date: 2017-05<BR>ratio: 0.921<BR>benchmarks:<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: TAC2010 - Entity Disambiguation benchmarking - Micro Precision<BR>","<BR>task: Natural language generation<BR>date: 2017-06<BR>ratio: 0.42015<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: IWSLT2015 English-German - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Question answering<BR>date: 2017-06<BR>ratio: 0.1524<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2017-07<BR>ratio: 0.30775<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>  Grammatical Error Detection: CoNLL-2014 A1 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: CoNLL-2014 A2 - Grammatical Error Detection benchmarking - F0.5<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Question answering<BR>date: 2017-07<BR>ratio: 0.22825<BR>benchmarks:<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - MAP<BR>  Question Answering: SemEvalCQA - Question Answering benchmarking - P-at-1<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - MRR<BR>  Question Answering: YahooCQA - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Information extraction<BR>date: 2017-07<BR>ratio: 0.212<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>","<BR>task: Sentence embedding<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - CR<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-07<BR>ratio: 0.57325<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Amazon Review Full - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Amazon Review Polarity - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-08<BR>ratio: 0.2288<BR>benchmarks:<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2017-08<BR>ratio: 0.4269<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Question answering<BR>date: 2017-08<BR>ratio: 0.4228<BR>benchmarks:<BR>  Question Answering: AI2 Kaggle Dataset - Question Answering benchmarking - P-at-1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-09<BR>ratio: 0.3099<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2017-09<BR>ratio: 0.0407<BR>benchmarks:<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>","<BR>task: Inference and reasoning<BR>date: 2017-09<BR>ratio: 0.0196<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2017-09<BR>ratio: 0.6226<BR>benchmarks:<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-2<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-3<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-4<BR>  Text Generation: COCO Captions - Text Generation benchmarking - BLEU-5<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-2<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-3<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-4<BR>  Text Generation: EMNLP2017 WMT - Text Generation benchmarking - BLEU-5<BR>","<BR>task: Information extraction<BR>date: 2017-09<BR>ratio: 0.4385<BR>benchmarks:<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Dialog process<BR>date: 2017-09<BR>ratio: 0.56715<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Question answering<BR>date: 2017-10<BR>ratio: 0.6272<BR>benchmarks:<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Other NLP task<BR>date: 2017-10<BR>ratio: 0.7021<BR>benchmarks:<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - FID<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Oxford 102 Flowers - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Text classification<BR>date: 2017-10<BR>ratio: 0.0798<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2017-11<BR>ratio: 0.25695<BR>benchmarks:<BR>  Dialog Act Classification: Switchboard corpus - Dialog Act Classification benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2017-11<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Other NLP task<BR>date: 2017-11<BR>ratio: 0.6346<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Natural language generation<BR>date: 2017-11<BR>ratio: 0.0459<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Question answering<BR>date: 2017-12<BR>ratio: 0.0571<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2017-12<BR>ratio: 0.0392<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2017-12<BR>ratio: 0.1887<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2017-12<BR>ratio: 0.54575<BR>benchmarks:<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Discuss)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Unrelated)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Weighted Accuracy<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2018-01<BR>ratio: 0.4672<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-01<BR>ratio: 0.0809<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-02<BR>ratio: 0.1176<BR>benchmarks:<BR>  Citation Intent Classification: ACL-ARC - Citation Intent Classification benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2018-02<BR>ratio: 0.6357<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-02<BR>ratio: 0.2873<BR>benchmarks:<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-02<BR>ratio: 0.19115<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Natural language generation<BR>date: 2018-03<BR>ratio: 0.5076<BR>benchmarks:<BR>  Language Modelling: WikiText-103 - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Text classification<BR>date: 2018-03<BR>ratio: 0.0357<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>","<BR>task: Information retrieval<BR>date: 2018-03<BR>ratio: 0.541<BR>benchmarks:<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2018-03<BR>ratio: 0.2009<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-E<BR>  Semantic Textual Similarity: SentEval - Semantic Textual Similarity benchmarking - SICK-R<BR>","<BR>task: Question answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-04<BR>ratio: 0.5921<BR>benchmarks:<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-04<BR>ratio: 0.2342<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Electronics<BR>","<BR>task: Computer code processing<BR>date: 2018-04<BR>ratio: 0.137<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-04<BR>ratio: 0.0412<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-05<BR>ratio: 0.2896<BR>benchmarks:<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MR - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-5 Fine-grained classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2018-05<BR>ratio: 0.57845<BR>benchmarks:<BR>  Common Sense Reasoning: Event2Mind test - Common Sense Reasoning benchmarking - Average Cross-Ent<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Dialog process<BR>date: 2018-05<BR>ratio: 0.8365<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Request<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Request<BR>","<BR>task: Question answering<BR>date: 2018-05<BR>ratio: 0.36539999999999995<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Text classification<BR>date: 2018-05<BR>ratio: 0.3299<BR>benchmarks:<BR>  Text Classification: TREC-6 - Text Classification benchmarking - Error<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-05<BR>ratio: 0.11170000000000001<BR>benchmarks:<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Syntactic analysis<BR>date: 2018-05<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Inference and reasoning<BR>date: 2018-06<BR>ratio: 0.6132<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: V-SNLI - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>","<BR>task: Natural language generation<BR>date: 2018-06<BR>ratio: 0.11065<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>","<BR>task: Question answering<BR>date: 2018-06<BR>ratio: 0.3862<BR>benchmarks:<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: Story Cloze Test - Question Answering benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Question Answering: Quasart-T - Question Answering benchmarking - EM<BR>","<BR>task: Syntactic analysis<BR>date: 2018-07<BR>ratio: 0.791<BR>benchmarks:<BR>  Dependency Parsing: Penn Treebank - Dependency Parsing benchmarking - POS<BR>","<BR>task: Natural language generation<BR>date: 2018-07<BR>ratio: 0.0553<BR>benchmarks:<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>","<BR>task: Sentence embedding<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Sentence Compression: Google Dataset - Sentence Compression benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2018-07<BR>ratio: 0.3122<BR>benchmarks:<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-07<BR>ratio: 0.5132<BR>benchmarks:<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-07<BR>ratio: 0.0282<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-08<BR>ratio: 0.5092<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>  Text Classification: AG News - Text Classification benchmarking - Error<BR>","<BR>task: Syntactic analysis<BR>date: 2018-08<BR>ratio: 0.4922<BR>benchmarks:<BR>  Chunking: CoNLL 2000 - Chunking benchmarking - Exact Span F1<BR>  Chunking: Penn Treebank - Chunking benchmarking - F1 score<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>","<BR>task: Information extraction<BR>date: 2018-08<BR>ratio: 0.72315<BR>benchmarks:<BR>  Named Entity Recognition: Long-tail emerging entities - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>","<BR>task: Text summarization<BR>date: 2018-08<BR>ratio: 0.4058<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - PPL<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Natural language generation<BR>date: 2018-08<BR>ratio: 0.1946<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-French - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-German - Machine Translation benchmarking - BLEU score<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>","<BR>task: Question answering<BR>date: 2018-08<BR>ratio: 0.2718<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2018-09<BR>ratio: 0.2353<BR>benchmarks:<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Text classification<BR>date: 2018-09<BR>ratio: 0.652<BR>benchmarks:<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2018-09<BR>ratio: 0.2419<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Natural language generation<BR>date: 2018-09<BR>ratio: 0.53495<BR>benchmarks:<BR>  Language Modelling: One Billion Word - Language Modelling benchmarking - Validation perplexity<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>","<BR>task: Information extraction<BR>date: 2018-09<BR>ratio: 0.3185<BR>benchmarks:<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Dialog process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Question answering<BR>date: 2018-10<BR>ratio: 0.196<BR>benchmarks:<BR>  Question Answering: CoQA - Question Answering benchmarking - In-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Out-of-domain<BR>  Question Answering: CoQA - Question Answering benchmarking - Overall<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - EM<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2018-10<BR>ratio: 0.9057<BR>benchmarks:<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Semantic analysis<BR>date: 2018-10<BR>ratio: 0.43885<BR>benchmarks:<BR>  Semantic Parsing: ATIS - Semantic Parsing benchmarking - Accuracy<BR>  Semantic Role Labeling: CoNLL 2005 - Semantic Role Labeling benchmarking - F1<BR>  Semantic Role Labeling: OntoNotes - Semantic Role Labeling benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2018-10<BR>ratio: 0.2123<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>","<BR>task: Information extraction<BR>date: 2018-10<BR>ratio: 0.20085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-10<BR>ratio: 0.33775<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Books<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - DVD<BR>  Sentiment Analysis: Multi-Domain Sentiment Dataset - Sentiment Analysis benchmarking - Kitchen<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Computer code processing<BR>date: 2018-10<BR>ratio: 0.5481<BR>benchmarks:<BR>  Code Generation: Django - Code Generation benchmarking - Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>","<BR>task: Dialog process<BR>date: 2018-10<BR>ratio: 0.327<BR>benchmarks:<BR>  Dialog State Tracking: Second dialogue state tracking challenge - Dialog State Tracking benchmarking - Joint<BR>  Dialog State Tracking: Wizard-of-Oz - Dialog State Tracking benchmarking - Joint<BR>","<BR>task: Natural language generation<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Machine Translation: WMT 2017 Latvian-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Machine translation<BR>date: 2018-10<BR>ratio: 0.15<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Inference and reasoning<BR>date: 2018-10<BR>ratio: 0.48510000000000003<BR>benchmarks:<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Dev<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2018-11<BR>ratio: 0.2719<BR>benchmarks:<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>","<BR>task: Question answering<BR>date: 2018-11<BR>ratio: 0.1449<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - BLEU-1<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NewsQA - Question Answering benchmarking - EM<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2018-11<BR>ratio: 0.2698<BR>benchmarks:<BR>  Grammatical Error Detection: FCE - Grammatical Error Detection benchmarking - F0.5<BR>","<BR>task: Pragmatics analysis<BR>date: 2018-11<BR>ratio: 0.35325<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Agree)<BR>  Fake News Detection: FNC-1 - Fake News Detection benchmarking - Per-class Accuracy (Disagree)<BR>","<BR>task: Information extraction<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-10%<BR>  Relation Extraction: NYT Corpus - Relation Extraction benchmarking - P-at-30%<BR>","<BR>task: Text classification<BR>date: 2019-01<BR>ratio: 0.6137<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>","<BR>task: Information retrieval<BR>date: 2019-01<BR>ratio: 0.3558<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-01<BR>ratio: 0.37765000000000004<BR>benchmarks:<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: Yelp Fine-grained classification - Sentiment Analysis benchmarking - Error<BR>","<BR>task: Inference and reasoning<BR>date: 2019-01<BR>ratio: 0.3456<BR>benchmarks:<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>  Natural Language Inference: SciTail - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI French - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-01<BR>ratio: 0.2041<BR>benchmarks:<BR>  Question Answering: MS MARCO - Question Answering benchmarking - Rouge-L<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-1<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - BLEU-4<BR>  Question Answering: NarrativeQA - Question Answering benchmarking - Rouge-L<BR>  Question Answering: WikiHop - Question Answering benchmarking - Test<BR>","<BR>task: Natural language generation<BR>date: 2019-01<BR>ratio: 0.47159999999999996<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2017 English-Chinese - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2014 English-Czech - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 Romanian-English - Machine Translation benchmarking - BLEU score<BR>  Text Generation: Yahoo Questions - Text Generation benchmarking - Perplexity<BR>","<BR>task: Other NLP task<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Machine translation<BR>date: 2019-01<BR>ratio: 0.4127<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Information extraction<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Named Entity Recognition: NCBI-disease - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2019-02<BR>ratio: 0.3333<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Dialog process<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Inference and reasoning<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Common Sense Reasoning: Winograd Schema Challenge - Common Sense Reasoning benchmarking - Score<BR>","<BR>task: Machine translation<BR>date: 2019-02<BR>ratio: 0.397<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Text classification<BR>date: 2019-02<BR>ratio: 0.3546<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - Accuracy<BR>  Text Classification: Ohsumed - Text Classification benchmarking - Accuracy<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Question answering<BR>date: 2019-02<BR>ratio: 0.6327<BR>benchmarks:<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-CN<BR>  Question Answering: Children's Book Test - Question Answering benchmarking - Accuracy-NE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Text classification<BR>date: 2019-03<BR>ratio: 0.6981<BR>benchmarks:<BR>  Sentence Classification: ACL-ARC - Sentence Classification benchmarking - F1<BR>  Sentence Classification: SciCite - Sentence Classification benchmarking - F1<BR>  Sentence Classification: ScienceCite - Sentence Classification benchmarking - F1<BR>","<BR>task: Other NLP task<BR>date: 2019-03<BR>ratio: 0.1016<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>","<BR>task: Information extraction<BR>date: 2019-03<BR>ratio: 0.833<BR>benchmarks:<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: SciERC - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: ChemProt - Relation Extraction benchmarking - F1<BR>  Relation Extraction: SciERC - Relation Extraction benchmarking - F1<BR>","<BR>task: Text summarization<BR>date: 2019-03<BR>ratio: 0.3031<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Syntactic analysis<BR>date: 2019-03<BR>ratio: 0.1343<BR>benchmarks:<BR>  Constituency Parsing: Penn Treebank - Constituency Parsing benchmarking - F1 score<BR>","<BR>task: Other NLP task<BR>date: 2019-04<BR>ratio: 0.5876<BR>benchmarks:<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: COCO - Text-to-Image Generation benchmarking - SOA-C<BR>  Text-to-Image Generation: CUB - Text-to-Image Generation benchmarking - Inception score<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - Acc<BR>","<BR>task: Question answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-04<BR>ratio: 0.4341666666666666<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>","<BR>task: Syntactic analysis<BR>date: 2019-04<BR>ratio: 0.1967<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>","<BR>task: Information retrieval<BR>date: 2019-04<BR>ratio: 0.2915<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Natural language generation<BR>date: 2019-04<BR>ratio: 0.5763<BR>benchmarks:<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: DUC 2004 Task 1 - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Text classification<BR>date: 2019-04<BR>ratio: 0.0123<BR>benchmarks:<BR>  Document Classification: Cora - Document Classification benchmarking - Accuracy<BR>","<BR>task: Dialog process<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Question answering<BR>date: 2019-05<BR>ratio: 0.45885<BR>benchmarks:<BR>  Question Answering: TrecQA - Question Answering benchmarking - MAP<BR>  Question Answering: TrecQA - Question Answering benchmarking - MRR<BR>  Question Answering: WikiQA - Question Answering benchmarking - MAP<BR>  Question Answering: WikiQA - Question Answering benchmarking - MRR<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Text classification<BR>date: 2019-05<BR>ratio: 0.4549<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: Sogou News - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yahoo! Answers - Text Classification benchmarking - Accuracy<BR>","<BR>task: Text summarization<BR>date: 2019-05<BR>ratio: 0.2864<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-2<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-L<BR>","<BR>task: Natural language generation<BR>date: 2019-05<BR>ratio: 0.47556666666666664<BR>benchmarks:<BR>  Machine Translation: IWSLT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-German - Machine Translation benchmarking - BLEU score<BR>  Question Generation: SQuAD1.1 - Question Generation benchmarking - BLEU-4<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: GigaWord - Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Semantic analysis<BR>date: 2019-05<BR>ratio: 0.575<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>  Word Sense Disambiguation: SemEval 2007 Task 17 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2007 Task 7 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2013 Task 12 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SemEval 2015 Task 13 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 2 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Task 1 - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2007<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2013<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - SemEval 2015<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 2<BR>  Word Sense Disambiguation: Supervised: - Word Sense Disambiguation benchmarking - Senseval 3<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-05<BR>ratio: 0.3432<BR>benchmarks:<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: MPQA - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Machine translation<BR>date: 2019-05<BR>ratio: 0.3157<BR>benchmarks:<BR>  Unsupervised Machine Translation: WMT2014 English-French - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2014 French-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 English-German - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 German-English - Unsupervised Machine Translation benchmarking - BLEU<BR>  Unsupervised Machine Translation: WMT2016 Romanian-English - Unsupervised Machine Translation benchmarking - BLEU<BR>","<BR>task: Information extraction<BR>date: 2019-05<BR>ratio: 0.2659<BR>benchmarks:<BR>  Relation Extraction: ACE 2004 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - NER Macro F1<BR>  Relation Extraction: ADE Corpus - Relation Extraction benchmarking - RE+ Macro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: CoNLL04 - Relation Extraction benchmarking - RE+ Micro F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2019-06<BR>ratio: 0.5752<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT 2018 Finnish-English - Machine Translation benchmarking - BLEU<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-06<BR>ratio: 0.5772666666666667<BR>benchmarks:<BR>  Intent Detection: ATIS - Intent Detection benchmarking - Accuracy<BR>  Intent Detection: SNIPS - Intent Detection benchmarking - Slot F1 Score<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - Accuracy<BR>  Paraphrase Identification: Quora Question Pairs - Paraphrase Identification benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-06<BR>ratio: 0.5346<BR>benchmarks:<BR>  Text Classification: IMDb - Text Classification benchmarking - Accuracy (2 classes)<BR>  Text Classification: R52 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: Yelp-2 - Text Classification benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-06<BR>ratio: 0.2085<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>  Relation Extraction: TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Syntactic analysis<BR>date: 2019-06<BR>ratio: 0.7168<BR>benchmarks:<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Max F1 (WSJ10)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ)<BR>  Constituency Grammar Induction: PTB - Constituency Grammar Induction benchmarking - Mean F1 (WSJ10)<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Inference and reasoning<BR>date: 2019-06<BR>ratio: 0.4636<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-06<BR>ratio: 0.4252<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Semantic Textual Similarity: STS Benchmark - Semantic Textual Similarity benchmarking - Pearson Correlation<BR>","<BR>task: Question answering<BR>date: 2019-06<BR>ratio: 0.2683<BR>benchmarks:<BR>  Question Answering: Quora Question Pairs - Question Answering benchmarking - Accuracy<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-h<BR>  Question Answering: RACE - Question Answering benchmarking - RACE-m<BR>  Question Answering: RACE - Question Answering benchmarking - RACE<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD1.1 dev - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 dev - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Question answering<BR>date: 2019-07<BR>ratio: 0.46240000000000003<BR>benchmarks:<BR>  Question Answering: NewsQA - Question Answering benchmarking - F1<BR>  Question Answering: TriviaQA - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-07<BR>ratio: 0.5883499999999999<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: MSRA Dev - Chinese Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: Re-TACRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Inference and reasoning<BR>date: 2019-07<BR>ratio: 0.3592<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Common Sense Reasoning: SWAG - Common Sense Reasoning benchmarking - Test<BR>  Natural Language Inference: ANLI test - Natural Language Inference benchmarking - A1<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Mismatched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: XNLI Chinese Dev - Natural Language Inference benchmarking - Accuracy<BR>","<BR>task: Semantic analysis<BR>date: 2019-07<BR>ratio: 0.0577<BR>benchmarks:<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-07<BR>ratio: 0.2631<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Coreference Resolution: OntoNotes - Coreference Resolution benchmarking - F1<BR>  Sentiment Analysis: IMDb - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-08<BR>ratio: 0.18335<BR>benchmarks:<BR>  Coreference Resolution: CoNLL 2012 - Coreference Resolution benchmarking - Avg F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Text summarization<BR>date: 2019-08<BR>ratio: 0.1081<BR>benchmarks:<BR>  Document Summarization: CNN / Daily Mail - Document Summarization benchmarking - ROUGE-1<BR>","<BR>task: Information extraction<BR>date: 2019-08<BR>ratio: 0.4151<BR>benchmarks:<BR>  Named Entity Recognition: ACE 2004 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: ACE 2005 - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: GENIA - Named Entity Recognition benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2019-08<BR>ratio: 0.23105<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Text classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: BBCSport - Document Classification benchmarking - Accuracy<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-09<BR>ratio: 0.02725<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Sentiment Analysis: SST-2 Binary classification - Sentiment Analysis benchmarking - Accuracy<BR>","<BR>task: Other NLP task<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Text-to-Image Generation: Multi-Modal-CelebA-HQ - Text-to-Image Generation benchmarking - FID<BR>","<BR>task: Inference and reasoning<BR>date: 2019-09<BR>ratio: 0.12815000000000001<BR>benchmarks:<BR>  Common Sense Reasoning: CommonsenseQA - Common Sense Reasoning benchmarking - Accuracy<BR>  Natural Language Inference: MultiNLI - Natural Language Inference benchmarking - Matched<BR>  Natural Language Inference: QNLI - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: RTE - Natural Language Inference benchmarking - Accuracy<BR>  Natural Language Inference: SNLI - Natural Language Inference benchmarking - % Test Accuracy<BR>","<BR>task: Text classification<BR>date: 2019-09<BR>ratio: 0.8833<BR>benchmarks:<BR>  Text Classification: 20NEWS - Text Classification benchmarking - F-measure<BR>  Text Classification: Amazon-2 - Text Classification benchmarking - Error<BR>  Text Classification: Amazon-5 - Text Classification benchmarking - Error<BR>  Text Classification: R8 - Text Classification benchmarking - Accuracy<BR>  Text Classification: R8 - Text Classification benchmarking - F-measure<BR>","<BR>task: Information extraction<BR>date: 2019-09<BR>ratio: 0.60275<BR>benchmarks:<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Entity F1<BR>  Joint Entity and Relation Extraction: SciERC - Joint Entity and Relation Extraction benchmarking - Relation F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - NER Micro F1<BR>  Relation Extraction: ACE 2005 - Relation Extraction benchmarking - RE Micro F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - Ign F1<BR>  Relation Extraction: NYT - Relation Extraction benchmarking - F1<BR>  Relation Extraction: NYT-single - Relation Extraction benchmarking - F1<BR>  Relation Extraction: WebNLG - Relation Extraction benchmarking - F1<BR>","<BR>task: Natural language generation<BR>date: 2019-09<BR>ratio: 0.2846<BR>benchmarks:<BR>  Machine Translation: WMT2014 German-English - Machine Translation benchmarking - BLEU score<BR>  Machine Translation: WMT2016 English-Romanian - Machine Translation benchmarking - BLEU score<BR>","<BR>task: Semantic analysis<BR>date: 2019-09<BR>ratio: 0.5651333333333334<BR>benchmarks:<BR>  Entity Disambiguation: ACE2004 - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: AIDA-CoNLL - Entity Disambiguation benchmarking - In-KB Accuracy<BR>  Entity Disambiguation: AQUAINT - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: MSNBC - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-CWEB - Entity Disambiguation benchmarking - Micro-F1<BR>  Entity Disambiguation: WNED-WIKI - Entity Disambiguation benchmarking - Micro-F1<BR>  Semantic Textual Similarity: MRPC - Semantic Textual Similarity benchmarking - Accuracy<BR>  Word Sense Disambiguation: SensEval 2 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>  Word Sense Disambiguation: SensEval 3 Lexical Sample - Word Sense Disambiguation benchmarking - F1<BR>","<BR>task: Question answering<BR>date: 2019-09<BR>ratio: 0.09475<BR>benchmarks:<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - EM<BR>  Question Answering: SQuAD2.0 - Question Answering benchmarking - F1<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Syntactic analysis<BR>date: 2019-09<BR>ratio: 0.1429<BR>benchmarks:<BR>  Linguistic Acceptability Assessment: CoLA - Linguistic Acceptability Assessment benchmarking - Accuracy<BR>","<BR>task: Text summarization<BR>date: 2019-10<BR>ratio: 0.2178<BR>benchmarks:<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-1<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-2<BR>  Abstractive Text Summarization: CNN / Daily Mail - Abstractive Text Summarization benchmarking - ROUGE-L<BR>","<BR>task: Natural language generation<BR>date: 2019-10<BR>ratio: 0.6335500000000001<BR>benchmarks:<BR>  Machine Translation: IWSLT2015 English-Vietnamese - Machine Translation benchmarking - BLEU<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-1<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-2<BR>  Text Summarization: X-Sum - Text Summarization benchmarking - ROUGE-3<BR>","<BR>task: Computer code processing<BR>date: 2019-10<BR>ratio: 0.1871<BR>benchmarks:<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Exact Match Accuracy<BR>  Code Generation: WikiSQL - Code Generation benchmarking - Execution Accuracy<BR>","<BR>task: Information extraction<BR>date: 2019-11<BR>ratio: 0.32506666666666667<BR>benchmarks:<BR>  Chinese Named Entity Recognition: MSRA - Chinese Named Entity Recognition benchmarking - F1<BR>  Chinese Named Entity Recognition: Resume NER - Chinese Named Entity Recognition benchmarking - F1<BR>  Named Entity Recognition: BC5CDR - Named Entity Recognition benchmarking - F1<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>","<BR>task: Information retrieval<BR>date: 2019-11<BR>ratio: 0.4096<BR>benchmarks:<BR>  Conversational Response Selection: DSTC7 Ubuntu - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI AmazonQA - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>  Conversational Response Selection: PolyAI Reddit - Conversational Response Selection benchmarking - 1-of-100 Accuracy<BR>","<BR>task: Pragmatics analysis<BR>date: 2019-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Intent Detection: ASOS.com user intent - Intent Detection benchmarking - F1<BR>","<BR>task: Text classification<BR>date: 2020-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Document Classification: Reuters-21578 - Document Classification benchmarking - F1<BR>  Text Classification: RCV1 - Text Classification benchmarking - Micro F1<BR>","<BR>task: Question answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Information extraction<BR>date: 2020-03<BR>ratio: 0.3066<BR>benchmarks:<BR>  Relation Extraction: DocRED - Relation Extraction benchmarking - F1<BR>","<BR>task: Information extraction<BR>date: 2020-04<BR>ratio: 0.2667<BR>benchmarks:<BR>  Relation Extraction: SemEval-2010 Task 8 - Relation Extraction benchmarking - F1<BR>"],"marker":{"color":[0.5551,0.76,0.3305,0.2549,0.2308,0.0886,0.0904,0.3032,1.0,0.1186,0.0427,0.232,0.5,0.1435,0.2159,0.4025,0.25,0.483,0.3491,0.3173,0.0718,0.2,0.40459999999999996,0.5215,0.1323,0.1966,0.2762,0.7941,0.3097,0.0702,0.0678,0.30955,0.094,0.9481,0.218,0.1882,0.3589,0.36160000000000003,0.098,0.3558,0.101,0.0286,0.8,0.6069,0.0632,0.29685,0.385075,0.0123,0.6667,0.7801,0.0708,0.0597,0.1937,0.0392,0.2857,0.033,0.55,0.06,0.6016,0.14375,0.29195,0.2511,0.2379,0.921,0.42015,0.1524,0.30775,0.22825,0.212,1.0,0.57325,0.2288,0.4269,0.4228,0.3099,0.0407,0.0196,0.6226,0.4385,0.56715,0.6272,0.7021,0.0798,0.25695,0.0392,0.6346,0.0459,0.0571,0.0392,0.1887,0.54575,0.0882,0.4672,0.0809,0.1176,0.6357,0.2873,0.19115,0.5076,0.0357,0.541,0.2009,1.0,0.1587,0.5921,0.2342,0.137,0.0412,0.2896,0.57845,0.8365,0.36539999999999995,0.3299,0.11170000000000001,0.1343,0.6132,0.7567,0.11065,0.3862,1.0,0.791,0.0553,1.0,0.3122,0.5132,0.0282,0.5092,0.4922,0.72315,0.4058,0.1946,0.2718,0.2353,0.652,0.2419,0.53495,0.3185,0.1442,0.196,0.9057,0.43885,0.2123,0.20085,0.33775,0.5481,0.327,1.0,0.15,0.48510000000000003,0.2719,0.1449,0.2698,0.35325,1.0,0.6137,0.3558,0.37765000000000004,0.3456,0.2041,0.47159999999999996,1.0,0.4127,1.0,0.3333,0.1426,1.0,0.397,0.3546,0.6327,0.6981,0.1016,0.833,0.3031,0.1343,0.5876,0.3147,0.4341666666666666,0.1967,0.2915,0.5763,1.0,0.0123,0.4213,0.45885,0.4549,0.2864,0.47556666666666664,0.575,0.3432,0.3157,0.2659,0.5752,0.5772666666666667,0.5346,0.2085,0.7168,0.4636,0.4252,0.2683,0.46240000000000003,0.5883499999999999,0.3592,0.0577,0.2631,0.18335,0.1081,0.4151,0.23105,1.0,0.02725,1.0,0.12815000000000001,0.8833,0.60275,0.2846,0.5651333333333334,0.09475,0.1429,0.2178,0.6335500000000001,0.1871,0.32506666666666667,0.4096,1.0,1.0,0.5266,0.3066,0.2667],"colorbar":{"len":500,"lenmode":"pixels","thickness":10,"title":{"text":"ratio"}},"colorscale":[[0.0,"rgb(255,255,229)"],[0.125,"rgb(247,252,185)"],[0.25,"rgb(217,240,163)"],[0.375,"rgb(173,221,142)"],[0.5,"rgb(120,198,121)"],[0.625,"rgb(65,171,93)"],[0.75,"rgb(35,132,67)"],[0.875,"rgb(0,104,55)"],[1.0,"rgb(0,69,41)"]],"opacity":0.7,"showscale":true,"size":20,"symbol":"circle","line":{"color":"black","width":1}},"mode":"markers","x":["2013-10","2014-06","2014-06","2014-08","2014-08","2014-09","2014-10","2014-12","2014-12","2015-02","2015-06","2015-06","2015-08","2015-09","2015-11","2015-11","2015-11","2016-01","2016-02","2016-02","2016-02","2016-03","2016-03","2016-03","2016-03","2016-03","2016-05","2016-06","2016-06","2016-06","2016-06","2016-06","2016-07","2016-07","2016-07","2016-08","2016-08","2016-09","2016-09","2016-09","2016-09","2016-09","2016-10","2016-10","2016-11","2016-11","2016-11","2016-11","2016-12","2016-12","2016-12","2017-01","2017-02","2017-02","2017-02","2017-03","2017-04","2017-04","2017-04","2017-04","2017-05","2017-05","2017-05","2017-05","2017-06","2017-06","2017-07","2017-07","2017-07","2017-07","2017-07","2017-08","2017-08","2017-08","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-10","2017-10","2017-10","2017-11","2017-11","2017-11","2017-11","2017-12","2017-12","2017-12","2017-12","2018-01","2018-01","2018-01","2018-02","2018-02","2018-02","2018-02","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-04","2018-04","2018-04","2018-04","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-06","2018-06","2018-06","2018-06","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-08","2018-08","2018-08","2018-08","2018-08","2018-08","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-11","2018-11","2018-11","2018-11","2018-12","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-03","2019-03","2019-03","2019-03","2019-03","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-07","2019-07","2019-07","2019-07","2019-07","2019-08","2019-08","2019-08","2019-08","2019-08","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-10","2019-10","2019-10","2019-11","2019-11","2019-12","2020-02","2020-02","2020-03","2020-04"],"y":["Pragmatics analysis","Question answering","Pragmatics analysis","Inference and reasoning","Pragmatics analysis","Natural language generation","Natural language generation","Question answering","Natural language generation","Pragmatics analysis","Pragmatics analysis","Question answering","Inference and reasoning","Natural language generation","Question answering","Pragmatics analysis","Text classification","Information extraction","Pragmatics analysis","Natural language generation","Question answering","Natural language generation","Question answering","Text classification","Semantic analysis","Syntactic analysis","Question answering","Text classification","Question answering","Pragmatics analysis","Semantic analysis","Natural language generation","Pragmatics analysis","Syntactic analysis","Natural language generation","Natural language generation","Question answering","Natural language generation","Inference and reasoning","Text classification","Question answering","Pragmatics analysis","Natural language generation","Question answering","Natural language generation","Question answering","Syntactic analysis","Text classification","Natural language generation","Question answering","Other NLP task","Natural language generation","Pragmatics analysis","Inference and reasoning","Text classification","Question answering","Semantic analysis","Question answering","Syntactic analysis","Pragmatics analysis","Natural language generation","Text summarization","Question answering","Semantic analysis","Natural language generation","Question answering","Syntactic analysis","Question answering","Information extraction","Sentence embedding","Pragmatics analysis","Pragmatics analysis","Computer code processing","Question answering","Pragmatics analysis","Semantic analysis","Inference and reasoning","Natural language generation","Information extraction","Dialog process","Question answering","Other NLP task","Text classification","Dialog process","Inference and reasoning","Other NLP task","Natural language generation","Question answering","Inference and reasoning","Semantic analysis","Pragmatics analysis","Text classification","Question answering","Pragmatics analysis","Text classification","Natural language generation","Pragmatics analysis","Semantic analysis","Natural language generation","Text classification","Information retrieval","Computer code processing","Semantic analysis","Question answering","Question answering","Pragmatics analysis","Computer code processing","Inference and reasoning","Pragmatics analysis","Inference and reasoning","Dialog process","Question answering","Text classification","Semantic analysis","Syntactic analysis","Inference and reasoning","Pragmatics analysis","Natural language generation","Question answering","Question answering","Syntactic analysis","Natural language generation","Sentence embedding","Information extraction","Text classification","Pragmatics analysis","Text classification","Syntactic analysis","Information extraction","Text summarization","Natural language generation","Question answering","Inference and reasoning","Text classification","Question answering","Natural language generation","Information extraction","Dialog process","Question answering","Text classification","Semantic analysis","Syntactic analysis","Information extraction","Pragmatics analysis","Computer code processing","Dialog process","Natural language generation","Machine translation","Inference and reasoning","Semantic analysis","Question answering","Syntactic analysis","Pragmatics analysis","Information extraction","Text classification","Information retrieval","Pragmatics analysis","Inference and reasoning","Question answering","Natural language generation","Other NLP task","Machine translation","Information extraction","Information extraction","Dialog process","Inference and reasoning","Machine translation","Text classification","Question answering","Text classification","Other NLP task","Information extraction","Text summarization","Syntactic analysis","Other NLP task","Question answering","Information extraction","Syntactic analysis","Information retrieval","Natural language generation","Pragmatics analysis","Text classification","Dialog process","Question answering","Text classification","Text summarization","Natural language generation","Semantic analysis","Pragmatics analysis","Machine translation","Information extraction","Natural language generation","Pragmatics analysis","Text classification","Information extraction","Syntactic analysis","Inference and reasoning","Semantic analysis","Question answering","Question answering","Information extraction","Inference and reasoning","Semantic analysis","Pragmatics analysis","Pragmatics analysis","Text summarization","Information extraction","Question answering","Text classification","Pragmatics analysis","Other NLP task","Inference and reasoning","Text classification","Information extraction","Natural language generation","Semantic analysis","Question answering","Syntactic analysis","Text summarization","Natural language generation","Computer code processing","Information extraction","Information retrieval","Pragmatics analysis","Text classification","Question answering","Information extraction","Information extraction"],"type":"scatter","line":{"color":"black","width":0}}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"},"showgrid":true,"gridcolor":"lightBlue","tickmode":"auto"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{},"categoryorder":"array","categoryarray":["Text summarization","Text classification","Syntactic analysis","Sentence embedding","Semantic analysis","Question answering","Pragmatics analysis","Other NLP task","Natural language generation","Machine translation","Information retrieval","Information extraction","Inference and reasoning","Dialog process","Computer code processing"],"showgrid":true,"gridcolor":"lightBlue","side":"left"},"legend":{"title":{"text":"task"},"tracegroupgap":0},"margin":{"t":60},"title":{"text":"Natural Language Processing","y":0.995},"font":{"size":21},"showlegend":false,"plot_bgcolor":"white","height":750.0,"width":1500},                        {"responsive": true}                    )                };                            </script>        </div>
</body>
</html>