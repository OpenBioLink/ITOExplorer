<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="c911d122-94ec-44b3-8bac-98c9ce316a06" class="plotly-graph-div" style="height:5872.5px; width:1500px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c911d122-94ec-44b3-8bac-98c9ce316a06")) {                    Plotly.newPlot(                        "c911d122-94ec-44b3-8bac-98c9ce316a06",                        [{"hovertemplate":"color=Zero-Shot Action Recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Zero-Shot Action Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Zero-Shot Action Recognition","showlegend":true,"x":["2021-08-01T00:00:00","2020-03-01T00:00:00","2019-07-01T00:00:00","2018-03-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2016-11-01T00:00:00","2015-10-01T00:00:00","2015-07-01T00:00:00"],"xaxis":"x","y":["Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Weakly-supervised instance segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Weakly-supervised instance segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Weakly-supervised instance segmentation","showlegend":true,"x":["2020-09-01T00:00:00","2021-03-01T00:00:00"],"xaxis":"x","y":["Weakly-supervised instance segmentation","Weakly-supervised instance segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Visual reasoning<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Visual reasoning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Visual reasoning","showlegend":true,"x":["2021-11-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-04-01T00:00:00","2021-02-01T00:00:00","2020-08-01T00:00:00","2020-06-01T00:00:00","2019-09-01T00:00:00"],"xaxis":"x","y":["Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Visual place recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Visual place recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Visual place recognition","showlegend":true,"x":["2019-11-01T00:00:00","2018-12-01T00:00:00","2021-07-01T00:00:00"],"xaxis":"x","y":["Visual place recognition","Visual place recognition","Visual place recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Visual dialog<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Visual dialog","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Visual dialog","showlegend":true,"x":["2017-06-01T00:00:00","2018-09-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2021-04-01T00:00:00","2017-11-01T00:00:00","2017-09-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-11-01T00:00:00"],"xaxis":"x","y":["Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Visual Relationship Detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Visual Relationship Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Visual Relationship Detection","showlegend":true,"x":["2017-02-01T00:00:00","2019-01-01T00:00:00","2018-04-01T00:00:00","2017-07-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00"],"xaxis":"x","y":["Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Visual Odometry<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Visual Odometry","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Visual Odometry","showlegend":true,"x":["2019-01-01T00:00:00","2015-11-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00"],"xaxis":"x","y":["Visual Odometry","Visual Odometry","Visual Odometry","Visual Odometry"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video super-resolution<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video super-resolution","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video super-resolution","showlegend":true,"x":["2019-03-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-06-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-05-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-01-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00"],"xaxis":"x","y":["Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video summarization<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video summarization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video summarization","showlegend":true,"x":["2019-12-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-10-01T00:00:00","2021-12-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00"],"xaxis":"x","y":["Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video semantic segmentation","showlegend":true,"x":["2021-02-01T00:00:00","2020-04-01T00:00:00","2016-12-01T00:00:00"],"xaxis":"x","y":["Video process // Video semantic segmentation","Video process // Video semantic segmentation","Video process // Video semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video segmentation","showlegend":true,"x":["2020-08-01T00:00:00","2018-08-01T00:00:00"],"xaxis":"x","y":["Video process // Video segmentation","Video process // Video segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video retrieval<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video retrieval","showlegend":true,"x":["2020-02-01T00:00:00","2016-12-01T00:00:00","2017-07-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video question answering<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video question answering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video question answering","showlegend":true,"x":["2020-11-01T00:00:00","2021-03-01T00:00:00","2020-05-01T00:00:00","2020-02-01T00:00:00","2018-09-01T00:00:00"],"xaxis":"x","y":["Video process // Video question answering","Video process // Video question answering","Video process // Video question answering","Video process // Video question answering","Video process // Video question answering"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video prediction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video prediction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video prediction","showlegend":true,"x":["2017-06-01T00:00:00","2018-02-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2016-05-01T00:00:00","2018-10-01T00:00:00","2018-04-01T00:00:00","2020-02-01T00:00:00","2018-11-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2021-07-01T00:00:00"],"xaxis":"x","y":["Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video object segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video object segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video object segmentation","showlegend":true,"x":["2018-09-01T00:00:00","2017-06-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2018-03-01T00:00:00","2018-02-01T00:00:00","2017-12-01T00:00:00","2017-09-01T00:00:00","2017-07-01T00:00:00","2019-09-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2017-01-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-06-01T00:00:00","2015-12-01T00:00:00","2015-06-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2020-01-01T00:00:00","2019-02-01T00:00:00","2020-07-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2019-03-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-10-01T00:00:00","2020-08-01T00:00:00","2021-06-01T00:00:00","2020-04-01T00:00:00","2020-02-01T00:00:00","2019-10-01T00:00:00","2019-08-01T00:00:00","2019-06-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video instance segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video instance segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video instance segmentation","showlegend":true,"x":["2020-07-01T00:00:00","2018-02-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-11-01T00:00:00","2021-06-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video inpainting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video inpainting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video inpainting","showlegend":true,"x":["2021-09-01T00:00:00","2020-09-01T00:00:00","2020-07-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00"],"xaxis":"x","y":["Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video generation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video generation","showlegend":true,"x":["2017-10-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2017-07-01T00:00:00","2016-11-01T00:00:00","2018-04-01T00:00:00","2021-06-01T00:00:00","2019-07-01T00:00:00","2020-03-01T00:00:00","2020-11-01T00:00:00","2019-12-01T00:00:00","2021-11-01T00:00:00"],"xaxis":"x","y":["Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video frame interpolation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video frame interpolation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video frame interpolation","showlegend":true,"x":["2021-11-01T00:00:00","2021-08-01T00:00:00","2021-03-01T00:00:00","2020-11-01T00:00:00","2020-07-01T00:00:00","2020-03-01T00:00:00","2019-04-01T00:00:00","2018-10-01T00:00:00","2017-11-01T00:00:00"],"xaxis":"x","y":["Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video denoising<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video denoising","showlegend":true,"x":["2019-07-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00"],"xaxis":"x","y":["Video process // Video denoising","Video process // Video denoising","Video process // Video denoising"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video classification","showlegend":true,"x":["2017-07-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2017-12-01T00:00:00","2017-11-01T00:00:00","2017-10-01T00:00:00","2017-08-01T00:00:00","2016-07-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-08-01T00:00:00","2019-03-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2016-01-01T00:00:00","2015-06-01T00:00:00","2015-05-01T00:00:00","2014-12-01T00:00:00","2019-02-01T00:00:00","2015-03-01T00:00:00","2019-04-01T00:00:00","2020-12-01T00:00:00","2019-05-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-11-01T00:00:00","2019-12-01T00:00:00","2020-08-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00","2020-10-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00"],"xaxis":"x","y":["Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video captioning<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video captioning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video captioning","showlegend":true,"x":["2019-04-01T00:00:00","2020-05-01T00:00:00","2020-02-01T00:00:00","2021-07-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00"],"xaxis":"x","y":["Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video Quality Assessment<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video Quality Assessment","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video Quality Assessment","showlegend":true,"x":["2021-01-01T00:00:00","2020-11-01T00:00:00","2020-08-01T00:00:00","2020-05-01T00:00:00","2019-08-01T00:00:00"],"xaxis":"x","y":["Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Video Enhancement<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Video Enhancement","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Video Enhancement","showlegend":true,"x":["2019-05-01T00:00:00","2019-02-01T00:00:00","2020-04-01T00:00:00","2021-04-01T00:00:00"],"xaxis":"x","y":["Video process // Video Enhancement","Video process // Video Enhancement","Video process // Video Enhancement","Video process // Video Enhancement"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Object tracking<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Object tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Object tracking","showlegend":true,"x":["2018-03-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2018-11-01T00:00:00","2018-06-01T00:00:00","2018-04-01T00:00:00","2015-04-01T00:00:00","2017-04-01T00:00:00","2018-02-01T00:00:00","2017-10-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2016-11-01T00:00:00","2016-08-01T00:00:00","2019-04-01T00:00:00","2019-03-01T00:00:00","2020-11-01T00:00:00","2019-06-01T00:00:00","2021-01-01T00:00:00","2019-07-01T00:00:00","2021-12-01T00:00:00","2021-10-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-07-01T00:00:00","2020-12-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2019-11-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Activity recognition in videos<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Activity recognition in videos","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Activity recognition in videos","showlegend":true,"x":["2015-05-01T00:00:00","2014-12-01T00:00:00","2016-05-01T00:00:00"],"xaxis":"x","y":["Video process // Activity recognition in videos","Video process // Activity recognition in videos","Video process // Activity recognition in videos"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Action spotting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Action spotting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Action spotting","showlegend":true,"x":["2021-02-01T00:00:00","2019-12-01T00:00:00"],"xaxis":"x","y":["Video process // Action spotting","Video process // Action spotting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Action classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Action classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Action classification","showlegend":true,"x":["2017-10-01T00:00:00","2018-02-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-07-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2015-06-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2020-08-01T00:00:00","2019-04-01T00:00:00","2020-07-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-09-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2020-10-01T00:00:00","2019-05-01T00:00:00","2019-07-01T00:00:00","2019-10-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-06-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Video process // Abnormal event detection in video<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Video process // Abnormal event detection in video","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Video process // Abnormal event detection in video","showlegend":true,"x":["2021-01-01T00:00:00","2020-07-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2017-01-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Text based person retrieval<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Text based person retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Text based person retrieval","showlegend":true,"x":["2020-10-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2021-05-01T00:00:00","2021-01-01T00:00:00","2021-10-01T00:00:00","2018-09-01T00:00:00"],"xaxis":"x","y":["Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Surface normals estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Surface normals estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Surface normals estimation","showlegend":true,"x":["2019-04-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Surface normals estimation","Surface normals estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Super-resolution<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Super-resolution","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Super-resolution","showlegend":true,"x":["2018-12-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2017-12-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-03-01T00:00:00","2018-02-01T00:00:00","2016-09-01T00:00:00","2017-07-01T00:00:00","2017-04-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-08-01T00:00:00","2016-06-01T00:00:00","2016-03-01T00:00:00","2015-11-01T00:00:00","2015-08-01T00:00:00","2019-03-01T00:00:00","2019-02-01T00:00:00","2018-01-01T00:00:00","2019-04-01T00:00:00","2020-05-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-08-01T00:00:00","2021-05-01T00:00:00","2020-12-01T00:00:00","2020-11-01T00:00:00","2020-08-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2021-06-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00"],"xaxis":"x","y":["Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Single-object discovery<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Single-object discovery","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Single-object discovery","showlegend":true,"x":["2021-09-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Single-object discovery","Single-object discovery"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Sign language translation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Sign language translation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Sign language translation","showlegend":true,"x":["2020-04-01T00:00:00","2021-09-01T00:00:00"],"xaxis":"x","y":["Sign language translation","Sign language translation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Sign language recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Sign language recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Sign language recognition","showlegend":true,"x":["2021-03-01T00:00:00","2021-01-01T00:00:00","2020-07-01T00:00:00","2020-05-01T00:00:00"],"xaxis":"x","y":["Sign language recognition","Sign language recognition","Sign language recognition","Sign language recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semi-supervised object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semi-supervised object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semi-supervised object detection","showlegend":true,"x":["2021-11-01T00:00:00","2021-06-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Semi-supervised object detection","Semi-supervised object detection","Semi-supervised object detection","Semi-supervised object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Weakly-supervised semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Weakly-supervised semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Weakly-supervised semantic segmentation","showlegend":true,"x":["2021-05-01T00:00:00","2020-09-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2019-10-01T00:00:00","2021-08-01T00:00:00","2021-12-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Unsupervised semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Unsupervised semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Unsupervised semantic segmentation","showlegend":true,"x":["2021-10-01T00:00:00","2021-03-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Semi-supervised semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Semi-supervised semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Semi-supervised semantic segmentation","showlegend":true,"x":["2021-03-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-12-01T00:00:00","2019-06-01T00:00:00","2021-04-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2021-06-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Scene segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Scene segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Scene segmentation","showlegend":true,"x":["2019-08-01T00:00:00","2016-12-01T00:00:00","2019-06-01T00:00:00","2019-05-01T00:00:00","2021-05-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Real-time semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Real-time semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Real-time semantic segmentation","showlegend":true,"x":["2018-11-01T00:00:00","2015-02-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2021-11-01T00:00:00","2019-03-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Panoptic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Panoptic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Panoptic segmentation","showlegend":true,"x":["2020-04-01T00:00:00","2019-01-01T00:00:00","2019-05-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2018-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-12-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-09-01T00:00:00","2021-12-01T00:00:00","2020-11-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // LIDAR semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // LIDAR semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // LIDAR semantic segmentation","showlegend":true,"x":["2020-04-01T00:00:00","2019-04-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // LIDAR semantic segmentation","Semantic segmentation // LIDAR semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Human part segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Human part segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Human part segmentation","showlegend":true,"x":["2018-09-01T00:00:00","2017-08-01T00:00:00","2018-05-01T00:00:00","2019-10-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // Few-shot semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // Few-shot semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // Few-shot semantic segmentation","showlegend":true,"x":["2020-07-01T00:00:00","2019-03-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // 3D semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // 3D semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // 3D semantic segmentation","showlegend":true,"x":["2019-10-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2019-04-01T00:00:00","2020-07-01T00:00:00","2020-03-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // 3D part segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // 3D part segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // 3D part segmentation","showlegend":true,"x":["2018-01-01T00:00:00","2018-03-01T00:00:00","2019-02-01T00:00:00","2017-06-01T00:00:00","2016-12-01T00:00:00","2018-11-01T00:00:00","2018-06-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation // 2D Semantic Segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation // 2D Semantic Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation // 2D Semantic Segmentation","showlegend":true,"x":["2020-04-01T00:00:00","2017-11-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2020-02-01T00:00:00","2018-04-01T00:00:00","2020-10-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2020-11-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Semantic segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Semantic segmentation","showlegend":true,"x":["2018-04-01T00:00:00","2014-11-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-08-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2015-11-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-06-01T00:00:00","2016-05-01T00:00:00","2016-03-01T00:00:00","2019-03-01T00:00:00","2015-09-01T00:00:00","2015-04-01T00:00:00","2015-03-01T00:00:00","2015-02-01T00:00:00","2014-12-01T00:00:00","2019-02-01T00:00:00","2020-06-01T00:00:00","2019-04-01T00:00:00","2020-11-01T00:00:00","2019-05-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-03-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2020-08-01T00:00:00","2019-11-01T00:00:00","2019-06-01T00:00:00","2020-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00"],"xaxis":"x","y":["Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Scene text detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Scene text detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Scene text detection","showlegend":true,"x":["2018-06-01T00:00:00","2015-04-01T00:00:00","2016-04-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2020-05-01T00:00:00","2018-07-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2021-11-01T00:00:00","2018-11-01T00:00:00"],"xaxis":"x","y":["Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Scene parsing<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Scene parsing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Scene parsing","showlegend":true,"x":["2018-08-01T00:00:00","2019-04-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2015-07-01T00:00:00","2018-06-01T00:00:00","2017-11-01T00:00:00","2016-09-01T00:00:00","2016-03-01T00:00:00","2020-02-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Saliency detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Saliency detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Saliency detection","showlegend":true,"x":["2020-07-01T00:00:00","2019-10-01T00:00:00","2020-04-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Robot navigation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Robot navigation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Robot navigation","showlegend":true,"x":["2021-04-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Robot navigation","Robot navigation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Remote sensing<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Remote sensing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Remote sensing","showlegend":true,"x":["2021-02-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Remote sensing","Remote sensing"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Reconstruction","showlegend":true,"x":["2021-12-01T00:00:00","2021-08-01T00:00:00","2020-12-01T00:00:00","2020-08-01T00:00:00","2020-06-01T00:00:00"],"xaxis":"x","y":["Reconstruction","Reconstruction","Reconstruction","Reconstruction","Reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Rain removal<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Rain removal","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Rain removal","showlegend":true,"x":["2020-03-01T00:00:00","2018-02-01T00:00:00","2018-07-01T00:00:00","2019-06-01T00:00:00","2019-01-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-05-01T00:00:00","2021-11-01T00:00:00"],"xaxis":"x","y":["Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Quantization<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Quantization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Quantization","showlegend":true,"x":["2021-11-01T00:00:00","2021-03-01T00:00:00"],"xaxis":"x","y":["Quantization","Quantization"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Pose tracking<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Pose tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Pose tracking","showlegend":true,"x":["2018-04-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2019-02-01T00:00:00","2019-05-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Point cloud super resolution<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Point cloud super resolution","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Point cloud super resolution","showlegend":true,"x":["2021-02-01T00:00:00","2019-08-01T00:00:00"],"xaxis":"x","y":["Point cloud super resolution","Point cloud super resolution"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Point cloud registration<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Point cloud registration","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Point cloud registration","showlegend":true,"x":["2020-04-01T00:00:00","2018-11-01T00:00:00","2019-10-01T00:00:00","2018-08-01T00:00:00","2020-03-01T00:00:00","2020-11-01T00:00:00","2020-09-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00"],"xaxis":"x","y":["Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Point cloud generation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Point cloud generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Point cloud generation","showlegend":true,"x":["2021-08-01T00:00:00","2020-06-01T00:00:00","2019-11-01T00:00:00","2019-06-01T00:00:00","2018-08-01T00:00:00"],"xaxis":"x","y":["Point cloud generation","Point cloud generation","Point cloud generation","Point cloud generation","Point cloud generation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Point cloud classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Point cloud classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Point cloud classification","showlegend":true,"x":["2020-08-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2018-01-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2020-05-01T00:00:00","2019-10-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-05-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Point Cloud Segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Point Cloud Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Point Cloud Segmentation","showlegend":true,"x":["2020-10-01T00:00:00","2017-06-01T00:00:00","2020-12-01T00:00:00"],"xaxis":"x","y":["Point Cloud Segmentation","Point Cloud Segmentation","Point Cloud Segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Person search<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Person search","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Person search","showlegend":true,"x":["2021-12-01T00:00:00","2021-09-01T00:00:00"],"xaxis":"x","y":["Person search","Person search"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Person re-identification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Person re-identification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Person re-identification","showlegend":true,"x":["2017-11-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2017-01-01T00:00:00","2017-10-01T00:00:00","2017-09-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-03-01T00:00:00","2019-05-01T00:00:00","2016-11-01T00:00:00","2016-10-01T00:00:00","2016-07-01T00:00:00","2016-04-01T00:00:00","2016-03-01T00:00:00","2019-04-01T00:00:00","2019-03-01T00:00:00","2019-08-01T00:00:00","2021-01-01T00:00:00","2021-12-01T00:00:00","2019-09-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-11-01T00:00:00","2020-12-01T00:00:00","2020-09-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2020-10-01T00:00:00"],"xaxis":"x","y":["Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Out-of-distribution detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Out-of-distribution detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Out-of-distribution detection","showlegend":true,"x":["2020-11-01T00:00:00","2018-07-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2021-04-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Optical flow estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Optical flow estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Optical flow estimation","showlegend":true,"x":["2020-11-01T00:00:00","2019-04-01T00:00:00","2018-12-01T00:00:00","2017-09-01T00:00:00"],"xaxis":"x","y":["Optical flow estimation","Optical flow estimation","Optical flow estimation","Optical flow estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Optical character recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Optical character recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Optical character recognition","showlegend":true,"x":["2021-06-01T00:00:00","2018-11-01T00:00:00"],"xaxis":"x","y":["Optical character recognition","Optical character recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object segmentation","showlegend":true,"x":["2020-07-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Object segmentation","Object segmentation","Object segmentation","Object segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object reconstruction","showlegend":true,"x":["2021-08-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2019-05-01T00:00:00","2019-01-01T00:00:00","2018-02-01T00:00:00","2016-12-01T00:00:00"],"xaxis":"x","y":["Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object localization<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object localization","showlegend":true,"x":["2018-07-01T00:00:00","2019-11-01T00:00:00","2019-08-01T00:00:00","2018-04-01T00:00:00"],"xaxis":"x","y":["Object localization","Object localization","Object localization","Object localization"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Weakly supervised object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Weakly supervised object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Weakly supervised object detection","showlegend":true,"x":["2017-04-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2019-04-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-03-01T00:00:00","2018-02-01T00:00:00","2017-11-01T00:00:00","2019-10-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-11-01T00:00:00","2020-04-01T00:00:00","2020-10-01T00:00:00","2015-11-01T00:00:00"],"xaxis":"x","y":["Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Video object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Video object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Video object detection","showlegend":true,"x":["2020-03-01T00:00:00","2019-07-01T00:00:00","2018-11-01T00:00:00"],"xaxis":"x","y":["Object detection // Video object detection","Object detection // Video object detection","Object detection // Video object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Surgical tool detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Surgical tool detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Surgical tool detection","showlegend":true,"x":["2018-12-01T00:00:00","2018-06-01T00:00:00"],"xaxis":"x","y":["Object detection // Surgical tool detection","Object detection // Surgical tool detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Salient object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Salient object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Salient object detection","showlegend":true,"x":["2020-08-01T00:00:00","2019-08-01T00:00:00","2019-06-01T00:00:00","2018-09-01T00:00:00","2018-06-01T00:00:00","2016-06-01T00:00:00","2015-12-01T00:00:00","2015-06-01T00:00:00","2021-05-01T00:00:00"],"xaxis":"x","y":["Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Real-time object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Real-time object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Real-time object detection","showlegend":true,"x":["2020-10-01T00:00:00","2021-04-01T00:00:00","2016-05-01T00:00:00","2017-08-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00"],"xaxis":"x","y":["Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // RGB-D salient object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // RGB-D salient object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // RGB-D salient object detection","showlegend":true,"x":["2021-04-01T00:00:00","2021-02-01T00:00:00","2020-09-01T00:00:00","2020-08-01T00:00:00","2020-07-01T00:00:00","2020-05-01T00:00:00","2020-04-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2018-06-01T00:00:00"],"xaxis":"x","y":["Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // RGB salient object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // RGB salient object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // RGB salient object detection","showlegend":true,"x":["2017-12-01T00:00:00","2017-07-01T00:00:00","2018-06-01T00:00:00","2017-08-01T00:00:00","2019-04-01T00:00:00","2017-04-01T00:00:00","2016-06-01T00:00:00","2015-12-01T00:00:00","2015-06-01T00:00:00","2018-09-01T00:00:00","2020-04-01T00:00:00","2019-06-01T00:00:00","2019-10-01T00:00:00","2019-08-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-03-01T00:00:00","2021-06-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Object proposal generation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Object proposal generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Object proposal generation","showlegend":true,"x":["2021-11-01T00:00:00","2017-12-01T00:00:00"],"xaxis":"x","y":["Object detection // Object proposal generation","Object detection // Object proposal generation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Object detection in aerial images<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Object detection in aerial images","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Object detection in aerial images","showlegend":true,"x":["2020-04-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2021-01-01T00:00:00","2020-08-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00"],"xaxis":"x","y":["Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Object Detection In Indoor Scenes<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Object Detection In Indoor Scenes","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Object Detection In Indoor Scenes","showlegend":true,"x":["2017-11-01T00:00:00","2017-10-01T00:00:00"],"xaxis":"x","y":["Object detection // Object Detection In Indoor Scenes","Object detection // Object Detection In Indoor Scenes"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Medical object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Medical object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Medical object detection","showlegend":true,"x":["2021-03-01T00:00:00","2020-05-01T00:00:00","2019-08-01T00:00:00","2019-06-01T00:00:00"],"xaxis":"x","y":["Object detection // Medical object detection","Object detection // Medical object detection","Object detection // Medical object detection","Object detection // Medical object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Few-shot object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Few-shot object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Few-shot object detection","showlegend":true,"x":["2020-03-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2018-12-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Face detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Face detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Face detection","showlegend":true,"x":["2017-09-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-12-01T00:00:00","2017-08-01T00:00:00","2015-11-01T00:00:00","2018-02-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2019-05-01T00:00:00","2020-11-01T00:00:00","2021-07-01T00:00:00","2018-03-01T00:00:00"],"xaxis":"x","y":["Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Dense object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Dense object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Dense object detection","showlegend":true,"x":["2020-07-01T00:00:00","2019-11-01T00:00:00","2019-04-01T00:00:00"],"xaxis":"x","y":["Object detection // Dense object detection","Object detection // Dense object detection","Object detection // Dense object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Camouflaged object segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Camouflaged object segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Camouflaged object segmentation","showlegend":true,"x":["2020-07-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2021-02-01T00:00:00"],"xaxis":"x","y":["Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // Birds eye view object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // Birds eye view object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // Birds eye view object detection","showlegend":true,"x":["2021-04-01T00:00:00","2019-12-01T00:00:00","2019-10-01T00:00:00","2019-07-01T00:00:00","2018-12-01T00:00:00","2017-12-01T00:00:00"],"xaxis":"x","y":["Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // 3D object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // 3D object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // 3D object detection","showlegend":true,"x":["2019-04-01T00:00:00","2020-01-01T00:00:00","2019-12-01T00:00:00","2019-09-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2020-04-01T00:00:00","2019-03-01T00:00:00","2018-12-01T00:00:00","2018-10-01T00:00:00","2018-02-01T00:00:00","2017-12-01T00:00:00","2016-06-01T00:00:00","2020-03-01T00:00:00","2017-11-01T00:00:00","2020-06-01T00:00:00","2021-06-01T00:00:00","2021-12-01T00:00:00","2020-07-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection // 2D object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection // 2D object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection // 2D object detection","showlegend":true,"x":["2020-03-01T00:00:00","2017-06-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object detection","showlegend":true,"x":["2017-08-01T00:00:00","2019-04-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2018-03-01T00:00:00","2017-12-01T00:00:00","2017-11-01T00:00:00","2016-05-01T00:00:00","2017-03-01T00:00:00","2016-12-01T00:00:00","2016-09-01T00:00:00","2019-06-01T00:00:00","2015-12-01T00:00:00","2015-06-01T00:00:00","2015-04-01T00:00:00","2014-06-01T00:00:00","2019-05-01T00:00:00","2019-03-01T00:00:00","2019-08-01T00:00:00","2020-12-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-06-01T00:00:00","2021-03-01T00:00:00","2021-01-01T00:00:00","2021-05-01T00:00:00","2020-11-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-02-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00"],"xaxis":"x","y":["Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Object counting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Object counting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Object counting","showlegend":true,"x":["2021-07-01T00:00:00","2019-04-01T00:00:00","2017-07-01T00:00:00","2016-09-01T00:00:00"],"xaxis":"x","y":["Object counting","Object counting","Object counting","Object counting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Multispectral Object Detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Multispectral Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Multispectral Object Detection","showlegend":true,"x":["2021-10-01T00:00:00","2021-01-01T00:00:00"],"xaxis":"x","y":["Multispectral Object Detection","Multispectral Object Detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Multimodal machine translation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Multimodal machine translation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Multimodal machine translation","showlegend":true,"x":["2019-11-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2020-09-01T00:00:00"],"xaxis":"x","y":["Multimodal machine translation","Multimodal machine translation","Multimodal machine translation","Multimodal machine translation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Multi-target domain adaptation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Multi-target domain adaptation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Multi-target domain adaptation","showlegend":true,"x":["2021-04-01T00:00:00","2020-07-01T00:00:00","2019-12-01T00:00:00","2019-07-01T00:00:00"],"xaxis":"x","y":["Multi-target domain adaptation","Multi-target domain adaptation","Multi-target domain adaptation","Multi-target domain adaptation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Meta-learning<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Meta-learning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Meta-learning","showlegend":true,"x":["2018-12-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2019-05-01T00:00:00","2019-04-01T00:00:00","2019-03-01T00:00:00","2019-02-01T00:00:00","2019-01-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2019-11-01T00:00:00","2018-05-01T00:00:00","2017-11-01T00:00:00","2017-06-01T00:00:00","2017-03-01T00:00:00","2016-05-01T00:00:00","2016-03-01T00:00:00","2014-09-01T00:00:00","2019-10-01T00:00:00","2021-02-01T00:00:00","2019-12-01T00:00:00","2020-12-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2020-01-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-01-01T00:00:00","2021-07-01T00:00:00","2020-11-01T00:00:00","2020-09-01T00:00:00","2020-08-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2020-02-01T00:00:00","2020-10-01T00:00:00"],"xaxis":"x","y":["Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Medical diagnosis<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Medical diagnosis","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Medical diagnosis","showlegend":true,"x":["2019-10-01T00:00:00","2017-07-01T00:00:00"],"xaxis":"x","y":["Medical diagnosis","Medical diagnosis"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Material property prediction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Material property prediction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Material property prediction","showlegend":true,"x":["2018-06-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-09-01T00:00:00","2017-12-01T00:00:00","2021-06-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00"],"xaxis":"x","y":["Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Line segment detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Line segment detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Line segment detection","showlegend":true,"x":["2018-12-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-04-01T00:00:00","2021-01-01T00:00:00"],"xaxis":"x","y":["Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Keyword spotting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Keyword spotting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Keyword spotting","showlegend":true,"x":["2021-06-01T00:00:00","2021-05-01T00:00:00","2021-03-01T00:00:00","2020-08-01T00:00:00","2020-05-01T00:00:00","2020-04-01T00:00:00","2020-01-01T00:00:00","2019-01-01T00:00:00","2018-08-01T00:00:00"],"xaxis":"x","y":["Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Interest point detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Interest point detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Interest point detection","showlegend":true,"x":["2019-09-01T00:00:00","2017-09-01T00:00:00"],"xaxis":"x","y":["Interest point detection","Interest point detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Intelligent surveillance<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Intelligent surveillance","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Intelligent surveillance","showlegend":true,"x":["2021-04-01T00:00:00","2020-04-01T00:00:00","2021-02-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Intelligent surveillance","Intelligent surveillance","Intelligent surveillance","Intelligent surveillance","Intelligent surveillance"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Instance segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Instance segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Instance segmentation","showlegend":true,"x":["2019-12-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2019-04-01T00:00:00","2019-02-01T00:00:00","2018-12-01T00:00:00","2018-03-01T00:00:00","2016-11-01T00:00:00","2017-12-01T00:00:00","2017-07-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00","2021-08-01T00:00:00","2020-04-01T00:00:00","2021-04-01T00:00:00","2020-06-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-09-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2020-12-01T00:00:00","2020-11-01T00:00:00","2020-10-01T00:00:00","2020-09-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image/document clustering<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image/document clustering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image/document clustering","showlegend":true,"x":["2021-04-01T00:00:00","2021-05-01T00:00:00"],"xaxis":"x","y":["Image/document clustering","Image/document clustering"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image tagging<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image tagging","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image tagging","showlegend":true,"x":["2020-03-01T00:00:00","2017-06-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2021-01-01T00:00:00","2020-04-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image retrieval<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image retrieval","showlegend":true,"x":["2017-07-01T00:00:00","2018-11-01T00:00:00","2018-04-01T00:00:00","2018-03-01T00:00:00","2018-01-01T00:00:00","2017-12-01T00:00:00","2017-11-01T00:00:00","2016-11-01T00:00:00","2017-06-01T00:00:00","2016-12-01T00:00:00","2019-03-01T00:00:00","2016-08-01T00:00:00","2016-04-01T00:00:00","2015-11-01T00:00:00","2015-04-01T00:00:00","2018-12-01T00:00:00","2021-11-01T00:00:00","2019-05-01T00:00:00","2021-01-01T00:00:00","2021-10-01T00:00:00","2021-08-01T00:00:00","2021-06-01T00:00:00","2021-04-01T00:00:00","2019-08-01T00:00:00","2021-02-01T00:00:00","2020-12-01T00:00:00","2020-08-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2019-09-01T00:00:00"],"xaxis":"x","y":["Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image restoration<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image restoration","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image restoration","showlegend":true,"x":["2018-12-01T00:00:00","2016-06-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2017-08-01T00:00:00","2020-04-01T00:00:00","2021-11-01T00:00:00","2021-04-01T00:00:00","2021-09-01T00:00:00"],"xaxis":"x","y":["Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image reconstruction","showlegend":true,"x":["2019-03-01T00:00:00","2017-06-01T00:00:00"],"xaxis":"x","y":["Image reconstruction","Image reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image recognition","showlegend":true,"x":["2017-03-01T00:00:00","2015-03-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2018-09-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2018-01-01T00:00:00"],"xaxis":"x","y":["Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image question answering<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image question answering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image question answering","showlegend":true,"x":["2017-05-01T00:00:00","2019-02-01T00:00:00","2018-10-01T00:00:00","2018-05-01T00:00:00","2018-03-01T00:00:00","2017-09-01T00:00:00","2017-08-01T00:00:00","2016-03-01T00:00:00","2017-04-01T00:00:00","2016-11-01T00:00:00","2016-06-01T00:00:00","2016-05-01T00:00:00","2016-04-01T00:00:00","2019-05-01T00:00:00","2015-11-01T00:00:00","2019-04-01T00:00:00","2017-07-01T00:00:00","2019-06-01T00:00:00","2021-01-01T00:00:00","2019-07-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-07-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2020-12-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2020-02-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00"],"xaxis":"x","y":["Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image quality assessment<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image quality assessment","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image quality assessment","showlegend":true,"x":["2018-10-01T00:00:00","2017-04-01T00:00:00","2016-04-01T00:00:00"],"xaxis":"x","y":["Image quality assessment","Image quality assessment","Image quality assessment"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image matting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image matting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image matting","showlegend":true,"x":["2020-03-01T00:00:00","2019-01-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-01-01T00:00:00","2020-10-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-09-01T00:00:00"],"xaxis":"x","y":["Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image matching<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image matching","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image matching","showlegend":true,"x":["2019-11-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image generation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image generation","showlegend":true,"x":["2021-05-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2018-11-01T00:00:00","2018-09-01T00:00:00","2018-08-01T00:00:00","2018-07-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-03-01T00:00:00","2018-02-01T00:00:00","2017-12-01T00:00:00","2017-11-01T00:00:00","2017-10-01T00:00:00","2017-09-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2017-03-01T00:00:00","2017-02-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-06-01T00:00:00","2016-01-01T00:00:00","2015-12-01T00:00:00","2014-06-01T00:00:00","2018-10-01T00:00:00","2016-10-01T00:00:00","2018-12-01T00:00:00","2020-03-01T00:00:00","2019-01-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2020-10-01T00:00:00","2020-08-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-05-01T00:00:00","2020-04-01T00:00:00","2020-11-01T00:00:00","2020-02-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-03-01T00:00:00","2019-07-01T00:00:00","2019-05-01T00:00:00","2019-04-01T00:00:00"],"xaxis":"x","y":["Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image enhancement<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image enhancement","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image enhancement","showlegend":true,"x":["2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-01-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00"],"xaxis":"x","y":["Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image denoising<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image denoising","showlegend":true,"x":["2019-07-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2021-12-01T00:00:00","2019-08-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2019-10-01T00:00:00"],"xaxis":"x","y":["Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image clustering<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image clustering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image clustering","showlegend":true,"x":["2017-10-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-07-01T00:00:00","2018-04-01T00:00:00","2016-05-01T00:00:00","2017-09-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2015-11-01T00:00:00","2019-04-01T00:00:00","2019-01-01T00:00:00","2021-05-01T00:00:00","2019-08-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-09-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2019-09-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00"],"xaxis":"x","y":["Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Unsupervised image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Unsupervised image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Unsupervised image classification","showlegend":true,"x":["2020-12-01T00:00:00","2017-02-01T00:00:00","2018-02-01T00:00:00","2018-07-01T00:00:00","2020-05-01T00:00:00","2021-08-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00"],"xaxis":"x","y":["Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Superpixel image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Superpixel image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Superpixel image classification","showlegend":true,"x":["2020-03-01T00:00:00","2020-02-01T00:00:00","2019-05-01T00:00:00","2017-11-01T00:00:00"],"xaxis":"x","y":["Image classification // Superpixel image classification","Image classification // Superpixel image classification","Image classification // Superpixel image classification","Image classification // Superpixel image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Sequential image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Sequential image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Sequential image classification","showlegend":true,"x":["2019-10-01T00:00:00","2015-11-01T00:00:00","2017-10-01T00:00:00","2018-03-01T00:00:00","2018-10-01T00:00:00","2016-03-01T00:00:00","2020-06-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Semi-supervised image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Semi-supervised image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Semi-supervised image classification","showlegend":true,"x":["2019-05-01T00:00:00","2016-06-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2017-04-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2021-03-01T00:00:00","2019-09-01T00:00:00"],"xaxis":"x","y":["Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Self-supervised image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Self-supervised image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Self-supervised image classification","showlegend":true,"x":["2020-05-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-02-01T00:00:00","2021-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2018-03-01T00:00:00"],"xaxis":"x","y":["Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Satellite image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Satellite image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Satellite image classification","showlegend":true,"x":["2019-11-01T00:00:00","2015-12-01T00:00:00"],"xaxis":"x","y":["Image classification // Satellite image classification","Image classification // Satellite image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Photo geolocation estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Photo geolocation estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Photo geolocation estimation","showlegend":true,"x":["2018-08-01T00:00:00","2017-05-01T00:00:00","2018-09-01T00:00:00"],"xaxis":"x","y":["Image classification // Photo geolocation estimation","Image classification // Photo geolocation estimation","Image classification // Photo geolocation estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Learning with noisy labels<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Learning with noisy labels","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Learning with noisy labels","showlegend":true,"x":["2020-02-01T00:00:00","2021-11-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2020-10-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2018-04-01T00:00:00"],"xaxis":"x","y":["Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Hyperspectral image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Hyperspectral image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Hyperspectral image classification","showlegend":true,"x":["2019-02-01T00:00:00","2018-07-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00"],"xaxis":"x","y":["Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Fine-grained image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Fine-grained image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Fine-grained image classification","showlegend":true,"x":["2018-11-01T00:00:00","2017-12-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2018-01-01T00:00:00","2019-03-01T00:00:00","2017-05-01T00:00:00","2016-11-01T00:00:00","2015-12-01T00:00:00","2014-07-01T00:00:00","2019-05-01T00:00:00","2019-01-01T00:00:00","2020-03-01T00:00:00","2019-06-01T00:00:00","2020-10-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-03-01T00:00:00","2021-01-01T00:00:00","2021-07-01T00:00:00","2020-07-01T00:00:00","2020-02-01T00:00:00","2019-12-01T00:00:00","2019-10-01T00:00:00"],"xaxis":"x","y":["Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Few-shot image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Few-shot image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Few-shot image classification","showlegend":true,"x":["2018-12-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2019-05-01T00:00:00","2019-04-01T00:00:00","2019-03-01T00:00:00","2019-02-01T00:00:00","2019-01-01T00:00:00","2014-09-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2017-06-01T00:00:00","2017-03-01T00:00:00","2016-05-01T00:00:00","2016-03-01T00:00:00","2019-11-01T00:00:00","2019-10-01T00:00:00","2017-11-01T00:00:00","2019-12-01T00:00:00","2020-12-01T00:00:00","2020-01-01T00:00:00","2021-11-01T00:00:00","2021-07-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2021-10-01T00:00:00","2020-11-01T00:00:00","2020-09-01T00:00:00","2020-02-01T00:00:00","2020-08-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-10-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification // Document image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification // Document image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification // Document image classification","showlegend":true,"x":["2019-12-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2020-12-01T00:00:00","2021-06-01T00:00:00"],"xaxis":"x","y":["Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Image classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Image classification","showlegend":true,"x":["2016-11-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2017-12-01T00:00:00","2018-06-01T00:00:00","2017-11-01T00:00:00","2017-10-01T00:00:00","2017-09-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-01-01T00:00:00","2014-06-01T00:00:00","2016-10-01T00:00:00","2016-08-01T00:00:00","2016-05-01T00:00:00","2016-03-01T00:00:00","2016-02-01T00:00:00","2015-12-01T00:00:00","2015-09-01T00:00:00","2015-06-01T00:00:00","2015-02-01T00:00:00","2014-12-01T00:00:00","2014-09-01T00:00:00","2018-08-01T00:00:00","2013-12-01T00:00:00","2013-02-01T00:00:00","2018-07-01T00:00:00","2019-11-01T00:00:00","2018-11-01T00:00:00","2020-08-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-06-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2018-12-01T00:00:00","2020-11-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2020-07-01T00:00:00","2019-07-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2020-06-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00"],"xaxis":"x","y":["Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Human-object interaction detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Human-object interaction detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Human-object interaction detection","showlegend":true,"x":["2020-04-01T00:00:00","2016-04-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Human parsing<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Human parsing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Human parsing","showlegend":true,"x":["2017-05-01T00:00:00","2017-09-01T00:00:00","2018-04-01T00:00:00"],"xaxis":"x","y":["Human parsing","Human parsing","Human parsing"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Human interaction recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Human interaction recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Human interaction recognition","showlegend":true,"x":["2017-04-01T00:00:00","2017-05-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2017-10-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2017-03-01T00:00:00","2018-11-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-06-01T00:00:00","2021-07-01T00:00:00","2018-06-01T00:00:00","2018-01-01T00:00:00","2018-12-01T00:00:00","2020-04-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2020-12-01T00:00:00","2019-04-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-10-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-09-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2019-05-01T00:00:00"],"xaxis":"x","y":["Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Horizon line estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Horizon line estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Horizon line estimation","showlegend":true,"x":["2016-08-01T00:00:00","2018-09-01T00:00:00"],"xaxis":"x","y":["Horizon line estimation","Horizon line estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Hand-related vision process<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Hand-related vision process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Hand-related vision process","showlegend":true,"x":["2017-12-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-05-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2016-09-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-06-01T00:00:00","2018-08-01T00:00:00","2018-06-01T00:00:00","2020-06-01T00:00:00","2018-12-01T00:00:00","2020-07-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2019-01-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2020-12-01T00:00:00","2020-10-01T00:00:00","2020-08-01T00:00:00","2021-06-01T00:00:00","2019-05-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Gesture recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Gesture recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Gesture recognition","showlegend":true,"x":["2017-10-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2018-05-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2016-07-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-06-01T00:00:00","2018-04-01T00:00:00","2021-06-01T00:00:00","2018-06-01T00:00:00","2019-12-01T00:00:00","2021-07-01T00:00:00","2021-04-01T00:00:00","2020-10-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2021-05-01T00:00:00","2019-11-01T00:00:00","2019-04-01T00:00:00","2019-09-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00"],"xaxis":"x","y":["Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Emotion recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Emotion recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Emotion recognition","showlegend":true,"x":["2020-05-01T00:00:00","2015-01-01T00:00:00","2017-07-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2018-06-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-10-01T00:00:00"],"xaxis":"x","y":["Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Emotion classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Emotion classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Emotion classification","showlegend":true,"x":["2021-01-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Emotion classification","Emotion classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Edge detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Edge detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Edge detection","showlegend":true,"x":["2020-11-01T00:00:00","2019-02-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Edge detection","Edge detection","Edge detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Domain adaptation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Domain adaptation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Domain adaptation","showlegend":true,"x":["2019-05-01T00:00:00","2019-03-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2018-07-01T00:00:00","2017-10-01T00:00:00","2018-06-01T00:00:00","2018-04-01T00:00:00","2018-03-01T00:00:00","2017-12-01T00:00:00","2021-06-01T00:00:00","2017-09-01T00:00:00","2017-08-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2016-08-01T00:00:00","2016-05-01T00:00:00","2015-05-01T00:00:00","2015-02-01T00:00:00","2014-09-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2021-12-01T00:00:00","2019-08-01T00:00:00","2020-11-01T00:00:00","2019-09-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2020-12-01T00:00:00","2021-08-01T00:00:00","2020-07-01T00:00:00","2020-05-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2020-02-01T00:00:00","2020-01-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2020-06-01T00:00:00"],"xaxis":"x","y":["Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Depth estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Depth estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Depth estimation","showlegend":true,"x":["2018-10-01T00:00:00","2019-06-01T00:00:00","2019-05-01T00:00:00","2019-04-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2016-07-01T00:00:00","2018-06-01T00:00:00","2018-03-01T00:00:00","2017-04-01T00:00:00","2015-11-01T00:00:00","2019-10-01T00:00:00","2019-07-01T00:00:00","2020-06-01T00:00:00","2019-11-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Depth completion<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Depth completion","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Depth completion","showlegend":true,"x":["2019-02-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-01-01T00:00:00","2020-07-01T00:00:00","2019-05-01T00:00:00","2021-03-01T00:00:00","2021-07-01T00:00:00"],"xaxis":"x","y":["Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Denoising<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Denoising","showlegend":true,"x":["2019-07-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2016-08-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2021-05-01T00:00:00","2019-08-01T00:00:00","2021-06-01T00:00:00","2019-10-01T00:00:00","2021-11-01T00:00:00","2021-08-01T00:00:00","2021-12-01T00:00:00","2020-12-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00"],"xaxis":"x","y":["Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Dehazing<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Dehazing","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Dehazing","showlegend":true,"x":["2019-11-01T00:00:00","2016-05-01T00:00:00","2017-08-01T00:00:00","2018-03-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-08-01T00:00:00","2021-05-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00"],"xaxis":"x","y":["Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Deception detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Deception detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Deception detection","showlegend":true,"x":["2015-11-01T00:00:00","2019-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00"],"xaxis":"x","y":["Deception detection","Deception detection","Deception detection","Deception detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Crowds<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Crowds","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Crowds","showlegend":true,"x":["2018-07-01T00:00:00","2015-11-01T00:00:00","2015-12-01T00:00:00","2016-01-01T00:00:00","2016-08-01T00:00:00","2017-07-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2017-08-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2021-07-01T00:00:00"],"xaxis":"x","y":["Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Continual learning<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Continual learning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Continual learning","showlegend":true,"x":["2019-09-01T00:00:00","2017-05-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-03-01T00:00:00","2018-09-01T00:00:00","2019-02-01T00:00:00","2019-06-01T00:00:00","2016-11-01T00:00:00","2019-10-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Blind face restoration<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Blind face restoration","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Blind face restoration","showlegend":true,"x":["2020-05-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00"],"xaxis":"x","y":["Blind face restoration","Blind face restoration","Blind face restoration"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Biomedical vision process<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Biomedical vision process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Biomedical vision process","showlegend":true,"x":["2015-11-01T00:00:00","2019-07-01T00:00:00","2018-12-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2018-02-01T00:00:00","2017-11-01T00:00:00","2017-09-01T00:00:00","2017-07-01T00:00:00","2017-03-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-02-01T00:00:00","2019-10-01T00:00:00","2019-08-01T00:00:00","2021-11-01T00:00:00","2019-12-01T00:00:00","2021-01-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-05-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2020-01-01T00:00:00","2020-10-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-04-01T00:00:00","2020-03-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Autonomous vehicle task<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Autonomous vehicle task","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Autonomous vehicle task","showlegend":true,"x":["2018-02-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-05-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2017-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2017-06-01T00:00:00","2017-02-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2018-12-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-10-01T00:00:00","2015-06-01T00:00:00","2015-01-01T00:00:00","2018-09-01T00:00:00","2015-07-01T00:00:00","2019-01-01T00:00:00","2020-07-01T00:00:00","2019-03-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2020-10-01T00:00:00","2020-08-01T00:00:00","2021-03-01T00:00:00","2020-06-01T00:00:00","2019-10-01T00:00:00","2019-04-01T00:00:00","2020-05-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00"],"xaxis":"x","y":["Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Anomaly detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Anomaly detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Anomaly detection","showlegend":true,"x":["2017-01-01T00:00:00","2020-07-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-02-01T00:00:00","2020-05-01T00:00:00","2020-01-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-10-01T00:00:00"],"xaxis":"x","y":["Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Ad-hoc video search<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Ad-hoc video search","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Ad-hoc video search","showlegend":true,"x":["2020-11-01T00:00:00","2020-09-01T00:00:00","2021-12-01T00:00:00"],"xaxis":"x","y":["Ad-hoc video search","Ad-hoc video search","Ad-hoc video search"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Activity recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Activity recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Activity recognition","showlegend":true,"x":["2017-08-01T00:00:00","2018-11-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-12-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2017-12-01T00:00:00","2017-11-01T00:00:00","2017-10-01T00:00:00","2016-08-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2014-12-01T00:00:00","2016-01-01T00:00:00","2015-06-01T00:00:00","2015-05-01T00:00:00","2015-03-01T00:00:00","2019-04-01T00:00:00","2019-03-01T00:00:00","2016-04-01T00:00:00","2019-05-01T00:00:00","2020-11-01T00:00:00","2019-06-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2016-09-01T00:00:00","2020-10-01T00:00:00","2020-01-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-08-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action quality assessment<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action quality assessment","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action quality assessment","showlegend":true,"x":["2021-08-01T00:00:00","2021-02-01T00:00:00","2020-06-01T00:00:00"],"xaxis":"x","y":["Action quality assessment","Action quality assessment","Action quality assessment"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // Weakly supervised action localization<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // Weakly supervised action localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // Weakly supervised action localization","showlegend":true,"x":["2019-11-01T00:00:00","2017-12-01T00:00:00","2018-07-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // Temporal action proposal generation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // Temporal action proposal generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // Temporal action proposal generation","showlegend":true,"x":["2020-09-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00","2020-11-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // Activity recognition in videos<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // Activity recognition in videos","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // Activity recognition in videos","showlegend":true,"x":["2016-05-01T00:00:00","2015-05-01T00:00:00","2014-12-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Activity recognition in videos"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // Action recognition in videos<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // Action recognition in videos","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // Action recognition in videos","showlegend":true,"x":["2019-11-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-08-01T00:00:00","2018-11-01T00:00:00","2019-09-01T00:00:00","2021-04-01T00:00:00","2020-04-01T00:00:00","2020-08-01T00:00:00","2021-03-01T00:00:00","2020-06-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // Action recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // Action recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // Action recognition","showlegend":true,"x":["2018-04-01T00:00:00","2017-11-01T00:00:00","2015-03-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2017-08-01T00:00:00","2018-07-01T00:00:00","2014-12-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2017-10-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-08-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2016-01-01T00:00:00","2015-06-01T00:00:00","2015-05-01T00:00:00","2019-03-01T00:00:00","2018-12-01T00:00:00","2019-08-01T00:00:00","2019-04-01T00:00:00","2020-12-01T00:00:00","2019-05-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-02-01T00:00:00","2021-01-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00","2019-12-01T00:00:00","2020-10-01T00:00:00","2019-07-01T00:00:00","2019-06-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-09-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization // 3D human action recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization // 3D human action recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization // 3D human action recognition","showlegend":true,"x":["2018-02-01T00:00:00","2018-01-01T00:00:00","2017-10-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2016-07-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2018-05-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-06-01T00:00:00","2018-04-01T00:00:00","2018-12-01T00:00:00","2018-06-01T00:00:00","2020-07-01T00:00:00","2021-07-01T00:00:00","2019-04-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2020-10-01T00:00:00","2021-06-01T00:00:00","2020-03-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2019-06-01T00:00:00","2020-06-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Temporal action localization<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Temporal action localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Temporal action localization","showlegend":true,"x":["2019-09-01T00:00:00","2016-01-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2016-09-01T00:00:00","2019-07-01T00:00:00","2019-11-01T00:00:00","2021-03-01T00:00:00","2021-09-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2020-11-01T00:00:00","2020-09-01T00:00:00"],"xaxis":"x","y":["Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action localization // Action segmentation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action localization // Action segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action localization // Action segmentation","showlegend":true,"x":["2020-03-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2018-06-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2020-07-01T00:00:00"],"xaxis":"x","y":["Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=Action detection<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"Action detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"Action detection","showlegend":true,"x":["2018-01-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2017-06-01T00:00:00","2018-05-01T00:00:00","2017-07-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-09-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2015-06-01T00:00:00","2018-12-01T00:00:00","2018-06-01T00:00:00","2019-06-01T00:00:00","2019-04-01T00:00:00","2020-10-01T00:00:00","2021-11-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2021-01-01T00:00:00","2020-11-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2020-01-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-09-01T00:00:00","2019-07-01T00:00:00","2019-05-01T00:00:00"],"xaxis":"x","y":["Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // Motion forecasting<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // Motion forecasting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // Motion forecasting","showlegend":true,"x":["2021-03-01T00:00:00","2021-01-01T00:00:00","2021-11-01T00:00:00","2021-05-01T00:00:00"],"xaxis":"x","y":["3D vision process // Motion forecasting","3D vision process // Motion forecasting","3D vision process // Motion forecasting","3D vision process // Motion forecasting"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D shape reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D shape reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D shape reconstruction","showlegend":true,"x":["2017-07-01T00:00:00","2018-11-01T00:00:00","2018-09-01T00:00:00","2018-07-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-03-01T00:00:00","2018-02-01T00:00:00","2016-12-01T00:00:00","2019-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-08-01T00:00:00","2016-06-01T00:00:00","2016-03-01T00:00:00","2015-11-01T00:00:00","2015-08-01T00:00:00","2019-02-01T00:00:00","2020-11-01T00:00:00","2019-04-01T00:00:00","2020-07-01T00:00:00","2019-06-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-08-01T00:00:00","2021-06-01T00:00:00","2020-12-01T00:00:00","2020-08-01T00:00:00","2021-05-01T00:00:00","2020-05-01T00:00:00","2020-03-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-08-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D shape classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D shape classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D shape classification","showlegend":true,"x":["2017-11-01T00:00:00","2018-04-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D shape classification","3D vision process // 3D shape classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D reconstruction","showlegend":true,"x":["2016-06-01T00:00:00","2019-08-01T00:00:00","2017-12-01T00:00:00","2018-04-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-01-01T00:00:00","2016-12-01T00:00:00","2020-05-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-09-01T00:00:00","2021-12-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D human reconstruction<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D human reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D human reconstruction","showlegend":true,"x":["2021-08-01T00:00:00","2020-08-01T00:00:00","2020-06-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D human reconstruction","3D vision process // 3D human reconstruction","3D vision process // 3D human reconstruction"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D human pose estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D human pose estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D human pose estimation","showlegend":true,"x":["2014-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-07-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2017-07-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2017-09-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-01-01T00:00:00","2016-12-01T00:00:00","2016-10-01T00:00:00","2016-09-01T00:00:00","2016-08-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2016-03-01T00:00:00","2015-11-01T00:00:00","2015-09-01T00:00:00","2015-08-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2019-09-01T00:00:00","2019-02-01T00:00:00","2020-10-01T00:00:00","2019-03-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-10-01T00:00:00","2021-09-01T00:00:00","2021-08-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-03-01T00:00:00","2020-12-01T00:00:00","2020-11-01T00:00:00","2021-07-01T00:00:00","2020-08-01T00:00:00","2019-10-01T00:00:00","2020-07-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D human action recognition<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D human action recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D human action recognition","showlegend":true,"x":["2018-02-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2017-10-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2017-06-01T00:00:00","2017-05-01T00:00:00","2016-04-01T00:00:00","2017-03-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-07-01T00:00:00","2016-06-01T00:00:00","2015-06-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2019-06-01T00:00:00","2018-06-01T00:00:00","2020-06-01T00:00:00","2018-12-01T00:00:00","2021-07-01T00:00:00","2021-06-01T00:00:00","2021-05-01T00:00:00","2020-10-01T00:00:00","2020-07-01T00:00:00","2021-04-01T00:00:00","2020-03-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-09-01T00:00:00","2019-07-01T00:00:00","2019-04-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D car instance understanding<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D car instance understanding","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D car instance understanding","showlegend":true,"x":["2017-06-01T00:00:00","2018-06-01T00:00:00","2018-05-01T00:00:00","2018-04-01T00:00:00","2018-02-01T00:00:00","2018-01-01T00:00:00","2017-11-01T00:00:00","2017-10-01T00:00:00","2017-08-01T00:00:00","2017-07-01T00:00:00","2016-07-01T00:00:00","2015-06-01T00:00:00","2017-05-01T00:00:00","2017-04-01T00:00:00","2017-03-01T00:00:00","2016-12-01T00:00:00","2016-11-01T00:00:00","2016-09-01T00:00:00","2016-06-01T00:00:00","2016-04-01T00:00:00","2019-01-01T00:00:00","2018-12-01T00:00:00","2020-08-01T00:00:00","2019-03-01T00:00:00","2020-10-01T00:00:00","2019-04-01T00:00:00","2021-12-01T00:00:00","2021-11-01T00:00:00","2021-07-01T00:00:00","2021-05-01T00:00:00","2021-04-01T00:00:00","2021-01-01T00:00:00","2020-12-01T00:00:00","2021-06-01T00:00:00","2020-07-01T00:00:00","2020-06-01T00:00:00","2020-03-01T00:00:00","2019-12-01T00:00:00","2019-11-01T00:00:00","2019-06-01T00:00:00","2019-10-01T00:00:00","2019-09-01T00:00:00","2019-07-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D Shape Representation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D Shape Representation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D Shape Representation","showlegend":true,"x":["2019-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-10-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D Depth Estimation<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D Depth Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D Depth Estimation","showlegend":true,"x":["2021-12-01T00:00:00","2020-08-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D Depth Estimation","3D vision process // 3D Depth Estimation"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D vision process // 3D Classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D vision process // 3D Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D vision process // 3D Classification","showlegend":true,"x":["2019-04-01T00:00:00","2019-03-01T00:00:00","2018-11-01T00:00:00","2018-04-01T00:00:00","2018-10-01T00:00:00","2018-09-01T00:00:00","2018-08-01T00:00:00","2016-11-01T00:00:00","2018-03-01T00:00:00","2018-02-01T00:00:00","2017-11-01T00:00:00","2017-07-01T00:00:00","2016-12-01T00:00:00","2019-07-01T00:00:00","2019-05-01T00:00:00","2021-09-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-12-01T00:00:00"],"xaxis":"x","y":["3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":"color=3D Point Cloud Linear Classification<br>x=%{x}<br>y=%{y}<extra></extra>","legendgroup":"3D Point Cloud Linear Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"gray","width":1}},"mode":"lines","name":"3D Point Cloud Linear Classification","showlegend":true,"x":["2020-08-01T00:00:00","2019-01-01T00:00:00","2021-09-01T00:00:00","2017-12-01T00:00:00"],"xaxis":"x","y":["3D Point Cloud Linear Classification","3D Point Cloud Linear Classification","3D Point Cloud Linear Classification","3D Point Cloud Linear Classification"],"yaxis":"y","type":"scattergl"},{"hovertemplate":["<BR>task: 3D Point Cloud Linear Classification<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  3D Point Cloud Linear Classification - ModelNet40: Overall Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Depth Estimation<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>","<BR>task: 3D vision process // 3D Shape Representation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D Dense Shape Correspondence - SHREC'19: Accuracy at 1%<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-09-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-01-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-07-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>  3D human pose estimation - Surreal: MPJPE<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  3D multi-person pose estimation (absolute) - MuPoTS-3D: 3DPCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  3D multi-person pose estimation - MuPoTS-3D: 3DPCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPVPE<BR>  3D human pose estimation - 3DPW: Number of parameters (M)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D human pose estimation - CMU Panoptic: Average MPJPE (mm)<BR>  3D multi-person pose estimation - CMU Panoptic Studio Dataset: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-01-01<BR>Anchor.<BR>benchmarks:<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: 3D vision process // 3D human reconstruction<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D reconstruction - DTU: Acc<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D reconstruction - Data3D\u2212R2N2: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Scene Completion - NYUv2: mIoU<BR>  3D Semantic Scene Completion - SemanticKITTI: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised 3D Human Pose Estimation - Human3.6M: MPJPE<BR>","<BR>task: 3D vision process // 3D shape classification<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  3D shape classification - Pix3D: R-at-16<BR>  3D shape classification - Pix3D: R-at-1<BR>  3D shape classification - Pix3D: R-at-2<BR>  3D shape classification - Pix3D: R-at-32<BR>  3D shape classification - Pix3D: R-at-4<BR>  3D shape classification - Pix3D: R-at-8<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-01-01<BR>Anchor.<BR>benchmarks:<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: 3D vision process // Motion forecasting<BR>date: 2020-08-01<BR>Anchor.<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>","<BR>task: Action detection<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action detection<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action detection<BR>date: 2015-07-01<BR>Anchor.<BR>benchmarks:<BR>  Action detection - Multi-THUMOS: mAP<BR>","<BR>task: Action detection<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action detection<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action detection<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action detection - UCF101-24: Video-mAP 0.2<BR>","<BR>task: Action detection<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action detection<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action detection<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action detection<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Online Action Detection - THUMOS'14: mAP<BR>  Online Action Detection - TVSeries: mCAP<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Audio-visual active speaker detection - AVA-ActiveSpeaker: validation mean average precision<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2016-02-01<BR>Anchor.<BR>benchmarks:<BR>  Action segmentation - GTEA: Acc<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>  Action segmentation - GTEA: F1@50%<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Segmentation (Transcript) - Breakfast: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action segmentation - GTEA: Edit<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - 50 Salads: Edit<BR>  Action segmentation - 50 Salads: F1@10%<BR>  Action segmentation - 50 Salads: F1@25%<BR>  Action segmentation - 50 Salads: F1@50%<BR>  Action segmentation - Breakfast: Acc<BR>  Action segmentation - Breakfast: Edit<BR>  Action segmentation - Breakfast: F1@10%<BR>  Action segmentation - Breakfast: F1@25%<BR>  Action segmentation - Breakfast: F1@50%<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - CrossTask: Recall<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.1<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - J-HMDB-21: Frame-mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - UCF101-24: Frame-mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: Avg mAP (0.3:0.7)<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.75<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.95<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Diving-48: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Activity recognition in videos<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (test)<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP@0.5:0.95<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised action localization - BEOID: mAP-at-0.5<BR>  Weakly supervised action localization - BEOID: mAP@0.1:0.7<BR>  Weakly supervised action localization - GTEA: mAP-at-0.5<BR>  Weakly supervised action localization - GTEA: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1-0.5)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1:0.7)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.3-0.7)<BR>","<BR>task: Action quality assessment<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action quality assessment - AQA-7: Spearman Correlation<BR>","<BR>task: Action quality assessment<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Action quality assessment - MTL-AQA: Spearman Correlation<BR>","<BR>task: Activity recognition<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Activity recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Group activity recognition - Collective Activity: Accuracy<BR>  Multimodal activity recognition - EV-Action: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Group activity recognition - Volleyball: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Activity recognition<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>","<BR>task: Activity recognition<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Activity recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Multimodal activity recognition - UTD-MHAD: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Egocentric activity recognition - EGTEA: Average Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S2)<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Diving-48: Accuracy<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S1)<BR>","<BR>task: Activity recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Ad-hoc video search<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Ad-hoc video search - TRECVID-AVS16 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS17 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS18 (IACC.3): infAP<BR>","<BR>task: Anomaly detection<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>","<BR>task: Anomaly detection<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Anomaly detection<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Detection AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - ShanghaiTech: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: RBDC<BR>  Anomaly detection - CHUK Avenue: TBDC<BR>  Anomaly detection - ShanghaiTech: RBDC<BR>  Anomaly detection - ShanghaiTech: TBDC<BR>","<BR>task: Anomaly detection<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - UBnormal: AUC<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Anomaly detection<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - One-class CIFAR-100: AUROC<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Fishyscapes: AP<BR>  Anomaly detection - Fishyscapes: FPR<BR>","<BR>task: Anomaly detection<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Fishyscapes L&F: AP<BR>  Anomaly detection - Fishyscapes L&F: FPR95<BR>","<BR>task: Anomaly detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Anomaly Detection on Anomaly Detection on Unlabeled ImageNet-30 vs Flowers-102: ROC-AUC<BR>  Anomaly detection - Anomaly Detection on Unlabeled ImageNet-30 vs CUB-200: ROC-AUC<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Road Anomaly: AP<BR>","<BR>task: Anomaly detection<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - Anomaly Detection on Unlabeled CIFAR-10 vs LSUN (Fix): ROC-AUC<BR>","<BR>task: Anomaly detection<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection - CIFAR-10: ROC AUC<BR>","<BR>task: Autonomous vehicle task<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian detection - TJU-Ped-traffic: HO (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R+HO (miss rate)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian detection - CityPersons: Reasonable MR^-2<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Lane detection - TuSimple: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-09-01<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian attribute recognition - PA-100K: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Lane detection - TuSimple: F1 score<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Autonomous driving - CARLA Leaderboard: Driving Score<BR>  Autonomous driving - CARLA Leaderboard: Route Completion<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Lane detection - LLAMAS: F1<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-08-01<BR>Anchor.<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-10-01<BR>Anchor.<BR>benchmarks:<BR>  CARLA MAP Leaderboard - CARLA: Driving score<BR>  CARLA MAP Leaderboard - CARLA: Infraction penalty<BR>  CARLA MAP Leaderboard - CARLA: Route completion<BR>","<BR>task: Biomedical vision process<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-tissue nucleus segmentation - Kumar: Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Brain tumor segmentation - BRATS-2013: Dice Score<BR>  Colorectal gland segmentation - CRAG: Dice<BR>  Colorectal gland segmentation - CRAG: F1-score<BR>  Colorectal gland segmentation - CRAG: Hausdorff Distance (mm)<BR>  Lung nodule segmentation - LUNA: AUC<BR>  Lung nodule segmentation - LUNA: F1 score<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - Kvasir-SEG: Average MAE<BR>  Medical image segmentation - Kvasir-SEG: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: max E-Measure<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Medical image segmentation - RITE: Jaccard Index<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>  Retinal vessel segmentation - DRIVE: F1 score<BR>  Retinal vessel segmentation - STARE: F1 score<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: AUC<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: F1 score<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): S measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): weighted F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): S-Measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean E-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): weighted F-measure<BR>","<BR>task: Biomedical vision process<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Dice<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : IoU<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Precision<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Recall<BR>","<BR>task: Biomedical vision process<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Breast tumour classification - PCam: AUC<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Biomedical vision process<BR>date: 2016-02-01<BR>Anchor.<BR>benchmarks:<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Biomedical vision process<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Nuclear segmentation - Cell17: Dice<BR>  Nuclear segmentation - Cell17: F1-score<BR>  Nuclear segmentation - Cell17: Hausdorff<BR>","<BR>task: Biomedical vision process<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Medical image segmentation - 2018 Data Science Bowl: Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Medical image segmentation - Medical Segmentation Decathlon: Dice (Average)<BR>  Medical image segmentation - Medical Segmentation Decathlon: NSD<BR>  Medical image segmentation - Synapse multi-organ CT: Avg DSC<BR>","<BR>task: Biomedical vision process<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Lesion segmentation - ISIC 2018: Dice Score<BR>","<BR>task: Biomedical vision process<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Retinal vessel segmentation - DRIVE: Accuracy<BR>","<BR>task: Biomedical vision process<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Liver segmentation - LiTS2017: Dice<BR>  Liver segmentation - LiTS2017: IoU<BR>","<BR>task: Biomedical vision process<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: mean Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  Medical image segmentation - 2018 Data Science Bowl: Recall<BR>  Medical image segmentation - CVC-ColonDB: Average MAE<BR>  Medical image segmentation - CVC-ColonDB: S-Measure<BR>  Medical image segmentation - CVC-ColonDB: mIoU<BR>  Medical image segmentation - CVC-ColonDB: max E-Measure<BR>  Medical image segmentation - CVC-ColonDB: mean Dice<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: mIoU<BR>","<BR>task: Biomedical vision process<BR>date: 2021-02-01<BR>Anchor.<BR>benchmarks:<BR>  Medical image segmentation - Automatic Cardiac Diagnosis Challenge (ACDC): Avg DSC<BR>","<BR>task: Blind face restoration<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Blind face restoration - CelebA-Test: FID<BR>  Blind face restoration - CelebA-Test: NIQE<BR>","<BR>task: Continual learning<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Class-incremental learning - cifar100: 10-stage average accuracy<BR>  Continual learning - CUBS (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Flowers (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Sketch (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Stanford Cars (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Wikiart (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): Avg. Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): decathlon discipline (Score)<BR>","<BR>task: Continual learning<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Continual learning - 20Newsgroup (10 tasks): F1 - macro<BR>  Continual learning - ASC (19 tasks): F1 - macro<BR>  Continual learning - DSC (10 tasks): F1 - macro<BR>","<BR>task: Continual learning<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Continual learning - Cifar100 (20 tasks): Average Accuracy<BR>","<BR>task: Continual learning<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Continual learning - ImageNet-50 (5 tasks) : Accuracy<BR>","<BR>task: Crowds<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2016-01-01<BR>Anchor.<BR>benchmarks:<BR>  Crowd counting - Venice: MAE<BR>","<BR>task: Crowds<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MSE<BR>","<BR>task: Deception detection<BR>date: 2014-08-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Deception detection<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Dehazing<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Dehazing<BR>date: 2016-01-01<BR>Anchor.<BR>benchmarks:<BR>  Image dehazing - KITTI: PSNR<BR>","<BR>task: Dehazing<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>  Image dehazing - SOTS Outdoor: PSNR<BR>  Image dehazing - SOTS Outdoor: SSIM<BR>","<BR>task: Dehazing<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image dehazing - O-Haze: PSNR<BR>","<BR>task: Dehazing<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Denoising<BR>date: 2015-08-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>  Denoising - Darmstadt Noise Dataset: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>","<BR>task: Denoising<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>","<BR>task: Denoising<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Denoising<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Depth completion<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: MAE<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>  Depth completion - KITTI Depth Completion: Runtime [ms]<BR>","<BR>task: Depth completion<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Depth completion - VOID: MAE<BR>  Depth completion - VOID: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2014-08-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Depth estimation<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE log<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE<BR>  Monocular depth estimation - Mid-Air Dataset: SQ Rel<BR>","<BR>task: Depth estimation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Stereo-lidar fusion - KITTI Depth Completion Validation: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split: RMSE<BR>  Monocular depth estimation - Make3D: Abs Rel<BR>","<BR>task: Depth estimation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Depth estimation - DCM: RMSE log<BR>  Depth estimation - DCM: RMSE<BR>  Depth estimation - eBDtheque: RMSE log<BR>  Depth estimation - eBDtheque: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Stereo depth estimation - KITTI2015: three pixel error<BR>","<BR>task: Depth estimation<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25^2<BR>  Monocular depth estimation - NYU-Depth V2: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Depth estimation<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^3<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Sq Rel<BR>","<BR>task: Domain adaptation<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - HMDBsmall-to-UCF: Accuracy<BR>  Domain adaptation - Olympic-to-HMDBsmall: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - HMDBfull-to-UCF: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - Office-Caltech: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2015-02-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - ImageCLEF-DA: Accuracy<BR>  Domain adaptation - MNIST-to-MNIST-M: Accuracy<BR>  Domain adaptation - SVNH-to-MNIST: Accuracy<BR>  Domain adaptation - SYNSIG-to-GTSRB: Accuracy<BR>  Domain adaptation - Synth Digits-to-SVHN: Accuracy<BR>  Domain adaptation - Synth Signs-to-GTSRB: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - Office-31: Average Accuracy<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>  Domain generalization - ImageNet-R: Top-1 Error Rate<BR>","<BR>task: Domain adaptation<BR>date: 2016-05-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - VisDA2017: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2016-07-01<BR>Anchor.<BR>benchmarks:<BR>  Domain generalization - NICO Animal: Accuracy<BR>  Domain generalization - NICO Vehicle: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-USPS: Accuracy<BR>  Domain adaptation - SVHN-to-MNIST: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - USPS-to-MNIST: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>","<BR>task: Domain adaptation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - Synscapes-to-Cityscapes: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised domain adaptation - Cityscapes to Foggy Cityscapes: mAP-at-0.5<BR>","<BR>task: Domain adaptation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Partial domain adaptation - DomainNet: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>  Partial domain adaptation - Office-Home: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Partial domain adaptation - ImageNet-Caltech: Accuracy (%)<BR>  Partial domain adaptation - Office-31: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - Office-Home: Accuracy<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised domain adaptation - Cityscapes-to-OxfordCar: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Domain adaptation - Cityscapes to ACDC: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Domain generalization - Office-Home: Average Accuracy<BR>  Domain generalization - VLCS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2020-08-01<BR>Anchor.<BR>benchmarks:<BR>  Domain generalization - DomainNet: Average Accuracy<BR>","<BR>task: Edge detection<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Edge detection - BIPED: Number of parameters (M)<BR>  Edge detection - BIPED: ODS<BR>  Edge detection - MDBD: ODS<BR>","<BR>task: Emotion classification<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion classification - SemEval 2018 Task 1E-c: Macro-F1<BR>","<BR>task: Emotion recognition<BR>date: 2014-08-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion recognition in conversation - CPED: Accuracy of Sentiment<BR>  Emotion recognition in conversation - CPED: Macro-F1 of Sentiment<BR>","<BR>task: Emotion recognition<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>  Emotion recognition in conversation - IEMOCAP: Macro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Accuracy<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Expectancy)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Power)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Valence)<BR>","<BR>task: Emotion recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Speech emotion recognition - IEMOCAP: UA<BR>","<BR>task: Emotion recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion-cause pair extraction - ECPE: F1<BR>","<BR>task: Emotion recognition<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion recognition in conversation - DailyDialog: Micro-F1<BR>  Emotion recognition in conversation - EmoryNLP: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  Speech emotion recognition - CREMA-D: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2021-06-01<BR>Anchor.<BR>benchmarks:<BR>  Emotion recognition - RAVDESS: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Gesture recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Gesture recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Gesture recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>  Hand pose estimation - MSRA Hands: Average 3D Error<BR>  Hand pose estimation - NYU Hands: Average 3D Error<BR>","<BR>task: Hand-related vision process<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Gesture-to-gesture translation - NTU Hand Digit: AMT<BR>  Gesture-to-gesture translation - NTU Hand Digit: IS<BR>  Gesture-to-gesture translation - NTU Hand Digit: PSNR<BR>  Gesture-to-gesture translation - Senz3D: AMT<BR>  Gesture-to-gesture translation - Senz3D: IS<BR>  Gesture-to-gesture translation - Senz3D: PSNR<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Hand pose estimation - HANDS 2017: Average 3D Error<BR>","<BR>task: Hand-related vision process<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Hand-related vision process<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  3D hand pose estimation - FreiHAND: PA-MPVPE<BR>","<BR>task: Hand-related vision process<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-MPJPE<BR>","<BR>task: Horizon line estimation<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Horizon line estimation - Eurasian Cities Dataset: AUC (horizon error)<BR>  Horizon line estimation - York Urban Dataset: AUC (horizon error)<BR>","<BR>task: Human interaction recognition<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Human interaction recognition - UT: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Human interaction recognition - BIT: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  One-shot 3D action recognition - NTU RGB+D 120: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Human interaction recognition<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Human parsing<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-human parsing - PASCAL-Part: AP 0.5<BR>","<BR>task: Human parsing<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-human parsing - MHP v2.0: AP 0.5<BR>","<BR>task: Human-object interaction detection<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Human-object interaction detection - HICO: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Human-object interaction detection - Ambiguious-HOI: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Image classification<BR>date: 2013-01-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2013-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - MNIST: Accuracy<BR>  Image classification - MNIST: Percentage error<BR>","<BR>task: Image classification<BR>date: 2013-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2013-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: Number of params<BR>","<BR>task: Image classification<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - ImageNet: GFLOPs<BR>  Image classification - OmniBenchmark: Average Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Kuzushiji-MNIST: Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Colored-MNIST(with spurious correlation): Accuracy<BR>  Image classification - EMNIST-Balanced: Accuracy<BR>  Image classification - EMNIST-Digits: Accuracy (%)<BR>  Image classification - EMNIST-Letters: Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>  Image classification - iNaturalist: Top 1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Fashion-MNIST: Percentage error<BR>","<BR>task: Image classification<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - CIFAR-10: PARAMS<BR>  Image classification - smallNORB: Classification Error<BR>","<BR>task: Image classification<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - WebVision-1000: ImageNet Top-1 Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Clothing1M: Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - CIFAR-100: PARAMS<BR>","<BR>task: Image classification<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Tiny ImageNet Classification: Validation Acc<BR>","<BR>task: Image classification<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Flowers-102: Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - ObjectNet: Top-1 Accuracy<BR>  Image classification - ObjectNet: Top-5 Accuracy<BR>  Image classification - VTAB-1k: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Places205: Top 1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - SVHN: Percentage error<BR>  Image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image classification - Food-101: Accuracy (%)<BR>","<BR>task: Image classification // Document image classification<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2013-12-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2013-12-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - NABirds: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - Oxford 102 Flowers: Accuracy<BR>  Fine-grained image classification - Stanford Dogs: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - Caltech-101: Accuracy<BR>  Fine-grained image classification - Oxford 102 Flowers: Top-1 Error Rate<BR>  Fine-grained image classification - Oxford-IIIT Pets: Accuracy<BR>  Fine-grained image classification - Oxford-IIIT Pets: Top-1 Error Rate<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - Birdsnap: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Fine-grained image classification - Bird-225: Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral image classification - Indian Pines: Overall Accuracy<BR>  Hyperspectral image classification - Pavia University: Overall Accuracy<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-100N: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Aggregate: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random1: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random2: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random3: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Worst: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Learning with noisy labels - ANIMAL: Accuracy<BR>","<BR>task: Image classification // Photo geolocation estimation<BR>date: 2016-02-01<BR>Anchor.<BR>benchmarks:<BR>  Photo geolocation estimation - Im2GPS: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS: Street level (1 km)<BR>","<BR>task: Image classification // Photo geolocation estimation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Photo geolocation estimation - Im2GPS3k: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS3k: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS3k: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS3k: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS3k: Street level (1 km)<BR>","<BR>task: Image classification // Satellite image classification<BR>date: 2015-09-01<BR>Anchor.<BR>benchmarks:<BR>  Satellite image classification - SAT-4: Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Number of Params<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - STL-10, 1000 Labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2015-07-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>  Semi-supervised image classification - SVHN, 250 Labels: Accuracy<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 250 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 5 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - STL-10: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 1% labeled data: Top 5 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 1 Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Sequential image classification - Sequential CIFAR-10: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Sequential image classification - noise padded CIFAR-10: % Test Accuracy<BR>","<BR>task: Image classification // Superpixel image classification<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Superpixel image classification - 75 Superpixel MNIST: Classification Error<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised image classification - MNIST: Accuracy<BR>  Unsupervised image classification - SVHN: Acc<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised image classification - CIFAR-10: Accuracy<BR>  Unsupervised image classification - CIFAR-20: Accuracy<BR>  Unsupervised image classification - STL-10: Accuracy<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised image classification - ImageNet: ARI<BR>  Unsupervised image classification - ImageNet: Accuracy (%)<BR>","<BR>task: Image clustering<BR>date: 2013-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2015-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - Extended Yale-B: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - CMU-PIE: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - CUB Birds: Accuracy<BR>  Image clustering - CUB Birds: NMI<BR>  Image clustering - MNIST-full: NMI<BR>  Image clustering - MNIST-test: NMI<BR>  Image clustering - Stanford Cars: Accuracy<BR>  Image clustering - Stanford Cars: NMI<BR>  Image clustering - Stanford Dogs: Accuracy<BR>  Image clustering - Stanford Dogs: NMI<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2016-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - USPS: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - MNIST-test: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - Fashion-MNIST: NMI<BR>","<BR>task: Image clustering<BR>date: 2020-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image clustering - Imagenet-dog-15: ARI<BR>","<BR>task: Image denoising<BR>date: 2015-08-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>","<BR>task: Image denoising<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>","<BR>task: Image denoising<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Image denoising<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Image enhancement<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Low-light image enhancement - DICM: User Study Score<BR>  Low-light image enhancement - MEF: User Study Score<BR>  Low-light image enhancement - VV: User Study Score<BR>","<BR>task: Image enhancement<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image relighting - VIDIT\u201920 validation set: LPIPS<BR>  Image relighting - VIDIT\u201920 validation set: PSNR<BR>  Image relighting - VIDIT\u201920 validation set: SSIM<BR>","<BR>task: Image enhancement<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image enhancement - TIP 2018: PSNR<BR>  Image enhancement - TIP 2018: SSIM<BR>","<BR>task: Image generation<BR>date: 2013-10-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - Binarized MNIST: nats<BR>","<BR>task: Image generation<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: Image generation<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>","<BR>task: Image generation<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2016-01-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - ImageNet 32x32: bpd<BR>","<BR>task: Image generation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - CIFAR-10: Inception score<BR>  Image generation - CUB 128 x 128: FID<BR>  Image generation - ImageNet 64x64: Bits per dim<BR>  Image generation - Stanford Cars: FID<BR>  Image generation - Stanford Dogs: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>","<BR>task: Image generation<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Image generation<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>","<BR>task: Image generation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - CAT 256x256: FID<BR>  Image generation - CIFAR-10: FID<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>","<BR>task: Image generation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Pose transfer - Deep-Fashion: IS<BR>  Pose transfer - Deep-Fashion: SSIM<BR>","<BR>task: Image generation<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>","<BR>task: Image generation<BR>date: 2017-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - STL-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>  Image generation - CelebA-HQ 1024x1024: FID<BR>  Image generation - CelebA-HQ 256x256: FID<BR>  Image generation - FFHQ: FID<BR>  Image generation - LSUN Bedroom 256 x 256: FID<BR>  Image generation - LSUN Cat 256 x 256: FID<BR>  Image generation - LSUN Churches 256 x 256: FID<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Image generation - CelebA 64x64: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Image generation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Conditional image generation - ImageNet 128x128: FID<BR>  Image generation - STL-10: FID<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: Image generation<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>","<BR>task: Image generation<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Layout-to-image generation - COCO-Stuff 64x64: FID<BR>  Layout-to-image generation - COCO-Stuff 64x64: Inception Score<BR>  Layout-to-image generation - Visual Genome 64x64: FID<BR>  Layout-to-image generation - Visual Genome 64x64: Inception Score<BR>","<BR>task: Image generation<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - ImageNet 128x128: FID<BR>  Image generation - ImageNet 256x256: FID<BR>","<BR>task: Image generation<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - CelebA-HQ 128x128: FID<BR>","<BR>task: Image generation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Pose transfer - Market-1501: SSIM<BR>  Pose transfer - Market-1501: mask-SSIM<BR>","<BR>task: Image generation<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - CelebA 256x256: FID<BR>","<BR>task: Image generation<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Layout-to-image generation - COCO-Stuff 128x128: FID<BR>  Layout-to-image generation - COCO-Stuff 128x128: Inception Score<BR>  Layout-to-image generation - Visual Genome 128x128: FID<BR>  Layout-to-image generation - Visual Genome 128x128: Inception Score<BR>","<BR>task: Image generation<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: Image generation<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - ADE-Indoor: FID<BR>  Image generation - CIFAR-100: FID<BR>","<BR>task: Image generation<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - LSUN Horse 256 x 256: FID<BR>","<BR>task: Image generation<BR>date: 2020-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image generation - FFHQ 256 x 256: FID<BR>","<BR>task: Image matching<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image matching - IMC PhotoTourism: mean average accuracy @ 10<BR>","<BR>task: Image matching<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - PF-WILLOW: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matting<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image matting - Composition-1K: Conn<BR>  Image matting - Composition-1K: Grad<BR>  Image matting - Composition-1K: MSE<BR>  Semantic image matting - Semantic Image Matting Dataset: Conn<BR>  Semantic image matting - Semantic Image Matting Dataset: Grad<BR>  Semantic image matting - Semantic Image Matting Dataset: MSE(10^3)<BR>  Semantic image matting - Semantic Image Matting Dataset: SAD<BR>","<BR>task: Image matting<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image matting - AIM-500: MSE<BR>  Image matting - AIM-500: SAD<BR>  Image matting - AM-2K: MAD<BR>  Image matting - AM-2K: MSE<BR>  Image matting - P3M-10k: MSE<BR>  Image matting - P3M-10k: SAD<BR>","<BR>task: Image matting<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image matting - Composition-1K: SAD<BR>","<BR>task: Image quality assessment<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Aesthetics quality assessment - AVA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 multiple choice: Percentage correct<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 open ended: Percentage correct<BR>","<BR>task: Image question answering<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>  Image question answering - VQA v1 test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>  Image question answering - MSVD-QA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - CLEVR-Humans: Accuracy<BR>  Image question answering - CLEVR: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - GQA Test2019: Accuracy<BR>  Image question answering - GQA Test2019: Binary<BR>  Image question answering - GQA Test2019: Consistency<BR>  Image question answering - GQA Test2019: Open<BR>  Image question answering - GQA Test2019: Validity<BR>","<BR>task: Image question answering<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - VQA-CP: Score<BR>","<BR>task: Image question answering<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - GQA test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - GQA test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - VQA v2 test-std: number<BR>  Image question answering - VQA v2 test-std: other<BR>  Image question answering - VQA v2 test-std: yes/no<BR>","<BR>task: Image question answering<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image question answering - PlotQA-D2: 1:1 Accuracy<BR>  Image question answering - VCR (Q-A) test: Accuracy<BR>  Image question answering - VCR (Q-AR) test: Accuracy<BR>  Image question answering - VCR (QA-R) test: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Age-invariant face recognition - CACDVS: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2015-01-01<BR>Anchor.<BR>benchmarks:<BR>  Face recognition - CASIA-WebFace+masks: Accuracy<BR>  Face recognition - CelebA+masks: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2021-05-01<BR>Anchor.<BR>benchmarks:<BR>  Face recognition - AgeDB-30: Accuracy<BR>","<BR>task: Image reconstruction<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image reconstruction - Edge-to-Handbags: FID<BR>  Image reconstruction - Edge-to-Handbags: LPIPS<BR>  Image reconstruction - Edge-to-Shoes: FID<BR>  Image reconstruction - Edge-to-Shoes: LPIPS<BR>","<BR>task: Image restoration<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 30 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  JPEG artifact correction - Classic5 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 40 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 40 Grayscale): PSNR<BR>","<BR>task: Image restoration<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Underwater Image Restoration - LSUI: PSNR<BR>","<BR>task: Image retrieval<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-10<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-1<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-50<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-10<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-1<BR>  Image retrieval with multi-modal query - MIT-States: Recall@5<BR>","<BR>task: Image retrieval<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-10<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-1<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-5<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@10<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@1<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@5<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Text-image retrieval - COCO (image as query): Recall-at-10<BR>","<BR>task: Image retrieval<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - Oxf105k: MAP<BR>  Image retrieval - Par106k: mAP<BR>  Image retrieval - Par6k: mAP<BR>","<BR>task: Image retrieval<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - Oxf5k: MAP<BR>","<BR>task: Image retrieval<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - SOP: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - CARS196: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>","<BR>task: Image retrieval<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Cross-modal retrieval - Recipe1M: Image-to-text R-at-1<BR>  Cross-modal retrieval - Recipe1M: Text-to-image R@1<BR>  Image retrieval - In-Shop: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-20<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-50<BR>","<BR>task: Image retrieval<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Image retrieval - iNaturalist: R-at-16<BR>  Image retrieval - iNaturalist: R-at-1<BR>  Image retrieval - iNaturalist: R-at-32<BR>  Image retrieval - iNaturalist: R-at-5<BR>","<BR>task: Image tagging<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: METEOR<BR>","<BR>task: Image tagging<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>","<BR>task: Image tagging<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Image tagging<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Image tagging<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Image tagging<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>","<BR>task: Image tagging<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>","<BR>task: Image tagging<BR>date: 2021-01-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Image/document clustering<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image/document clustering - pendigits: Accuracy (%)<BR>","<BR>task: Image/document clustering<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Image/document clustering - pendigits: NMI<BR>","<BR>task: Instance segmentation<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: AP50<BR>","<BR>task: Instance segmentation<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: IoU mean<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: Precision@0.8<BR>","<BR>task: Instance segmentation<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>","<BR>task: Instance segmentation<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>  Instance segmentation - Cityscapes test: Average Precision<BR>","<BR>task: Instance segmentation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Instance segmentation - BDD100K val: AP<BR>  Instance segmentation - COCO test-dev: AP75<BR>","<BR>task: Instance segmentation<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>","<BR>task: Instance segmentation<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic instance segmentation - ScanNetV2: mAP-at-0.50<BR>  Instance segmentation - COCO minival: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Referring expression segmentation - RefCOCO testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F 1st frame<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F Full video<BR>","<BR>task: Instance segmentation<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>","<BR>task: Instance segmentation<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - COCO test-dev: AP@50<BR>","<BR>task: Instance segmentation<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mPrec<BR>  3D instance segmentation - ScanNet(v2): mAP<BR>","<BR>task: Instance segmentation<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Instance segmentation - COCO minival: AP50<BR>  Instance segmentation - COCO minival: AP75<BR>  Instance segmentation - COCO minival: APL<BR>  Instance segmentation - COCO minival: APS<BR>","<BR>task: Instance segmentation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.7<BR>  Real-time instance segmentation - MSCOCO: AP50<BR>  Real-time instance segmentation - MSCOCO: AP75<BR>  Real-time instance segmentation - MSCOCO: APL<BR>  Real-time instance segmentation - MSCOCO: APM<BR>  Real-time instance segmentation - MSCOCO: APS<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mCov<BR>  3D instance segmentation - S3DIS: mWCov<BR>","<BR>task: Instance segmentation<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Referring expression segmentation - J-HMDB: Precision@0.9<BR>","<BR>task: Intelligent surveillance<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Vehicle re-identification - VeRi-776: mAP<BR>","<BR>task: Intelligent surveillance<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Vehicle re-identification - VehicleID Small: Rank-5<BR>","<BR>task: Interest point detection<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Homography estimation - S-COCO: MACE<BR>","<BR>task: Keyword spotting<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>","<BR>task: Keyword spotting<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 20<BR>","<BR>task: Line segment detection<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP5<BR>  Line segment detection - wireframe dataset: sAP10<BR>  Line segment detection - wireframe dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: F1 score<BR>  Line segment detection - York Urban Dataset: sAP15<BR>  Line segment detection - wireframe dataset: sAP15<BR>","<BR>task: Material property prediction<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Formation energy prediction - Materials Project: MAE<BR>","<BR>task: Medical diagnosis<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Meta-learning<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2013-12-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>","<BR>task: Meta-learning<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>","<BR>task: Meta-learning<BR>date: 2020-08-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-target domain adaptation - Office-31: Accuracy<BR>  Multi-target domain adaptation - Office-Home: Accuracy<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-target domain adaptation - DomainNet: Accuracy<BR>","<BR>task: Multimodal machine translation<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: BLEU (EN-DE)<BR>  Multimodal machine translation - Multi30K: Meteor (EN-DE)<BR>","<BR>task: Multimodal machine translation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: Meteor (EN-FR)<BR>","<BR>task: Multispectral Object Detection<BR>date: 2020-09-01<BR>Anchor.<BR>benchmarks:<BR>  Multispectral Object Detection - FLIR-aligned: mAP50<BR>","<BR>task: Object counting<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Object counting - CARPK: MAE<BR>  Object counting - CARPK: RMSE<BR>","<BR>task: Object detection<BR>date: 2013-11-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2014-07-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - PASCAL VOC 2012: MAP<BR>","<BR>task: Object detection<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - UA-DETRAC: mAP<BR>","<BR>task: Object detection<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - COCO minival: AP50<BR>  Object detection - COCO minival: AP75<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>","<BR>task: Object detection<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - KITTI Cars Easy: AP<BR>  Object detection - KITTI Cars Hard: AP<BR>  Object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>","<BR>task: Object detection<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - iSAID: Average Precision<BR>","<BR>task: Object detection<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - AI-TOD: AP50<BR>  Object detection - AI-TOD: AP75<BR>  Object detection - AI-TOD: AP<BR>  Object detection - AI-TOD: APm<BR>  Object detection - AI-TOD: APs<BR>  Object detection - AI-TOD: APt<BR>","<BR>task: Object detection<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection - UAVDT: mAP<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: METEOR<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2018-05-01<BR>Anchor.<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2020-11-01<BR>Anchor.<BR>benchmarks:<BR>  Open Vocabulary Object Detection - MSCOCO: AP 0.5<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-01-01<BR>Anchor.<BR>benchmarks:<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy val: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclist Easy val: AP<BR>  3D object detection - KITTI Cyclist Hard val: AP<BR>  3D object detection - KITTI Cyclist Moderate val: AP<BR>  3D object detection - KITTI Cyclists Easy: AP<BR>  3D object detection - KITTI Cyclists Hard: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>  3D object detection - KITTI Pedestrians Hard: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular 3D object detection - SUN RGB-D: AP@0.15<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>  3D object detection - ScanNetV2: mAP-at-0.5<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - SUN-RGBD val: mAP-at-0.5<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - OPV2V: AP@0.7@CulverCity<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cyclists Moderate: AP50<BR>  3D object detection - V2XSet: AP0.5 (Noisy)<BR>  3D object detection - V2XSet: AP0.7 (Noisy)<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D object detection - waymo cyclist: APH/L2<BR>  3D object detection - waymo pedestrian: APH/L2<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Pedestrians Moderate: AP50<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  Monocular 3D object detection - KITTI Cars Hard: AP Hard<BR>  Monocular 3D object detection - KITTI Pedestrian Hard: AP Hard<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Hard: AP<BR>  Birds eye view object detection - KITTI Cyclists Moderate: AP<BR>  Birds eye view object detection - KITTI Pedestrians Moderate: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Easy: AP<BR>  Birds eye view object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>","<BR>task: Object detection // Dense object detection<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Dense object detection - SKU-110K: AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2014-07-01<BR>Anchor.<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2014-08-01<BR>Anchor.<BR>benchmarks:<BR>  Face detection - Annotated Faces in the Wild: AP<BR>  Face detection - FDDB: AP<BR>  Face detection - PASCAL Face: AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Face detection - WIDER Face (Easy): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (10-shot): AP<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Medical object detection<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Medical object detection - DeepLesion: Sensitivity<BR>","<BR>task: Object detection // Object Detection In Indoor Scenes<BR>date: 2014-07-01<BR>Anchor.<BR>benchmarks:<BR>  Object Detection In Indoor Scenes - SUN RGB-D: AP 0.5<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object proposal generation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Object proposal generation - PASCAL VOC 2012, 60 proposals per image: Average Recall<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - ISTD: Balanced Error Rate<BR>  RGB salient object detection - SBU: Balanced Error Rate<BR>  RGB salient object detection - UCF: Balanced Error Rate<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - SOC: Average MAE<BR>  RGB salient object detection - SOC: S-Measure<BR>  RGB salient object detection - SOC: mean E-Measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: S-Measure<BR>  RGB salient object detection - DUTS-TE: mean E-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - PASCAL-S: MAE<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  RGB salient object detection - DUT-OMRON: MAE<BR>  RGB salient object detection - ECSSD: MAE<BR>  RGB salient object detection - HKU-IS: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: max E-measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  RGB-D salient object detection - DES: Average MAE<BR>  RGB-D salient object detection - DES: S-Measure<BR>  RGB-D salient object detection - DES: max E-Measure<BR>  RGB-D salient object detection - DES: max F-Measure<BR>  RGB-D salient object detection - LFSD: Average MAE<BR>  RGB-D salient object detection - LFSD: S-Measure<BR>  RGB-D salient object detection - LFSD: max E-Measure<BR>  RGB-D salient object detection - LFSD: max F-Measure<BR>  RGB-D salient object detection - NLPR: Average MAE<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - NLPR: max E-Measure<BR>  RGB-D salient object detection - NLPR: max F-Measure<BR>  RGB-D salient object detection - RGBD135: Average MAE<BR>  RGB-D salient object detection - RGBD135: S-Measure<BR>  RGB-D salient object detection - RGBD135: max E-Measure<BR>  RGB-D salient object detection - RGBD135: max F-Measure<BR>  RGB-D salient object detection - SIP: Average MAE<BR>  RGB-D salient object detection - SIP: S-Measure<BR>  RGB-D salient object detection - SIP: max E-Measure<BR>  RGB-D salient object detection - SIP: max F-Measure<BR>  RGB-D salient object detection - STERE: Average MAE<BR>  RGB-D salient object detection - STERE: S-Measure<BR>  RGB-D salient object detection - STERE: max E-Measure<BR>  RGB-D salient object detection - STERE: max F-Measure<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  Salient object detection - DUT-OMRON: MAE<BR>  Salient object detection - HKU-IS: E-measure<BR>  Salient object detection - PASCAL-S: max_F1<BR>","<BR>task: Object detection // Surgical tool detection<BR>date: 2016-02-01<BR>Anchor.<BR>benchmarks:<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Object detection // Video object detection<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Video object detection - ImageNet VID: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2014-03-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised object detection - Charades: MAP<BR>  Weakly supervised object detection - HICO-DET: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised object detection - COCO test-dev: AP50<BR>  Weakly supervised object detection - COCO: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object localization<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised object localization - ILSVRC 2016: Top-5 Error<BR>","<BR>task: Object localization<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised object localization - CUB-200-2011: Top-1 Error Rate<BR>","<BR>task: Object reconstruction<BR>date: 2013-03-01<BR>Anchor.<BR>benchmarks:<BR>  3D object reconstruction from a single image - RenderPeople: Chamfer (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Point-to-surface distance (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Surface normal consistency<BR>","<BR>task: Object reconstruction<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D object reconstruction - Data3D\u2212R2N2: 3DIoU<BR>  3D object reconstruction - Data3D\u2212R2N2: Avg F1<BR>","<BR>task: Object reconstruction<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  3D object reconstruction from a single image - BUFF: Chamfer (cm)<BR>  3D object reconstruction from a single image - BUFF: Surface normal consistency<BR>","<BR>task: Object segmentation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object segmentation<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>","<BR>task: Optical character recognition<BR>date: 2015-07-01<BR>Anchor.<BR>benchmarks:<BR>  Optical character recognition - Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study: Accuracy (%)<BR>","<BR>task: Optical flow estimation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015 (train) : EPE<BR>","<BR>task: Optical flow estimation<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015: Fl-all<BR>","<BR>task: Out-of-distribution detection<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUPR<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Out-of-distribution detection - ImageNet dogs vs ImageNet non-dogs: AUROC<BR>  Out-of-distribution detection - MS-1M vs. IJB-C: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100 vs SVHN: AUROC<BR>  Out-of-distribution detection - CIFAR-100: FPR95<BR>  Out-of-distribution detection - CIFAR-10: AUROC<BR>  Out-of-distribution detection - CIFAR-10: FPR95<BR>","<BR>task: Out-of-distribution detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100 vs CIFAR-10: AUROC<BR>","<BR>task: Person re-identification<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - SYSU-30k: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - CUHK03: MAP<BR>  Person re-identification - CUHK03: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - CUHK-SYSU: MAP<BR>  Person re-identification - CUHK-SYSU: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-1<BR>  Person re-identification - MARS: Rank-5<BR>  Person re-identification - MARS: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2017-09-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - PRID2011: Rank-1<BR>  Person re-identification - PRID2011: Rank-20<BR>  Person re-identification - PRID2011: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - CUHK03-C: Rank-1<BR>  Person re-identification - CUHK03-C: mAP<BR>  Person re-identification - CUHK03-C: mINP<BR>  Person re-identification - Market-1501-C: Rank-1<BR>  Person re-identification - Market-1501-C: mAP<BR>  Person re-identification - Market-1501-C: mINP<BR>  Person re-identification - UAV-Human: Rank-1<BR>  Person re-identification - UAV-Human: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-10<BR>  Person re-identification - MARS: Rank-20<BR>  Person re-identification - MSMT17-C: Rank-1<BR>  Person re-identification - MSMT17-C: mAP<BR>  Person re-identification - MSMT17-C: mINP<BR>  Person re-identification - iLIDS-VID: Rank-1<BR>  Person re-identification - iLIDS-VID: Rank-20<BR>  Person re-identification - iLIDS-VID: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: mAP<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - Market-1501->MSMT17: Rank-1<BR>  Generalizable person re-identification - Market-1501->MSMT17: mAP<BR>","<BR>task: Person re-identification<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-10<BR>  Person re-identification - DukeMTMC-reID: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2020-02-01<BR>Anchor.<BR>benchmarks:<BR>  Self-Supervised Person Re-Identification - SYSU-30k: Rank-1<BR>","<BR>task: Person search<BR>date: 2021-03-01<BR>Anchor.<BR>benchmarks:<BR>  Person search - CUHK-SYSU: MAP<BR>  Person search - CUHK-SYSU: Top-1<BR>","<BR>task: Point Cloud Segmentation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Point Cloud Segmentation - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud classification<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Mean Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D point cloud classification - IntrA: F1 score (5-fold)<BR>  3D point cloud classification - ModelNet40-C: Error Rate<BR>  3D point cloud classification - ScanObjectNN: Mean Accuracy<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>  Point cloud classification - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud classification<BR>date: 2020-12-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud classification - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud generation<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud completion - ShapeNet: Chamfer Distance<BR>","<BR>task: Point cloud generation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud completion - Completion3D: Chamfer Distance<BR>","<BR>task: Point cloud generation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud completion - ShapeNet: F-Score@1%<BR>","<BR>task: Point cloud registration<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>","<BR>task: Point cloud registration<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud registration - KITTI: Success Rate<BR>","<BR>task: Point cloud registration<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud registration - 3DLoMatch (10-30% overlap): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - 3DMatch (at least 30% overlapped - sample 5k interest points): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - ETH (trained on 3DMatch): Recall<BR>","<BR>task: Point cloud registration<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud registration - 3DMatch (trained on KITTI): Recall<BR>  Point cloud registration - KITTI (trained on 3DMatch): Success Rate<BR>","<BR>task: Point cloud super resolution<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Point cloud super resolution - SHREC15: F-measure (%)<BR>","<BR>task: Pose tracking<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>  Pose tracking - PoseTrack2017: mAP<BR>","<BR>task: Quantization<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Data Free Quantization - CIFAR-100: CIFAR-100 W5A5 Top-1 Accuracy<BR>","<BR>task: Rain removal<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Rain100H: SSIM<BR>  Single image deraining - Rain100L: PSNR<BR>  Single image deraining - Rain100L: SSIM<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test1200: SSIM<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Reconstruction<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: Reconstruction<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>","<BR>task: Remote sensing<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Change detection for remote sensing images - CDD Dataset (season-varying): F1-Score<BR>","<BR>task: Robot navigation<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SPL<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SUCCESS<BR>","<BR>task: Saliency detection<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Saliency detection<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Video saliency detection - DHF1K: NSS<BR>","<BR>task: Saliency detection<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: max E-measure<BR>","<BR>task: Scene parsing<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2015-07-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Scene Graph Generation - PSG Dataset: R@20<BR>  Panoptic Scene Graph Generation - PSG Dataset: mR@20<BR>","<BR>task: Scene parsing<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Scene graph generation - Visual Genome: Recall-at-50<BR>","<BR>task: Scene parsing<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: Scene parsing<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Face parsing - CelebAMask-HQ: Mean F1<BR>  Face parsing - LaPa: Mean F1<BR>","<BR>task: Scene parsing<BR>date: 2020-02-01<BR>Anchor.<BR>benchmarks:<BR>  Unbiased Scene Graph Generation - Visual Genome: mR@20<BR>  Unbiased Scene Graph Generation - Visual Genome: ng-mR@20<BR>","<BR>task: Scene parsing<BR>date: 2021-08-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text recognition - IIIT5k: Accuracy<BR>  Scene text recognition - SVTP: Accuracy<BR>","<BR>task: Scene text detection<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Precision<BR>  Scene text detection - ICDAR 2013: Recall<BR>","<BR>task: Scene text detection<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Curved text detection - SCUT-CTW1500: F-Measure<BR>","<BR>task: Scene text detection<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - ICDAR 2015: Recall<BR>","<BR>task: Scene text detection<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - COCO-Text: F-Measure<BR>  Scene text detection - COCO-Text: Precision<BR>  Scene text detection - COCO-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>  Scene text detection - MSRA-TD500: Precision<BR>  Scene text detection - MSRA-TD500: Recall<BR>","<BR>task: Scene text detection<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>  Scene text detection - Total-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - ICDAR 2017 MLT: F-Measure<BR>  Scene text detection - ICDAR 2017 MLT: Precision<BR>  Scene text detection - ICDAR 2017 MLT: Recall<BR>  Scene text detection - SCUT-CTW1500: Precision<BR>  Scene text detection - SCUT-CTW1500: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Scene text detection - SCUT-CTW1500: F-Measure<BR>","<BR>task: Semantic segmentation<BR>date: 2014-07-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - SkyScapes-Dense: Mean IoU<BR>  Semantic segmentation - Trans10K: GFLOPs<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - CamVid: Mean IoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>","<BR>task: Semantic segmentation<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - RSMSS: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - DADA-seg: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - Semantic3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 val: mIoU<BR>  Semantic segmentation - ScanNetV2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - S3DIS: mAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2017-02-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ScanNet: 3DIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - LIP val: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ShapeNet: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - S3DIS Area5: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - S3DIS Area5: oAcc<BR>  Semantic segmentation - S3DIS: Mean IoU<BR>  Semantic segmentation - S3DIS: oAcc<BR>  Semantic segmentation - Semantic3D: oAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - EventScape: mIoU<BR>  Semantic segmentation - SynPASS: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - Stanford2D3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - Nighttime Driving: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-01-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: Pixel Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - ZJU-RGB-P: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-02-01<BR>Anchor.<BR>benchmarks:<BR>  Semantic segmentation - UAVid: Mean IoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-5 - PASCAL VOC 2012: Mean IoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Text style transfer - Yelp Review Dataset (Small): G-Score (BLEU, Accuracy)<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2019-11-01<BR>Anchor.<BR>benchmarks:<BR>  2D Semantic Segmentation - xBD: Weighted Average F1-score<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (A)<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (A)<BR>  3D part segmentation - IntrA: IoU (V)<BR>  3D part segmentation - ShapeNet-Part: Class Average IoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2018-10-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2020-08-01<BR>Anchor.<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Human part segmentation - PASCAL-Part: mIoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Human part segmentation - CIHP: Mean IoU<BR>","<BR>task: Semantic segmentation // LIDAR semantic segmentation<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  LIDAR semantic segmentation - Paris-Lille-3D: mIOU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes val: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes test: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes val: AP<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Cityscapes val: PQst<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes val: mIoU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>  Panoptic segmentation - Mapillary val: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-01-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Indian Driving Dataset: PQ<BR>  Panoptic segmentation - KITTI Panoptic Segmentation: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - Mapillary val: mIoU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: PQth<BR>  Panoptic segmentation - Mapillary val: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: RQ<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Frame (fps)<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 50% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 100 samples labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 1% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2020-07-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 25% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 50%: Validation mIoU<BR>","<BR>task: Semantic segmentation // Unsupervised semantic segmentation<BR>date: 2018-07-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised semantic segmentation - COCO-Stuff: Pixel Accuracy<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - COCO 2014 val: mIoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 val: Mean IoU<BR>","<BR>task: Semi-supervised object detection<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 1% labeled data: mAP<BR>  Semi-supervised object detection - COCO 10% labeled data: mAP<BR>  Semi-supervised object detection - COCO 2% labeled data: mAP<BR>  Semi-supervised object detection - COCO 5% labeled data: mAP<BR>","<BR>task: Semi-supervised object detection<BR>date: 2020-05-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 100% labeled data: mAP<BR>","<BR>task: Sign language recognition<BR>date: 2017-10-01<BR>Anchor.<BR>benchmarks:<BR>  Sign language recognition - RWTH-PHOENIX-Weather 2014: Word Error Rate (WER)<BR>","<BR>task: Sign language recognition<BR>date: 2019-10-01<BR>Anchor.<BR>benchmarks:<BR>  Sign language recognition - WLASL-2000: Top-1 Accuracy<BR>","<BR>task: Sign language translation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Sign language translation - RWTH-PHOENIX-Weather 2014 T: BLEU-4<BR>","<BR>task: Single-object discovery<BR>date: 2017-07-01<BR>Anchor.<BR>benchmarks:<BR>  Single-object discovery - COCO_20k: CorLoc<BR>","<BR>task: Super-resolution<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: PSNR 4x T2w<BR>  Super-resolution - IXI: SSIM 4x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Super-resolution<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>","<BR>task: Super-resolution<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>","<BR>task: Super-resolution<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>","<BR>task: Super-resolution<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2021-01-01<BR>Anchor.<BR>benchmarks:<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: Surface normals estimation<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Surface normals estimation - PCPNet: RMSE<BR>","<BR>task: Text based person retrieval<BR>date: 2018-08-01<BR>Anchor.<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2017-01-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Video process // Action classification<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CS<BR>  Action classification - Toyota Smarthome dataset: CV1<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-08-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>","<BR>task: Video process // Action classification<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - THUMOS\u201914: mAP<BR>","<BR>task: Video process // Action classification<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Moments in Time: Top 5 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Video process // Action classification<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2020-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action classification - Kinetics-700: Top-1 Accuracy<BR>  Action classification - Kinetics-700: Top-5 Accuracy<BR>","<BR>task: Video process // Action spotting<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Action spotting - SoccerNet: Average-mAP<BR>","<BR>task: Video process // Activity recognition in videos<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Video process // Object tracking<BR>date: 2014-07-01<BR>Anchor.<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2015-04-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - OTB-2013: AUC<BR>  Visual object tracking - OTB-50: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - OTB-2015: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-object tracking - MOT17: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-object tracking - MOT20: IDF1<BR>  Multi-object tracking - MOT20: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - VOT2017: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - GOT-10k: Average Overlap<BR>  Visual object tracking - GOT-10k: Success Rate 0.5<BR>  Visual object tracking - LaSOT: AUC<BR>  Visual object tracking - LaSOT: Normalized Precision<BR>  Visual object tracking - LaSOT: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-02-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-object tracking - MOTS20: IDF1<BR>  Online multi-object tracking - MOT16: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - VOT2019: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Visual object tracking - UAV123: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-04-01<BR>Anchor.<BR>benchmarks:<BR>  Multi-object tracking - DanceTrack: AssA<BR>  Multi-object tracking - DanceTrack: HOTA<BR>  Multi-object tracking - DanceTrack: IDF1<BR>  Multi-object tracking - MOT17: IDF1<BR>","<BR>task: Video process // Video Enhancement<BR>date: 2018-03-01<BR>Anchor.<BR>benchmarks:<BR>  Video Enhancement - MFQE v2: Incremental PSNR<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2017-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: KLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: PLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: SRCC<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video Quality Assessment - KoNViD-1k: PLCC<BR>","<BR>task: Video process // Video captioning<BR>date: 2018-04-01<BR>Anchor.<BR>benchmarks:<BR>  Video captioning - YouCook2: BLEU-3<BR>  Video captioning - YouCook2: CIDEr<BR>  Video captioning - YouCook2: METEOR<BR>  Video captioning - YouCook2: ROUGE-L<BR>","<BR>task: Video process // Video captioning<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Dense video captioning - ActivityNet Captions: BLEU-3<BR>  Dense video captioning - ActivityNet Captions: BLEU-4<BR>  Dense video captioning - ActivityNet Captions: METEOR<BR>","<BR>task: Video process // Video classification<BR>date: 2013-06-01<BR>Anchor.<BR>benchmarks:<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2014-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Video classification<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Video process // Video classification<BR>date: 2016-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2016-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Video classification<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video classification - YouTube-8M: Hit-at-1<BR>","<BR>task: Video process // Video classification<BR>date: 2016-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2017-11-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-01-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Video process // Video classification<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-11-01<BR>Anchor.<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-12-01<BR>Anchor.<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Diving-48: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2019-12-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-03-01<BR>Anchor.<BR>benchmarks:<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Video process // Video denoising<BR>date: 2019-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video denoising - DAVIS sigma10: PSNR<BR>  Video denoising - DAVIS sigma20: PSNR<BR>  Video denoising - Set8 sigma10: PSNR<BR>  Video denoising - Set8 sigma30: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Video frame interpolation - Middlebury: Interpolation Error<BR>  Video frame interpolation - Vimeo90K: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Video frame interpolation - UCF101: SSIM<BR>  Video frame interpolation - Vimeo90K: SSIM<BR>  Video frame interpolation - X4K1000FPS: PSNR<BR>  Video frame interpolation - X4K1000FPS: SSIM<BR>","<BR>task: Video process // Video generation<BR>date: 2016-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, Unconditional, Single GPU: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2019-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, 64x64, Unconditional: Inception Score<BR>","<BR>task: Video process // Video inpainting<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video inpainting - DAVIS: PSNR<BR>  Video inpainting - DAVIS: SSIM<BR>  Video inpainting - YouTube-VOS 2018 val: PSNR<BR>  Video inpainting - YouTube-VOS 2018 val: SSIM<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2017-03-01<BR>Anchor.<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP75<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AR10<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2014-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2015-12-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2016-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised video object segmentation - YouTube: mIoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  One-shot visual object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Overall<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Speed  (FPS)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-04-01<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised video object segmentation - SegTrack v2: Mean IoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J&F score<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J&F score<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-09-01<BR>Anchor.<BR>benchmarks:<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J&F score<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J&F<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J<BR>  Interactive video object segmentation - DAVIS 2017: J@60s<BR>","<BR>task: Video process // Video prediction<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video prediction - KTH: LPIPS<BR>  Video prediction - KTH: PSNR<BR>  Video prediction - KTH: SSIM<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Video prediction - Human3.6M: MAE<BR>  Video prediction - Human3.6M: MSE<BR>  Video prediction - Human3.6M: SSIM<BR>","<BR>task: Video process // Video question answering<BR>date: 2015-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video question answering - SUTD-TrafficQA: 1/2<BR>  Video question answering - SUTD-TrafficQA: 1/4<BR>","<BR>task: Video process // Video question answering<BR>date: 2019-04-01<BR>Anchor.<BR>benchmarks:<BR>  Video question answering - TVQA: Accuracy<BR>","<BR>task: Video process // Video retrieval<BR>date: 2015-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - YouCook2: text-to-video Median Rank<BR>  Video retrieval - YouCook2: text-to-video R-at-10<BR>  Video retrieval - YouCook2: text-to-video R-at-1<BR>  Video retrieval - YouCook2: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2016-10-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2018-02-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - MSR-VTT-1kA: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT: video-to-text R-at-1<BR>","<BR>task: Video process // Video retrieval<BR>date: 2019-07-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - ActivityNet: text-to-video Mean Rank<BR>  Video retrieval - ActivityNet: text-to-video Median Rank<BR>  Video retrieval - ActivityNet: text-to-video R-at-1<BR>  Video retrieval - ActivityNet: text-to-video R-at-50<BR>  Video retrieval - ActivityNet: text-to-video R-at-5<BR>  Video retrieval - DiDeMo: text-to-video Mean Rank<BR>  Video retrieval - DiDeMo: text-to-video Median Rank<BR>  Video retrieval - DiDeMo: text-to-video R-at-10<BR>  Video retrieval - DiDeMo: text-to-video R-at-1<BR>  Video retrieval - DiDeMo: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-02-01<BR>Anchor.<BR>benchmarks:<BR>  Video retrieval - MSR-VTT-1kA: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-5<BR>  Video retrieval - MSVD: video-to-text R-at-10<BR>  Video retrieval - MSVD: video-to-text R-at-1<BR>  Video retrieval - MSVD: video-to-text R-at-5<BR>","<BR>task: Video process // Video segmentation<BR>date: 2017-05-01<BR>Anchor.<BR>benchmarks:<BR>  Camera shot boundary detection - ClipShots: F1 score<BR>","<BR>task: Video process // Video semantic segmentation<BR>date: 2016-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video semantic segmentation - Cityscapes val: mIoU<BR>","<BR>task: Video process // Video summarization<BR>date: 2017-08-01<BR>Anchor.<BR>benchmarks:<BR>  Video summarization - SumMe: F1-score (Canonical)<BR>  Video summarization - TvSum: F1-score (Augmented)<BR>  Video summarization - TvSum: F1-score (Canonical)<BR>","<BR>task: Video process // Video summarization<BR>date: 2017-12-01<BR>Anchor.<BR>benchmarks:<BR>  Supervised video summarization - SumMe: F1-score (Canonical)<BR>  Supervised video summarization - TvSum: F1-score (Canonical)<BR>  Unsupervised video summarization - SumMe: training time (s)<BR>  Unsupervised video summarization - TvSum: F1-score<BR>  Unsupervised video summarization - TvSum: training time (s)<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2014-12-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2016-09-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2018-06-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-03-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-05-01<BR>Anchor.<BR>benchmarks:<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Visual Odometry<BR>date: 2014-08-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Visual Odometry<BR>date: 2020-03-01<BR>Anchor.<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Visual Relationship Detection<BR>date: 2016-07-01<BR>Anchor.<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Phrase Detection: R@100<BR>  Visual Relationship Detection - VRD Phrase Detection: R@50<BR>  Visual Relationship Detection - VRD Predicate Detection: R@100<BR>  Visual Relationship Detection - VRD Predicate Detection: R@50<BR>  Visual Relationship Detection - VRD Relationship Detection: R@100<BR>  Visual Relationship Detection - VRD Relationship Detection: R@50<BR>","<BR>task: Visual dialog<BR>date: 2016-05-01<BR>Anchor.<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2016-11-01<BR>Anchor.<BR>benchmarks:<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: Mean<BR>  Visual dialog - Visual Dialog v1.0 test-std: NDCG (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-10<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual place recognition<BR>date: 2015-11-01<BR>Anchor.<BR>benchmarks:<BR>  Visual place recognition - Berlin Kudamm: Recall-at-1<BR>","<BR>task: Visual reasoning<BR>date: 2019-08-01<BR>Anchor.<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>  Visual reasoning - NLVR2 Test: Accuracy<BR>  Visual reasoning - PHYRE-1B-Cross: AUCCESS<BR>  Visual reasoning - PHYRE-1B-Within: AUCCESS<BR>","<BR>task: Weakly-supervised instance segmentation<BR>date: 2020-01-01<BR>Anchor.<BR>benchmarks:<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: Average Best Overlap<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2014-09-01<BR>Anchor.<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - Olympics: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>"],"marker":{"line":{"width":1,"color":"gray"},"size":20,"symbol":42},"mode":"markers","x":["2016-10-01T00:00:00","2014-09-01T00:00:00","2015-05-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2018-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-10-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2014-06-01T00:00:00","2014-12-01T00:00:00","2015-04-01T00:00:00","2015-06-01T00:00:00","2015-09-01T00:00:00","2015-11-01T00:00:00","2016-01-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-01-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-12-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-04-01T00:00:00","2021-01-01T00:00:00","2019-04-01T00:00:00","2015-12-01T00:00:00","2016-04-01T00:00:00","2016-11-01T00:00:00","2017-10-01T00:00:00","2018-03-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2016-10-01T00:00:00","2014-12-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2017-08-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2021-01-01T00:00:00","2020-08-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2015-07-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2016-02-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2019-03-01T00:00:00","2015-06-01T00:00:00","2015-11-01T00:00:00","2016-04-01T00:00:00","2016-09-01T00:00:00","2017-03-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2013-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2021-03-01T00:00:00","2016-03-01T00:00:00","2018-06-01T00:00:00","2014-09-01T00:00:00","2017-07-01T00:00:00","2017-03-01T00:00:00","2017-12-01T00:00:00","2018-07-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2013-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2021-03-01T00:00:00","2019-10-01T00:00:00","2016-04-01T00:00:00","2017-01-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-05-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-09-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-10-01T00:00:00","2016-12-01T00:00:00","2017-02-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2020-04-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2014-11-01T00:00:00","2015-05-01T00:00:00","2015-11-01T00:00:00","2015-12-01T00:00:00","2016-02-01T00:00:00","2016-03-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2019-03-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2020-06-01T00:00:00","2021-02-01T00:00:00","2019-08-01T00:00:00","2016-06-01T00:00:00","2016-12-01T00:00:00","2017-11-01T00:00:00","2019-04-01T00:00:00","2013-06-01T00:00:00","2015-06-01T00:00:00","2016-01-01T00:00:00","2017-07-01T00:00:00","2014-08-01T00:00:00","2020-03-01T00:00:00","2015-06-01T00:00:00","2016-01-01T00:00:00","2017-10-01T00:00:00","2018-05-01T00:00:00","2020-05-01T00:00:00","2015-08-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-10-01T00:00:00","2018-05-01T00:00:00","2018-07-01T00:00:00","2018-12-01T00:00:00","2017-08-01T00:00:00","2018-07-01T00:00:00","2014-08-01T00:00:00","2014-11-01T00:00:00","2016-09-01T00:00:00","2017-03-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2018-10-01T00:00:00","2019-07-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2014-06-01T00:00:00","2014-09-01T00:00:00","2014-12-01T00:00:00","2015-02-01T00:00:00","2015-12-01T00:00:00","2016-05-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2017-02-01T00:00:00","2017-05-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2016-12-01T00:00:00","2018-12-01T00:00:00","2014-08-01T00:00:00","2017-07-01T00:00:00","2018-02-01T00:00:00","2019-06-01T00:00:00","2019-09-01T00:00:00","2020-01-01T00:00:00","2021-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-08-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-02-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-08-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2019-02-01T00:00:00","2019-06-01T00:00:00","2019-09-01T00:00:00","2016-04-01T00:00:00","2013-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2015-12-01T00:00:00","2017-03-01T00:00:00","2015-05-01T00:00:00","2017-04-01T00:00:00","2018-08-01T00:00:00","2020-03-01T00:00:00","2013-01-01T00:00:00","2013-02-01T00:00:00","2013-06-01T00:00:00","2013-11-01T00:00:00","2013-12-01T00:00:00","2014-09-01T00:00:00","2015-12-01T00:00:00","2016-03-01T00:00:00","2016-09-01T00:00:00","2017-02-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-03-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-10-01T00:00:00","2020-02-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2017-04-01T00:00:00","2013-06-01T00:00:00","2013-12-01T00:00:00","2014-09-01T00:00:00","2016-06-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-11-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2013-12-01T00:00:00","2015-04-01T00:00:00","2015-12-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2018-05-01T00:00:00","2018-11-01T00:00:00","2020-07-01T00:00:00","2016-12-01T00:00:00","2016-09-01T00:00:00","2019-06-01T00:00:00","2016-02-01T00:00:00","2017-05-01T00:00:00","2015-09-01T00:00:00","2016-03-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2019-05-01T00:00:00","2015-06-01T00:00:00","2015-07-01T00:00:00","2016-06-01T00:00:00","2016-10-01T00:00:00","2017-03-01T00:00:00","2017-08-01T00:00:00","2018-06-01T00:00:00","2019-05-01T00:00:00","2015-04-01T00:00:00","2018-03-01T00:00:00","2019-02-01T00:00:00","2016-11-01T00:00:00","2015-11-01T00:00:00","2018-07-01T00:00:00","2020-05-01T00:00:00","2013-12-01T00:00:00","2015-07-01T00:00:00","2015-11-01T00:00:00","2016-04-01T00:00:00","2016-05-01T00:00:00","2017-03-01T00:00:00","2018-10-01T00:00:00","2018-12-01T00:00:00","2020-09-01T00:00:00","2015-08-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-10-01T00:00:00","2018-05-01T00:00:00","2018-07-01T00:00:00","2018-12-01T00:00:00","2016-06-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2013-10-01T00:00:00","2014-09-01T00:00:00","2015-05-01T00:00:00","2015-11-01T00:00:00","2016-01-01T00:00:00","2016-06-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2017-07-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2017-11-01T00:00:00","2019-08-01T00:00:00","2017-03-01T00:00:00","2018-09-01T00:00:00","2019-08-01T00:00:00","2015-12-01T00:00:00","2015-05-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-07-01T00:00:00","2018-08-01T00:00:00","2019-02-01T00:00:00","2019-05-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2013-06-01T00:00:00","2015-01-01T00:00:00","2021-05-01T00:00:00","2016-11-01T00:00:00","2015-04-01T00:00:00","2016-08-01T00:00:00","2018-01-01T00:00:00","2014-11-01T00:00:00","2014-12-01T00:00:00","2015-04-01T00:00:00","2015-11-01T00:00:00","2016-04-01T00:00:00","2016-11-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2018-04-01T00:00:00","2019-08-01T00:00:00","2020-07-01T00:00:00","2014-11-01T00:00:00","2014-12-01T00:00:00","2015-11-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2021-01-01T00:00:00","2018-06-01T00:00:00","2019-03-01T00:00:00","2015-12-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-09-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-07-01T00:00:00","2018-11-01T00:00:00","2020-07-01T00:00:00","2016-06-01T00:00:00","2017-11-01T00:00:00","2018-08-01T00:00:00","2017-07-01T00:00:00","2018-12-01T00:00:00","2017-02-01T00:00:00","2017-10-01T00:00:00","2015-12-01T00:00:00","2013-06-01T00:00:00","2013-12-01T00:00:00","2014-09-01T00:00:00","2016-06-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-11-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2014-09-01T00:00:00","2019-04-01T00:00:00","2017-01-01T00:00:00","2018-08-01T00:00:00","2020-09-01T00:00:00","2015-06-01T00:00:00","2013-11-01T00:00:00","2014-07-01T00:00:00","2015-04-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-08-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-12-01T00:00:00","2018-03-01T00:00:00","2014-11-01T00:00:00","2014-12-01T00:00:00","2015-11-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2015-11-01T00:00:00","2016-11-01T00:00:00","2017-11-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-06-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2017-08-01T00:00:00","2014-07-01T00:00:00","2014-08-01T00:00:00","2016-04-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2014-07-01T00:00:00","2017-11-01T00:00:00","2017-03-01T00:00:00","2014-06-01T00:00:00","2015-06-01T00:00:00","2016-03-01T00:00:00","2016-11-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2018-06-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2020-04-01T00:00:00","2016-06-01T00:00:00","2019-06-01T00:00:00","2015-06-01T00:00:00","2020-05-01T00:00:00","2014-06-01T00:00:00","2015-06-01T00:00:00","2018-06-01T00:00:00","2019-11-01T00:00:00","2016-02-01T00:00:00","2017-03-01T00:00:00","2014-03-01T00:00:00","2015-05-01T00:00:00","2015-11-01T00:00:00","2016-09-01T00:00:00","2015-12-01T00:00:00","2018-07-01T00:00:00","2013-03-01T00:00:00","2016-04-01T00:00:00","2019-05-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2015-07-01T00:00:00","2016-12-01T00:00:00","2018-09-01T00:00:00","2016-10-01T00:00:00","2017-06-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2014-06-01T00:00:00","2015-12-01T00:00:00","2016-04-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2017-01-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2017-09-01T00:00:00","2017-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2020-02-01T00:00:00","2021-03-01T00:00:00","2016-12-01T00:00:00","2014-06-01T00:00:00","2015-05-01T00:00:00","2016-12-01T00:00:00","2020-12-01T00:00:00","2015-06-01T00:00:00","2018-02-01T00:00:00","2018-08-01T00:00:00","2016-03-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-10-01T00:00:00","2018-01-01T00:00:00","2017-10-01T00:00:00","2020-03-01T00:00:00","2016-09-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2020-04-01T00:00:00","2014-06-01T00:00:00","2015-07-01T00:00:00","2017-01-01T00:00:00","2017-07-01T00:00:00","2017-11-01T00:00:00","2018-03-01T00:00:00","2019-04-01T00:00:00","2020-02-01T00:00:00","2021-08-01T00:00:00","2014-12-01T00:00:00","2015-05-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2018-06-01T00:00:00","2014-07-01T00:00:00","2014-11-01T00:00:00","2014-12-01T00:00:00","2015-05-01T00:00:00","2015-11-01T00:00:00","2015-12-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-02-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-10-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2021-02-01T00:00:00","2016-06-01T00:00:00","2017-05-01T00:00:00","2019-11-01T00:00:00","2016-06-01T00:00:00","2016-12-01T00:00:00","2016-12-01T00:00:00","2018-12-01T00:00:00","2018-10-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2020-08-01T00:00:00","2015-11-01T00:00:00","2018-08-01T00:00:00","2017-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2018-01-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2019-01-01T00:00:00","2019-09-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2014-12-01T00:00:00","2015-05-01T00:00:00","2018-02-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2020-07-01T00:00:00","2021-06-01T00:00:00","2018-07-01T00:00:00","2018-06-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-05-01T00:00:00","2017-10-01T00:00:00","2019-10-01T00:00:00","2018-06-01T00:00:00","2017-07-01T00:00:00","2014-12-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2017-08-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2021-01-01T00:00:00","2018-12-01T00:00:00","2018-08-01T00:00:00","2016-04-01T00:00:00","2017-01-01T00:00:00","2018-01-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2016-04-01T00:00:00","2016-08-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2020-06-01T00:00:00","2018-04-01T00:00:00","2014-09-01T00:00:00","2014-07-01T00:00:00","2015-04-01T00:00:00","2015-12-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-02-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2020-04-01T00:00:00","2018-03-01T00:00:00","2017-09-01T00:00:00","2019-06-01T00:00:00","2018-04-01T00:00:00","2020-03-01T00:00:00","2013-06-01T00:00:00","2014-06-01T00:00:00","2014-11-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-09-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2021-03-01T00:00:00","2019-06-01T00:00:00","2017-08-01T00:00:00","2019-04-01T00:00:00","2016-05-01T00:00:00","2016-09-01T00:00:00","2019-09-01T00:00:00","2019-05-01T00:00:00","2017-03-01T00:00:00","2018-02-01T00:00:00","2019-05-01T00:00:00","2014-06-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-09-01T00:00:00","2019-04-01T00:00:00","2015-06-01T00:00:00","2017-12-01T00:00:00","2015-05-01T00:00:00","2019-04-01T00:00:00","2015-06-01T00:00:00","2016-09-01T00:00:00","2016-10-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2019-07-01T00:00:00","2021-02-01T00:00:00","2017-05-01T00:00:00","2016-05-01T00:00:00","2017-08-01T00:00:00","2017-12-01T00:00:00","2014-12-01T00:00:00","2016-09-01T00:00:00","2018-06-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2014-08-01T00:00:00","2020-03-01T00:00:00","2016-07-01T00:00:00","2016-05-01T00:00:00","2016-11-01T00:00:00","2015-11-01T00:00:00","2019-08-01T00:00:00","2020-01-01T00:00:00","2014-09-01T00:00:00"],"y":["3D Point Cloud Linear Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Depth Estimation","3D vision process // 3D Shape Representation","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D shape classification","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // Motion forecasting","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action quality assessment","Action quality assessment","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Ad-hoc video search","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Blind face restoration","Continual learning","Continual learning","Continual learning","Continual learning","Crowds","Crowds","Crowds","Crowds","Deception detection","Deception detection","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Depth completion","Depth completion","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Edge detection","Emotion classification","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Horizon line estimation","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human parsing","Human parsing","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification // Document image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Hyperspectral image classification","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Photo geolocation estimation","Image classification // Photo geolocation estimation","Image classification // Satellite image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Superpixel image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image enhancement","Image enhancement","Image enhancement","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image matching","Image matching","Image matting","Image matting","Image matting","Image quality assessment","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image recognition","Image recognition","Image recognition","Image reconstruction","Image restoration","Image restoration","Image restoration","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image/document clustering","Image/document clustering","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Intelligent surveillance","Intelligent surveillance","Interest point detection","Keyword spotting","Keyword spotting","Line segment detection","Line segment detection","Material property prediction","Material property prediction","Medical diagnosis","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Multi-target domain adaptation","Multi-target domain adaptation","Multimodal machine translation","Multimodal machine translation","Multispectral Object Detection","Object counting","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Dense object detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Few-shot object detection","Object detection // Medical object detection","Object detection // Object Detection In Indoor Scenes","Object detection // Object detection in aerial images","Object detection // Object proposal generation","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Surgical tool detection","Object detection // Video object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object localization","Object localization","Object reconstruction","Object reconstruction","Object reconstruction","Object segmentation","Object segmentation","Optical character recognition","Optical flow estimation","Optical flow estimation","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person search","Point Cloud Segmentation","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud generation","Point cloud generation","Point cloud generation","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud super resolution","Pose tracking","Quantization","Rain removal","Reconstruction","Reconstruction","Remote sensing","Robot navigation","Saliency detection","Saliency detection","Saliency detection","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // LIDAR semantic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semi-supervised object detection","Semi-supervised object detection","Sign language recognition","Sign language recognition","Sign language translation","Single-object discovery","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Surface normals estimation","Text based person retrieval","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action spotting","Video process // Activity recognition in videos","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Video Enhancement","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video captioning","Video process // Video captioning","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video denoising","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video inpainting","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video prediction","Video process // Video prediction","Video process // Video question answering","Video process // Video question answering","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video segmentation","Video process // Video semantic segmentation","Video process // Video summarization","Video process // Video summarization","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Visual Odometry","Visual Odometry","Visual Relationship Detection","Visual dialog","Visual dialog","Visual place recognition","Visual reasoning","Weakly-supervised instance segmentation","Zero-Shot Action Recognition"],"type":"scatter","line":{"color":"black","width":0}},{"hovertemplate":["<BR>task: 3D Point Cloud Linear Classification<BR>date: 2017-12-01<BR>ratio: 0.6711<BR>benchmarks:<BR>  3D Point Cloud Linear Classification - ModelNet40: Overall Accuracy<BR>","<BR>task: 3D Point Cloud Linear Classification<BR>date: 2019-01-01<BR>ratio: 0.2895<BR>benchmarks:<BR>  3D Point Cloud Linear Classification - ModelNet40: Overall Accuracy<BR>","<BR>task: 3D Point Cloud Linear Classification<BR>date: 2020-08-01<BR>ratio: 0.0132<BR>benchmarks:<BR>  3D Point Cloud Linear Classification - ModelNet40: Overall Accuracy<BR>","<BR>task: 3D Point Cloud Linear Classification<BR>date: 2021-09-01<BR>ratio: 0.0263<BR>benchmarks:<BR>  3D Point Cloud Linear Classification - ModelNet40: Overall Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2015-12-01<BR>ratio: 0.0148<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2016-11-01<BR>ratio: 0.7294<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2016-12-01<BR>ratio: 0.1552<BR>benchmarks:<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2017-07-01<BR>ratio: 0.1864<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2017-11-01<BR>ratio: 0.949<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-02-01<BR>ratio: 0.375<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-03-01<BR>ratio: 0.4841<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-04-01<BR>ratio: 0.7947<BR>benchmarks:<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-08-01<BR>ratio: 0.0643<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-09-01<BR>ratio: 0.1963<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-10-01<BR>ratio: 0.1206<BR>benchmarks:<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2018-11-01<BR>ratio: 0.0275<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-03-01<BR>ratio: 0.8755<BR>benchmarks:<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-04-01<BR>ratio: 0.9951<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-05-01<BR>ratio: 0.625<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-07-01<BR>ratio: 0.6195<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-08-01<BR>ratio: 0.0413<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-10-01<BR>ratio: 0.2319<BR>benchmarks:<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-11-01<BR>ratio: 0.2232<BR>benchmarks:<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2019-12-01<BR>ratio: 0.0634<BR>benchmarks:<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-03-01<BR>ratio: 0.2073<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-04-01<BR>ratio: 0.3693<BR>benchmarks:<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-07-01<BR>ratio: 0.1512<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-08-01<BR>ratio: 0.1014<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-11-01<BR>ratio: 0.0257<BR>benchmarks:<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2020-12-01<BR>ratio: 0.4017<BR>benchmarks:<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2021-01-01<BR>ratio: 0.3889<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2021-04-01<BR>ratio: 0.1659<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2021-09-01<BR>ratio: 0.0854<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2021-11-01<BR>ratio: 0.3593<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: 3D vision process // 3D Classification<BR>date: 2021-12-01<BR>ratio: 0.022<BR>benchmarks:<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>","<BR>task: 3D vision process // 3D Depth Estimation<BR>date: 2020-08-01<BR>ratio: 0.8252<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>","<BR>task: 3D vision process // 3D Depth Estimation<BR>date: 2021-12-01<BR>ratio: 0.9993<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>","<BR>task: 3D vision process // 3D Shape Representation<BR>date: 2019-08-01<BR>ratio: 0.0128<BR>benchmarks:<BR>  3D Dense Shape Correspondence - SHREC'19: Accuracy at 1%<BR>","<BR>task: 3D vision process // 3D Shape Representation<BR>date: 2020-10-01<BR>ratio: 0.109<BR>benchmarks:<BR>  3D Dense Shape Correspondence - SHREC'19: Accuracy at 1%<BR>","<BR>task: 3D vision process // 3D Shape Representation<BR>date: 2020-12-01<BR>ratio: 0.1282<BR>benchmarks:<BR>  3D Dense Shape Correspondence - SHREC'19: Accuracy at 1%<BR>","<BR>task: 3D vision process // 3D Shape Representation<BR>date: 2021-10-01<BR>ratio: 0.75<BR>benchmarks:<BR>  3D Dense Shape Correspondence - SHREC'19: Accuracy at 1%<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2016-12-01<BR>ratio: 0.0165<BR>benchmarks:<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-05-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-10-01<BR>ratio: 0.877<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2017-11-01<BR>ratio: 0.6651<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2018-12-01<BR>ratio: 0.872<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-01-01<BR>ratio: 0.128<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-03-01<BR>ratio: 0.0412<BR>benchmarks:<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - CUB: Inception score<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-07-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-10-01<BR>ratio: 0.1204<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-08-01<BR>ratio: 0.1216<BR>benchmarks:<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-10-01<BR>ratio: 0.1867<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2020-12-01<BR>ratio: 0.1486<BR>benchmarks:<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-01-01<BR>ratio: 0.0464<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-11-01<BR>ratio: 0.7835<BR>benchmarks:<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: 3D vision process // 3D car instance understanding<BR>date: 2021-12-01<BR>ratio: 0.0507<BR>benchmarks:<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2018-12-01<BR>ratio: 0.4719<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: 3D vision process // 3D human action recognition<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2014-09-01<BR>ratio: 0.1716<BR>benchmarks:<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-08-01<BR>ratio: 0.1475<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-09-01<BR>ratio: 0.4856<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2015-11-01<BR>ratio: 0.3931<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-03-01<BR>ratio: 0.0844<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-06-01<BR>ratio: 0.2222<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-07-01<BR>ratio: 0.1593<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-08-01<BR>ratio: 0.5294<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-09-01<BR>ratio: 0.2727<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-10-01<BR>ratio: 0.3009<BR>benchmarks:<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-11-01<BR>ratio: 0.5468<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2016-12-01<BR>ratio: 0.3015<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-01-01<BR>ratio: 0.3368<BR>benchmarks:<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-04-01<BR>ratio: 0.3213<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-05-01<BR>ratio: 0.1667<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-07-01<BR>ratio: 0.8641<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-09-01<BR>ratio: 0.3918<BR>benchmarks:<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-10-01<BR>ratio: 0.0524<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-11-01<BR>ratio: 0.2199<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2017-12-01<BR>ratio: 0.3256<BR>benchmarks:<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-02-01<BR>ratio: 0.0061<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-03-01<BR>ratio: 0.1868<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-04-01<BR>ratio: 0.764<BR>benchmarks:<BR>  3D human pose estimation - Surreal: MPJPE<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-05-01<BR>ratio: 0.4188<BR>benchmarks:<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-06-01<BR>ratio: 0.3502<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-07-01<BR>ratio: 0.4386<BR>benchmarks:<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-09-01<BR>ratio: 0.9448<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-11-01<BR>ratio: 0.5163<BR>benchmarks:<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2018-12-01<BR>ratio: 0.2952<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-01-01<BR>ratio: 0.3321<BR>benchmarks:<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-02-01<BR>ratio: 0.4007<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-03-01<BR>ratio: 0.8519<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-04-01<BR>ratio: 0.5217<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: MPJPE<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-05-01<BR>ratio: 0.185<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-06-01<BR>ratio: 0.961<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-07-01<BR>ratio: 0.7658<BR>benchmarks:<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-08-01<BR>ratio: 0.2<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  Image super-resolution - PIRM-test: NIQE<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-09-01<BR>ratio: 0.7778<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-10-01<BR>ratio: 0.8359<BR>benchmarks:<BR>  3D human pose estimation - Human3.6M: PA-MPJPE<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Monocular 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2019-12-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPJPE<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-02-01<BR>ratio: 0.1359<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-03-01<BR>ratio: 0.3407<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPJPE<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-04-01<BR>ratio: 0.4396<BR>benchmarks:<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  3D human pose estimation - Surreal: MPJPE<BR>  3D multi-person pose estimation (absolute) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-05-01<BR>ratio: 0.8441<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-07-01<BR>ratio: 0.7648<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPJPE<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-08-01<BR>ratio: 0.5446<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPVPE<BR>  3D human pose estimation - 3DPW: Number of parameters (M)<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D multi-person pose estimation (absolute) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation - MuPoTS-3D: 3DPCK<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-10-01<BR>ratio: 0.8987<BR>benchmarks:<BR>  3D human pose estimation - CMU Panoptic: Average MPJPE (mm)<BR>  3D human pose estimation - Total Capture: Average MPJPE (mm)<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: MPJPE<BR>  3D multi-person pose estimation - MuPoTS-3D: 3DPCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-11-01<BR>ratio: 0.6355<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPVPE<BR>  3D human pose estimation - 3DPW: Number of parameters (M)<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2020-12-01<BR>ratio: 0.1152<BR>benchmarks:<BR>  3D multi-person pose estimation (absolute) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-03-01<BR>ratio: 0.0625<BR>benchmarks:<BR>  3D human pose estimation - HumanEva-I: Mean Reconstruction Error (mm)<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-04-01<BR>ratio: 0.1394<BR>benchmarks:<BR>  3D multi-person pose estimation (absolute) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation (root-relative) - MuPoTS-3D: 3DPCK<BR>  3D multi-person pose estimation - CMU Panoptic Studio Dataset: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-05-01<BR>ratio: 0.3861<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPVPE<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-06-01<BR>ratio: 0.9104<BR>benchmarks:<BR>  3D human pose estimation - CMU Panoptic: Average MPJPE (mm)<BR>  3D multi-person pose estimation - CMU Panoptic Studio Dataset: Average MPJPE (mm)<BR>  3D multi-person pose estimation - Campus: PCP3D<BR>  3D multi-person pose estimation - Shelf: PCP3D<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-07-01<BR>ratio: 0.3409<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: AUC<BR>  3D human pose estimation - MPI-INF-3DHP: MPJPE<BR>  3D human pose estimation - MPI-INF-3DHP: PCK<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-08-01<BR>ratio: 0.2632<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-09-01<BR>ratio: 0.1339<BR>benchmarks:<BR>  3D human pose estimation - MPI-INF-3DHP: PA-MPJPE<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-10-01<BR>ratio: 0.2308<BR>benchmarks:<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-11-01<BR>ratio: 0.0693<BR>benchmarks:<BR>  3D human pose estimation - 3DPW: MPJPE<BR>  3D human pose estimation - 3DPW: PA-MPJPE<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D human pose estimation<BR>date: 2021-12-01<BR>ratio: 0.1046<BR>benchmarks:<BR>  Weakly-supervised 3D human pose estimation - Human3.6M: Average MPJPE (mm)<BR>","<BR>task: 3D vision process // 3D human reconstruction<BR>date: 2020-06-01<BR>ratio: 0.6957<BR>benchmarks:<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>","<BR>task: 3D vision process // 3D human reconstruction<BR>date: 2020-08-01<BR>ratio: 0.8916<BR>benchmarks:<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: 3D vision process // 3D human reconstruction<BR>date: 2021-08-01<BR>ratio: 0.1084<BR>benchmarks:<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2016-06-01<BR>ratio: 0.8125<BR>benchmarks:<BR>  3D reconstruction - DTU: Acc<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2016-12-01<BR>ratio: 0.9756<BR>benchmarks:<BR>  3D reconstruction - Data3D\u2212R2N2: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2017-12-01<BR>ratio: 0.8554<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2018-04-01<BR>ratio: 0.1644<BR>benchmarks:<BR>  3D Semantic Scene Completion - NYUv2: mIoU<BR>  3D Semantic Scene Completion - SemanticKITTI: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2018-08-01<BR>ratio: 0.0244<BR>benchmarks:<BR>  3D reconstruction - Data3D\u2212R2N2: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2018-09-01<BR>ratio: 0.2022<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2018-11-01<BR>ratio: 0.8757<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2019-01-01<BR>ratio: 0.6177<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2019-08-01<BR>ratio: 0.3288<BR>benchmarks:<BR>  3D Semantic Scene Completion - NYUv2: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-05-01<BR>ratio: 0.7978<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-08-01<BR>ratio: 0.1773<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-11-01<BR>ratio: 0.3731<BR>benchmarks:<BR>  3D Semantic Scene Completion - SemanticKITTI: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2020-12-01<BR>ratio: 0.5075<BR>benchmarks:<BR>  3D Semantic Scene Completion - SemanticKITTI: mIoU<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>  3D reconstruction - DTU: Acc<BR>  Unsupervised 3D Human Pose Estimation - Human3.6M: MPJPE<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2021-04-01<BR>ratio: 0.5068<BR>benchmarks:<BR>  3D Semantic Scene Completion - NYUv2: mIoU<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2021-09-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Unsupervised 3D Human Pose Estimation - Human3.6M: MPJPE<BR>","<BR>task: 3D vision process // 3D reconstruction<BR>date: 2021-12-01<BR>ratio: 0.5142<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>  Unsupervised 3D Human Pose Estimation - Human3.6M: MPJPE<BR>","<BR>task: 3D vision process // 3D shape classification<BR>date: 2017-11-01<BR>ratio: 0.8136<BR>benchmarks:<BR>  3D shape classification - Pix3D: R-at-16<BR>  3D shape classification - Pix3D: R-at-1<BR>  3D shape classification - Pix3D: R-at-2<BR>  3D shape classification - Pix3D: R-at-32<BR>  3D shape classification - Pix3D: R-at-4<BR>  3D shape classification - Pix3D: R-at-8<BR>","<BR>task: 3D vision process // 3D shape classification<BR>date: 2018-04-01<BR>ratio: 0.2188<BR>benchmarks:<BR>  3D shape classification - Pix3D: R-at-16<BR>  3D shape classification - Pix3D: R-at-1<BR>  3D shape classification - Pix3D: R-at-2<BR>  3D shape classification - Pix3D: R-at-32<BR>  3D shape classification - Pix3D: R-at-4<BR>  3D shape classification - Pix3D: R-at-8<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2015-08-01<BR>ratio: 0.1475<BR>benchmarks:<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2015-11-01<BR>ratio: 0.3931<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-03-01<BR>ratio: 0.0844<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-06-01<BR>ratio: 0.2222<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-08-01<BR>ratio: 0.5294<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-09-01<BR>ratio: 0.2727<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-11-01<BR>ratio: 0.2563<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2016-12-01<BR>ratio: 0.3015<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2017-07-01<BR>ratio: 0.8641<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-02-01<BR>ratio: 0.0061<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-03-01<BR>ratio: 0.1868<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-04-01<BR>ratio: 0.764<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-05-01<BR>ratio: 0.4188<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-07-01<BR>ratio: 0.0656<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-09-01<BR>ratio: 0.9448<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2018-11-01<BR>ratio: 0.0041<BR>benchmarks:<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-02-01<BR>ratio: 0.039<BR>benchmarks:<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-03-01<BR>ratio: 0.8519<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-04-01<BR>ratio: 0.5217<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-06-01<BR>ratio: 0.961<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-07-01<BR>ratio: 0.7658<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-08-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Image super-resolution - PIRM-test: NIQE<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-09-01<BR>ratio: 0.7778<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-10-01<BR>ratio: 0.8359<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2019-12-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-03-01<BR>ratio: 0.3407<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-05-01<BR>ratio: 0.8441<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-07-01<BR>ratio: 0.7648<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-08-01<BR>ratio: 0.0208<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-11-01<BR>ratio: 0.2529<BR>benchmarks:<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2020-12-01<BR>ratio: 0.0513<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-05-01<BR>ratio: 0.2352<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-06-01<BR>ratio: 0.8333<BR>benchmarks:<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-08-01<BR>ratio: 0.2632<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-10-01<BR>ratio: 0.2308<BR>benchmarks:<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: 3D vision process // 3D shape reconstruction<BR>date: 2021-11-01<BR>ratio: 0.028<BR>benchmarks:<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>","<BR>task: 3D vision process // Motion forecasting<BR>date: 2021-01-01<BR>ratio: 0.8612<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>","<BR>task: 3D vision process // Motion forecasting<BR>date: 2021-03-01<BR>ratio: 0.7375<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>","<BR>task: 3D vision process // Motion forecasting<BR>date: 2021-05-01<BR>ratio: 0.1975<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>","<BR>task: 3D vision process // Motion forecasting<BR>date: 2021-11-01<BR>ratio: 0.2133<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>","<BR>task: Action detection<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action detection<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action detection<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action detection<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action detection<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action detection<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action detection<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action detection<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action detection<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action detection<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Action detection<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action detection<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action detection<BR>date: 2017-12-01<BR>ratio: 0.404<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Action detection - Multi-THUMOS: mAP<BR>","<BR>task: Action detection<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Action detection<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action detection<BR>date: 2018-03-01<BR>ratio: 0.4274<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Action detection - Multi-THUMOS: mAP<BR>","<BR>task: Action detection<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action detection<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action detection<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action detection<BR>date: 2018-12-01<BR>ratio: 0.4719<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action detection<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Action detection - UCF101-24: Video-mAP 0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action detection<BR>date: 2019-05-01<BR>ratio: 0.0807<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>","<BR>task: Action detection<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action detection<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Action detection<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action detection<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action detection<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action detection<BR>date: 2020-01-01<BR>ratio: 0.5977<BR>benchmarks:<BR>  Action detection - UCF101-24: Video-mAP 0.2<BR>","<BR>task: Action detection<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2020-06-01<BR>ratio: 0.6757<BR>benchmarks:<BR>  Online Action Detection - THUMOS'14: mAP<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action detection<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2020-11-01<BR>ratio: 0.069<BR>benchmarks:<BR>  Online Action Detection - TVSeries: mCAP<BR>","<BR>task: Action detection<BR>date: 2021-01-01<BR>ratio: 0.1724<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>  Action detection - Multi-THUMOS: mAP<BR>  Audio-visual active speaker detection - AVA-ActiveSpeaker: validation mean average precision<BR>","<BR>task: Action detection<BR>date: 2021-03-01<BR>ratio: 0.1667<BR>benchmarks:<BR>  Action detection - Multi-THUMOS: mAP<BR>","<BR>task: Action detection<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Action detection<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2021-06-01<BR>ratio: 0.8276<BR>benchmarks:<BR>  Audio-visual active speaker detection - AVA-ActiveSpeaker: validation mean average precision<BR>  Online Action Detection - TVSeries: mCAP<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action detection<BR>date: 2021-07-01<BR>ratio: 0.6552<BR>benchmarks:<BR>  Online Action Detection - THUMOS'14: mAP<BR>  Online Action Detection - TVSeries: mCAP<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action detection<BR>date: 2021-11-01<BR>ratio: 0.0259<BR>benchmarks:<BR>  Action detection - Charades: mAP<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2016-08-01<BR>ratio: 0.7274<BR>benchmarks:<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2016-11-01<BR>ratio: 0.4185<BR>benchmarks:<BR>  Action segmentation - GTEA: Acc<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>  Action segmentation - GTEA: F1@50%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2017-05-01<BR>ratio: 0.1627<BR>benchmarks:<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2018-06-01<BR>ratio: 0.302<BR>benchmarks:<BR>  Action segmentation - GTEA: Acc<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>  Action segmentation - GTEA: F1@50%<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2019-01-01<BR>ratio: 0.3253<BR>benchmarks:<BR>  Weakly Supervised Action Segmentation (Transcript) - Breakfast: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2019-03-01<BR>ratio: 0.5448<BR>benchmarks:<BR>  Action segmentation - GTEA: Acc<BR>  Action segmentation - GTEA: Edit<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>  Action segmentation - GTEA: F1@50%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2019-04-01<BR>ratio: 0.869<BR>benchmarks:<BR>  Action segmentation - Breakfast: Edit<BR>  Action segmentation - Breakfast: F1@10%<BR>  Action segmentation - Breakfast: F1@25%<BR>  Action segmentation - Breakfast: F1@50%<BR>  Weakly Supervised Action Segmentation (Transcript) - Breakfast: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2019-09-01<BR>ratio: 0.2048<BR>benchmarks:<BR>  Weakly Supervised Action Segmentation (Transcript) - Breakfast: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2020-02-01<BR>ratio: 0.0251<BR>benchmarks:<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2020-03-01<BR>ratio: 0.6505<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - 50 Salads: Edit<BR>  Action segmentation - 50 Salads: F1@10%<BR>  Action segmentation - 50 Salads: F1@25%<BR>  Action segmentation - 50 Salads: F1@50%<BR>  Action segmentation - Breakfast: Acc<BR>  Action segmentation - Breakfast: F1@10%<BR>  Action segmentation - Breakfast: F1@25%<BR>  Action segmentation - Breakfast: F1@50%<BR>  Action segmentation - GTEA: Acc<BR>  Action segmentation - GTEA: Edit<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>  Action segmentation - GTEA: F1@50%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2020-06-01<BR>ratio: 0.0962<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - GTEA: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2020-07-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - 50 Salads: Edit<BR>  Action segmentation - 50 Salads: F1@10%<BR>  Action segmentation - 50 Salads: F1@25%<BR>  Action segmentation - 50 Salads: F1@50%<BR>  Action segmentation - Breakfast: F1@50%<BR>  Action segmentation - GTEA: F1@50%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2020-11-01<BR>ratio: 0.0339<BR>benchmarks:<BR>  Action segmentation - JIGSAWS: Edit Distance<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2021-01-01<BR>ratio: 0.188<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Edit<BR>  Action segmentation - 50 Salads: F1@10%<BR>  Action segmentation - 50 Salads: F1@25%<BR>  Action segmentation - 50 Salads: F1@50%<BR>  Action segmentation - Breakfast: Acc<BR>  Action segmentation - Breakfast: F1@10%<BR>  Action segmentation - Breakfast: F1@25%<BR>  Action segmentation - Breakfast: F1@50%<BR>  Action segmentation - GTEA: Edit<BR>  Action segmentation - GTEA: F1@10%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2021-04-01<BR>ratio: 0.0253<BR>benchmarks:<BR>  Action segmentation - Breakfast: Acc<BR>  Action segmentation - GTEA: F1@10%<BR>  Action segmentation - GTEA: F1@25%<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2021-05-01<BR>ratio: 0.5155<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - Breakfast: Acc<BR>  Action segmentation - Breakfast: F1@50%<BR>  Action segmentation - GTEA: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2021-08-01<BR>ratio: 0.1325<BR>benchmarks:<BR>  Action segmentation - Breakfast: Edit<BR>  Action segmentation - Breakfast: F1@25%<BR>  Weakly Supervised Action Segmentation (Transcript) - Breakfast: Acc<BR>","<BR>task: Action localization // Action segmentation<BR>date: 2021-10-01<BR>ratio: 0.1923<BR>benchmarks:<BR>  Action segmentation - 50 Salads: Acc<BR>  Action segmentation - 50 Salads: Edit<BR>  Action segmentation - 50 Salads: F1@50%<BR>  Action segmentation - Breakfast: F1@25%<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2016-01-01<BR>ratio: 0.0539<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2016-09-01<BR>ratio: 0.5391<BR>benchmarks:<BR>  Temporal action localization - J-HMDB-21: Frame-mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2017-03-01<BR>ratio: 0.2651<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.1<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2017-05-01<BR>ratio: 0.493<BR>benchmarks:<BR>  Temporal action localization - J-HMDB-21: Frame-mAP<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.1<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>  Temporal action localization - UCF101-24: Frame-mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2018-04-01<BR>ratio: 0.4388<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2018-06-01<BR>ratio: 0.3562<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-03-01<BR>ratio: 0.5382<BR>benchmarks:<BR>  Temporal action localization - CrossTask: Recall<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-04-01<BR>ratio: 0.1727<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: Avg mAP (0.3:0.7)<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-06-01<BR>ratio: 0.0588<BR>benchmarks:<BR>  Temporal action localization - CrossTask: Recall<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-07-01<BR>ratio: 0.5256<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.75<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.95<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-09-01<BR>ratio: 0.3782<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.1<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2019-11-01<BR>ratio: 0.507<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.95<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>  Temporal action localization - J-HMDB-21: Frame-mAP<BR>  Temporal action localization - UCF101-24: Frame-mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2020-08-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.95<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2020-09-01<BR>ratio: 0.1003<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.75<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2020-11-01<BR>ratio: 0.5006<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.75<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>  Temporal action localization - THUMOS\u201914: Avg mAP (0.3:0.7)<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.1<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.2<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2020-12-01<BR>ratio: 0.174<BR>benchmarks:<BR>  Temporal action localization - THUMOS\u201914: Avg mAP (0.3:0.7)<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2021-03-01<BR>ratio: 0.2192<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.75<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2021-05-01<BR>ratio: 0.3794<BR>benchmarks:<BR>  Temporal action localization - CrossTask: Recall<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2021-06-01<BR>ratio: 0.4685<BR>benchmarks:<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.5<BR>  Temporal action localization - ActivityNet-1.3: mAP IOU-at-0.95<BR>  Temporal action localization - ActivityNet-1.3: mAP<BR>  Temporal action localization - THUMOS\u201914: Avg mAP (0.3:0.7)<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.3<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.4<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.5<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.6<BR>  Temporal action localization - THUMOS\u201914: mAP IOU-at-0.7<BR>","<BR>task: Action localization // Temporal action localization<BR>date: 2021-09-01<BR>ratio: 0.0235<BR>benchmarks:<BR>  Temporal action localization - CrossTask: Recall<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2018-12-01<BR>ratio: 0.4719<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // 3D human action recognition<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2014-12-01<BR>ratio: 0.4206<BR>benchmarks:<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2015-03-01<BR>ratio: 0.726<BR>benchmarks:<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2015-05-01<BR>ratio: 0.2726<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-01-01<BR>ratio: 0.1095<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-06-01<BR>ratio: 0.3057<BR>benchmarks:<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-08-01<BR>ratio: 0.1598<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-03-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-04-01<BR>ratio: 0.2983<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-06-01<BR>ratio: 0.4678<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-10-01<BR>ratio: 0.1673<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-11-01<BR>ratio: 0.7219<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2017-12-01<BR>ratio: 0.2303<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-07-01<BR>ratio: 0.6555<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - Jester: Val<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-09-01<BR>ratio: 0.0676<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-10-01<BR>ratio: 0.0071<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-11-01<BR>ratio: 0.5667<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2018-12-01<BR>ratio: 0.6875<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-03-01<BR>ratio: 0.3298<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-05-01<BR>ratio: 0.0096<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-06-01<BR>ratio: 0.049<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-08-01<BR>ratio: 0.8605<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-10-01<BR>ratio: 0.108<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-11-01<BR>ratio: 0.9701<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-01-01<BR>ratio: 0.1804<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-04-01<BR>ratio: 0.348<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - AVA v2.2: mAP<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-07-01<BR>ratio: 0.4943<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-08-01<BR>ratio: 0.3977<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-10-01<BR>ratio: 0.9051<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-11-01<BR>ratio: 0.1022<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2020-12-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-01-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-02-01<BR>ratio: 0.7765<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-03-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-06-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-07-01<BR>ratio: 0.7047<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-08-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-09-01<BR>ratio: 0.2242<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-10-01<BR>ratio: 0.2404<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-11-01<BR>ratio: 0.3636<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>","<BR>task: Action localization // Temporal action localization // Action recognition<BR>date: 2021-12-01<BR>ratio: 0.6265<BR>benchmarks:<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2016-09-01<BR>ratio: 0.0264<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2016-11-01<BR>ratio: 0.1806<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2017-08-01<BR>ratio: 0.0725<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2018-11-01<BR>ratio: 0.1793<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2019-09-01<BR>ratio: 0.2181<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2019-11-01<BR>ratio: 0.9701<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2020-04-01<BR>ratio: 0.348<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2020-06-01<BR>ratio: 0.0815<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2020-08-01<BR>ratio: 0.0181<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2021-03-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Action recognition in videos<BR>date: 2021-04-01<BR>ratio: 0.0815<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Activity recognition in videos<BR>date: 2014-12-01<BR>ratio: 0.6093<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Activity recognition in videos<BR>date: 2015-05-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Activity recognition in videos<BR>date: 2016-05-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2018-06-01<BR>ratio: 0.3491<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (test)<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2018-11-01<BR>ratio: 0.0898<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2019-07-01<BR>ratio: 0.1321<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2020-09-01<BR>ratio: 0.357<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2020-11-01<BR>ratio: 0.1538<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Temporal action proposal generation<BR>date: 2021-10-01<BR>ratio: 0.724<BR>benchmarks:<BR>  Temporal action proposal generation - ActivityNet-1.3: AR@100<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (test)<BR>  Temporal action proposal generation - ActivityNet-1.3: AUC (val)<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2017-12-01<BR>ratio: 0.1013<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2018-07-01<BR>ratio: 0.1867<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-05-01<BR>ratio: 0.3964<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-06-01<BR>ratio: 0.031<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-08-01<BR>ratio: 0.0286<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-10-01<BR>ratio: 0.0443<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2019-11-01<BR>ratio: 0.2564<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP@0.5:0.95<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2020-03-01<BR>ratio: 0.5294<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2020-06-01<BR>ratio: 0.3846<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP@0.5:0.95<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1-0.5)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1:0.7)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.3-0.7)<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2020-12-01<BR>ratio: 0.2247<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - BEOID: mAP-at-0.5<BR>  Weakly supervised action localization - BEOID: mAP@0.1:0.7<BR>  Weakly supervised action localization - GTEA: mAP-at-0.5<BR>  Weakly supervised action localization - GTEA: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1-0.5)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1:0.7)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.3-0.7)<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2021-03-01<BR>ratio: 0.4726<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - GTEA: mAP-at-0.5<BR>  Weakly supervised action localization - GTEA: mAP@0.1:0.7<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2021-04-01<BR>ratio: 0.2793<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP@0.5:0.95<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2021-07-01<BR>ratio: 0.0759<BR>benchmarks:<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>","<BR>task: Action localization // Temporal action localization // Weakly supervised action localization<BR>date: 2021-08-01<BR>ratio: 0.8385<BR>benchmarks:<BR>  Weakly supervised action localization - ActivityNet-1.2: Mean mAP<BR>  Weakly supervised action localization - ActivityNet-1.2: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP-at-0.5<BR>  Weakly supervised action localization - ActivityNet-1.3: mAP@0.5:0.95<BR>  Weakly supervised action localization - BEOID: mAP-at-0.5<BR>  Weakly supervised action localization - BEOID: mAP@0.1:0.7<BR>  Weakly supervised action localization - GTEA: mAP-at-0.5<BR>  Weakly supervised action localization - GTEA: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS 2014: mAP-at-0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.5<BR>  Weakly supervised action localization - THUMOS 2014: mAP@0.1:0.7<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1-0.5)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.1:0.7)<BR>  Weakly supervised action localization - THUMOS14: avg-mAP (0.3-0.7)<BR>  Weakly supervised action localization - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Action quality assessment<BR>date: 2020-06-01<BR>ratio: 0.7958<BR>benchmarks:<BR>  Action quality assessment - AQA-7: Spearman Correlation<BR>  Action quality assessment - MTL-AQA: Spearman Correlation<BR>","<BR>task: Action quality assessment<BR>date: 2021-02-01<BR>ratio: 0.0897<BR>benchmarks:<BR>  Action quality assessment - MTL-AQA: Spearman Correlation<BR>","<BR>task: Action quality assessment<BR>date: 2021-08-01<BR>ratio: 0.4209<BR>benchmarks:<BR>  Action quality assessment - AQA-7: Spearman Correlation<BR>  Action quality assessment - MTL-AQA: Spearman Correlation<BR>","<BR>task: Activity recognition<BR>date: 2014-12-01<BR>ratio: 0.4206<BR>benchmarks:<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>","<BR>task: Activity recognition<BR>date: 2015-03-01<BR>ratio: 0.726<BR>benchmarks:<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2015-05-01<BR>ratio: 0.2726<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Activity recognition<BR>date: 2016-01-01<BR>ratio: 0.1095<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Activity recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Activity recognition<BR>date: 2016-06-01<BR>ratio: 0.3057<BR>benchmarks:<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Activity recognition<BR>date: 2016-08-01<BR>ratio: 0.8371<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Multimodal activity recognition - EV-Action: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2017-03-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-04-01<BR>ratio: 0.2983<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Multimodal activity recognition - EV-Action: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition<BR>date: 2017-06-01<BR>ratio: 0.4678<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Activity recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2017-10-01<BR>ratio: 0.1673<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Activity recognition<BR>date: 2017-11-01<BR>ratio: 0.7219<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Activity recognition<BR>date: 2017-12-01<BR>ratio: 0.2303<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Activity recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Activity recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-07-01<BR>ratio: 0.6555<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - Jester: Val<BR>","<BR>task: Activity recognition<BR>date: 2018-09-01<BR>ratio: 0.0676<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Activity recognition<BR>date: 2018-10-01<BR>ratio: 0.0071<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>","<BR>task: Activity recognition<BR>date: 2018-11-01<BR>ratio: 0.5667<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Egocentric activity recognition - EGTEA: Average Accuracy<BR>  Group activity recognition - Collective Activity: Accuracy<BR>  Group activity recognition - Volleyball: Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-12-01<BR>ratio: 0.6875<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S2)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-03-01<BR>ratio: 0.3298<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>","<BR>task: Activity recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Group activity recognition - Collective Activity: Accuracy<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2019-05-01<BR>ratio: 0.4124<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S1)<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S2)<BR>","<BR>task: Activity recognition<BR>date: 2019-06-01<BR>ratio: 0.049<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Activity recognition<BR>date: 2019-08-01<BR>ratio: 0.8605<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S1)<BR>","<BR>task: Activity recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Activity recognition<BR>date: 2019-10-01<BR>ratio: 0.108<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Activity recognition<BR>date: 2019-11-01<BR>ratio: 0.9701<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-01-01<BR>ratio: 0.3226<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S1)<BR>  Egocentric activity recognition - EPIC-KITCHENS-55: Actions Top-1 (S2)<BR>","<BR>task: Activity recognition<BR>date: 2020-02-01<BR>ratio: 0.0911<BR>benchmarks:<BR>  Egocentric activity recognition - EGTEA: Average Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-03-01<BR>ratio: 0.459<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Group activity recognition - Volleyball: Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2020-04-01<BR>ratio: 0.348<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - AVA v2.2: mAP<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-07-01<BR>ratio: 0.4943<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2020-08-01<BR>ratio: 0.3977<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Multimodal activity recognition - UTD-MHAD: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-10-01<BR>ratio: 0.9051<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2020-11-01<BR>ratio: 0.7836<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Egocentric activity recognition - EGTEA: Average Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2020-12-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-01-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-02-01<BR>ratio: 0.7765<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>","<BR>task: Activity recognition<BR>date: 2021-03-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Multimodal activity recognition - UTD-MHAD: Accuracy (CS)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Activity recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2021-06-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Activity recognition<BR>date: 2021-07-01<BR>ratio: 0.7047<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-08-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Group activity recognition - Volleyball: Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-09-01<BR>ratio: 0.2242<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2021-10-01<BR>ratio: 0.2404<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Activity recognition<BR>date: 2021-11-01<BR>ratio: 0.3636<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>","<BR>task: Activity recognition<BR>date: 2021-12-01<BR>ratio: 0.6265<BR>benchmarks:<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>  Group activity recognition - Collective Activity: Accuracy<BR>  Group activity recognition - Volleyball: Accuracy<BR>","<BR>task: Ad-hoc video search<BR>date: 2020-09-01<BR>ratio: 0.1571<BR>benchmarks:<BR>  Ad-hoc video search - TRECVID-AVS16 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS17 (IACC.3): infAP<BR>","<BR>task: Ad-hoc video search<BR>date: 2020-11-01<BR>ratio: 0.2692<BR>benchmarks:<BR>  Ad-hoc video search - TRECVID-AVS16 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS17 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS18 (IACC.3): infAP<BR>","<BR>task: Ad-hoc video search<BR>date: 2021-12-01<BR>ratio: 0.8169<BR>benchmarks:<BR>  Ad-hoc video search - TRECVID-AVS16 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS17 (IACC.3): infAP<BR>  Ad-hoc video search - TRECVID-AVS18 (IACC.3): infAP<BR>","<BR>task: Anomaly detection<BR>date: 2017-01-01<BR>ratio: 0.0344<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2017-10-01<BR>ratio: 0.0887<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2017-12-01<BR>ratio: 0.3057<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: AUC<BR>  Anomaly detection - ShanghaiTech: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2018-01-01<BR>ratio: 0.9286<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>  Anomaly detection - CHUK Avenue: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Anomaly detection<BR>date: 2018-02-01<BR>ratio: 0.9509<BR>benchmarks:<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Anomaly detection<BR>date: 2018-04-01<BR>ratio: 0.2166<BR>benchmarks:<BR>  Anomaly detection - ShanghaiTech: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2018-12-01<BR>ratio: 0.1592<BR>benchmarks:<BR>  Anomaly detection - ShanghaiTech: AUC<BR>  Anomaly detection - ShanghaiTech: RBDC<BR>","<BR>task: Anomaly detection<BR>date: 2019-06-01<BR>ratio: 0.0818<BR>benchmarks:<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2019-11-01<BR>ratio: 0.5164<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-01-01<BR>ratio: 0.679<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: RBDC<BR>  Anomaly detection - CHUK Avenue: TBDC<BR>","<BR>task: Anomaly detection<BR>date: 2020-02-01<BR>ratio: 0.7751<BR>benchmarks:<BR>  Anomaly detection - Anomaly Detection on Anomaly Detection on Unlabeled ImageNet-30 vs Flowers-102: ROC-AUC<BR>  Anomaly detection - Anomaly Detection on Unlabeled ImageNet-30 vs CUB-200: ROC-AUC<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-05-01<BR>ratio: 0.6753<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Detection AUROC<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-07-01<BR>ratio: 0.5737<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>  Anomaly detection - Anomaly Detection on Anomaly Detection on Unlabeled ImageNet-30 vs Flowers-102: ROC-AUC<BR>  Anomaly detection - Anomaly Detection on Unlabeled CIFAR-10 vs LSUN (Fix): ROC-AUC<BR>  Anomaly detection - One-class CIFAR-100: AUROC<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-08-01<BR>ratio: 0.9628<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: AUC<BR>  Anomaly detection - CHUK Avenue: RBDC<BR>  Anomaly detection - CIFAR-10: ROC AUC<BR>  Anomaly detection - ShanghaiTech: AUC<BR>  Anomaly detection - ShanghaiTech: RBDC<BR>  Anomaly detection - ShanghaiTech: TBDC<BR>  Anomaly detection - UBnormal: AUC<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Anomaly detection<BR>date: 2020-10-01<BR>ratio: 0.1727<BR>benchmarks:<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2020-11-01<BR>ratio: 0.375<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Detection AUROC<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>  Anomaly detection - ShanghaiTech: RBDC<BR>  Anomaly detection - ShanghaiTech: TBDC<BR>","<BR>task: Anomaly detection<BR>date: 2021-01-01<BR>ratio: 0.3527<BR>benchmarks:<BR>  Anomaly detection - CIFAR-10: ROC AUC<BR>  Anomaly detection - One-class CIFAR-100: AUROC<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Anomaly detection<BR>date: 2021-02-01<BR>ratio: 0.3956<BR>benchmarks:<BR>  Anomaly detection - UBnormal: AUC<BR>","<BR>task: Anomaly detection<BR>date: 2021-03-01<BR>ratio: 0.785<BR>benchmarks:<BR>  Anomaly detection - Anomaly Detection on Unlabeled CIFAR-10 vs LSUN (Fix): ROC-AUC<BR>  Anomaly detection - Fishyscapes L&F: AP<BR>  Anomaly detection - Fishyscapes L&F: FPR95<BR>  Anomaly detection - Fishyscapes: AP<BR>  Anomaly detection - Fishyscapes: FPR<BR>  Anomaly detection - Road Anomaly: AP<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2021-05-01<BR>ratio: 0.0602<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2021-06-01<BR>ratio: 0.3036<BR>benchmarks:<BR>  Anomaly detection - MVTec AD: Detection AUROC<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>  Anomaly detection - One-class CIFAR-100: AUROC<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2021-07-01<BR>ratio: 0.215<BR>benchmarks:<BR>  Anomaly detection - Fishyscapes L&F: FPR95<BR>  Anomaly detection - Fishyscapes: FPR<BR>  Anomaly detection - MVTec AD: Segmentation AUROC<BR>","<BR>task: Anomaly detection<BR>date: 2021-11-01<BR>ratio: 0.5476<BR>benchmarks:<BR>  Anomaly detection - CHUK Avenue: AUC<BR>  Anomaly detection - CHUK Avenue: RBDC<BR>  Anomaly detection - CHUK Avenue: TBDC<BR>  Anomaly detection - Fishyscapes L&F: AP<BR>  Anomaly detection - Fishyscapes L&F: FPR95<BR>  Anomaly detection - Fishyscapes: AP<BR>  Anomaly detection - Road Anomaly: AP<BR>  Anomaly detection - ShanghaiTech: AUC<BR>  Anomaly detection - ShanghaiTech: RBDC<BR>  Anomaly detection - ShanghaiTech: TBDC<BR>","<BR>task: Anomaly detection<BR>date: 2021-12-01<BR>ratio: 0.7184<BR>benchmarks:<BR>  Anomaly detection - Anomaly Detection on Anomaly Detection on Unlabeled ImageNet-30 vs Flowers-102: ROC-AUC<BR>  Anomaly detection - Anomaly Detection on Unlabeled CIFAR-10 vs LSUN (Fix): ROC-AUC<BR>  Anomaly detection - Anomaly Detection on Unlabeled ImageNet-30 vs CUB-200: ROC-AUC<BR>  Anomaly detection - Fishyscapes L&F: AP<BR>  Anomaly detection - Fishyscapes L&F: FPR95<BR>  Anomaly detection - One-class CIFAR-100: AUROC<BR>  Anomaly detection - One-class CIFAR-10: AUROC<BR>  Anomaly detection - Unlabeled CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Autonomous vehicle task<BR>date: 2015-01-01<BR>ratio: 0.1985<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>","<BR>task: Autonomous vehicle task<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2015-07-01<BR>ratio: 0.2795<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>","<BR>task: Autonomous vehicle task<BR>date: 2015-10-01<BR>ratio: 0.1082<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2016-12-01<BR>ratio: 0.0165<BR>benchmarks:<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-02-01<BR>ratio: 0.1149<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-05-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-08-01<BR>ratio: 0.8093<BR>benchmarks:<BR>  Pedestrian detection - TJU-Ped-traffic: HO (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R+HO (miss rate)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-10-01<BR>ratio: 0.877<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2017-11-01<BR>ratio: 0.6651<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>  Pedestrian detection - CityPersons: Reasonable MR^-2<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-03-01<BR>ratio: 0.0559<BR>benchmarks:<BR>  Lane detection - TuSimple: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>  Pedestrian detection - CityPersons: Reasonable MR^-2<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Lane detection - TuSimple: F1 score<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-07-01<BR>ratio: 0.213<BR>benchmarks:<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-09-01<BR>ratio: 0.3514<BR>benchmarks:<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>","<BR>task: Autonomous vehicle task<BR>date: 2018-12-01<BR>ratio: 0.872<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-01-01<BR>ratio: 0.128<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-03-01<BR>ratio: 0.0412<BR>benchmarks:<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-04-01<BR>ratio: 0.621<BR>benchmarks:<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>  Pedestrian detection - TJU-Ped-traffic: HO (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R (miss rate)<BR>  Pedestrian detection - TJU-Ped-traffic: R+HO (miss rate)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - CUB: Inception score<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-07-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-08-01<BR>ratio: 0.0782<BR>benchmarks:<BR>  Lane detection - TuSimple: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-10-01<BR>ratio: 0.6745<BR>benchmarks:<BR>  Pedestrian attribute recognition - PA-100K: Accuracy<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-11-01<BR>ratio: 0.6864<BR>benchmarks:<BR>  Autonomous driving - CARLA Leaderboard: Driving Score<BR>  Autonomous driving - CARLA Leaderboard: Route Completion<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-02-01<BR>ratio: 0.3553<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>  Pedestrian detection - CityPersons: Reasonable MR^-2<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-03-01<BR>ratio: 0.3739<BR>benchmarks:<BR>  Pedestrian detection - Caltech: Reasonable Miss Rate<BR>  Pedestrian detection - CityPersons: Heavy MR^-2<BR>  Pedestrian detection - CityPersons: Partial MR^-2<BR>  Pedestrian detection - CityPersons: Reasonable MR^-2<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-05-01<BR>ratio: 0.3255<BR>benchmarks:<BR>  Lane detection - TuSimple: F1 score<BR>  Pedestrian attribute recognition - PA-100K: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-08-01<BR>ratio: 0.1434<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>  Lane detection - TuSimple: Accuracy<BR>  Lane detection - TuSimple: F1 score<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-10-01<BR>ratio: 0.7017<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>  Lane detection - LLAMAS: F1<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2020-12-01<BR>ratio: 0.1486<BR>benchmarks:<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-01-01<BR>ratio: 0.8612<BR>benchmarks:<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-03-01<BR>ratio: 0.7375<BR>benchmarks:<BR>  Lane detection - CULane: F1 score<BR>  Lane detection - LLAMAS: F1<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=1)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minFDE (K=1)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Autonomous driving - CARLA Leaderboard: Route Completion<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-05-01<BR>ratio: 0.4759<BR>benchmarks:<BR>  Autonomous driving - CARLA Leaderboard: Driving Score<BR>  Autonomous driving - CARLA Leaderboard: Route Completion<BR>  CARLA MAP Leaderboard - CARLA: Driving score<BR>  CARLA MAP Leaderboard - CARLA: Infraction penalty<BR>  CARLA MAP Leaderboard - CARLA: Route completion<BR>  Lane detection - CULane: F1 score<BR>  Lane detection - TuSimple: Accuracy<BR>  Lane detection - TuSimple: F1 score<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-10-01<BR>ratio: 0.7095<BR>benchmarks:<BR>  Lane detection - TuSimple: Accuracy<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-11-01<BR>ratio: 0.9369<BR>benchmarks:<BR>  Autonomous driving - CARLA Leaderboard: Driving Score<BR>  Autonomous driving - CARLA Leaderboard: Route Completion<BR>  CARLA MAP Leaderboard - CARLA: Driving score<BR>  CARLA MAP Leaderboard - CARLA: Infraction penalty<BR>  CARLA MAP Leaderboard - CARLA: Route completion<BR>  Motion forecasting - Argoverse CVPR 2020: brier-minFDE (K=6)<BR>  Motion forecasting - Argoverse CVPR 2020: minADE (K=6)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Autonomous vehicle task<BR>date: 2021-12-01<BR>ratio: 0.0507<BR>benchmarks:<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Biomedical vision process<BR>date: 2015-11-01<BR>ratio: 0.2744<BR>benchmarks:<BR>  Medical image segmentation - RITE: Jaccard Index<BR>","<BR>task: Biomedical vision process<BR>date: 2016-02-01<BR>ratio: 0.5926<BR>benchmarks:<BR>  Breast tumour classification - PCam: AUC<BR>  Colorectal gland segmentation - CRAG: Dice<BR>  Colorectal gland segmentation - CRAG: F1-score<BR>  Colorectal gland segmentation - CRAG: Hausdorff Distance (mm)<BR>","<BR>task: Biomedical vision process<BR>date: 2016-11-01<BR>ratio: 0.4636<BR>benchmarks:<BR>  Nuclear segmentation - Cell17: Dice<BR>  Nuclear segmentation - Cell17: Hausdorff<BR>","<BR>task: Biomedical vision process<BR>date: 2016-12-01<BR>ratio: 0.5517<BR>benchmarks:<BR>  Multi-tissue nucleus segmentation - Kumar: Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2017-03-01<BR>ratio: 0.779<BR>benchmarks:<BR>  Nuclear segmentation - Cell17: Dice<BR>  Nuclear segmentation - Cell17: F1-score<BR>  Nuclear segmentation - Cell17: Hausdorff<BR>","<BR>task: Biomedical vision process<BR>date: 2017-07-01<BR>ratio: 0.874<BR>benchmarks:<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Biomedical vision process<BR>date: 2017-09-01<BR>ratio: 0.9095<BR>benchmarks:<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Dice<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : IoU<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Precision<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Recall<BR>","<BR>task: Biomedical vision process<BR>date: 2017-11-01<BR>ratio: 0.6809<BR>benchmarks:<BR>  Breast tumour classification - PCam: AUC<BR>  Colorectal gland segmentation - CRAG: Dice<BR>  Colorectal gland segmentation - CRAG: F1-score<BR>  Colorectal gland segmentation - CRAG: Hausdorff Distance (mm)<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Precision<BR>  Lung nodule segmentation - LUNA: AUC<BR>  Lung nodule segmentation - LUNA: F1 score<BR>  Multi-tissue nucleus segmentation - Kumar: Dice<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>  Retinal vessel segmentation - DRIVE: F1 score<BR>  Retinal vessel segmentation - STARE: F1 score<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: AUC<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: F1 score<BR>","<BR>task: Biomedical vision process<BR>date: 2018-02-01<BR>ratio: 0.8529<BR>benchmarks:<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - STARE: F1 score<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: AUC<BR>  Skin cancer segmentation - Kaggle Skin Lesion Segmentation: F1 score<BR>","<BR>task: Biomedical vision process<BR>date: 2018-06-01<BR>ratio: 0.5378<BR>benchmarks:<BR>  Colorectal gland segmentation - CRAG: F1-score<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>  Retinal vessel segmentation - DRIVE: F1 score<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Biomedical vision process<BR>date: 2018-07-01<BR>ratio: 0.2533<BR>benchmarks:<BR>  Medical image segmentation - Kvasir-SEG: Average MAE<BR>  Medical image segmentation - Kvasir-SEG: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: max E-Measure<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Sensitivity<BR>","<BR>task: Biomedical vision process<BR>date: 2018-09-01<BR>ratio: 0.264<BR>benchmarks:<BR>  Nuclear segmentation - Cell17: Dice<BR>  Nuclear segmentation - Cell17: F1-score<BR>  Nuclear segmentation - Cell17: Hausdorff<BR>","<BR>task: Biomedical vision process<BR>date: 2018-10-01<BR>ratio: 0.0608<BR>benchmarks:<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>","<BR>task: Biomedical vision process<BR>date: 2018-12-01<BR>ratio: 0.4622<BR>benchmarks:<BR>  Multi-tissue nucleus segmentation - Kumar: Dice<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Biomedical vision process<BR>date: 2019-07-01<BR>ratio: 0.0905<BR>benchmarks:<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Dice<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : IoU<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Precision<BR>  Retinal vessel segmentation - DRIVE: Accuracy<BR>","<BR>task: Biomedical vision process<BR>date: 2019-08-01<BR>ratio: 0.8699<BR>benchmarks:<BR>  Brain tumor segmentation - BRATS-2013: Dice Score<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Dice<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Precision<BR>  Lesion segmentation - Anatomical Tracings of Lesions After Stroke (ATLAS) : Recall<BR>  Lung nodule segmentation - LUNA: AUC<BR>  Lung nodule segmentation - LUNA: F1 score<BR>","<BR>task: Biomedical vision process<BR>date: 2019-10-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Biomedical vision process<BR>date: 2019-12-01<BR>ratio: 0.1061<BR>benchmarks:<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>","<BR>task: Biomedical vision process<BR>date: 2020-01-01<BR>ratio: 0.3765<BR>benchmarks:<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2020-03-01<BR>ratio: 0.5628<BR>benchmarks:<BR>  Lesion segmentation - ISIC 2018: Dice Score<BR>","<BR>task: Biomedical vision process<BR>date: 2020-04-01<BR>ratio: 0.5633<BR>benchmarks:<BR>  Breast tumour classification - PCam: AUC<BR>  Colorectal gland segmentation - CRAG: Dice<BR>  Colorectal gland segmentation - CRAG: F1-score<BR>  Colorectal gland segmentation - CRAG: Hausdorff Distance (mm)<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>  Retinal vessel segmentation - DRIVE: Accuracy<BR>","<BR>task: Biomedical vision process<BR>date: 2020-06-01<BR>ratio: 0.8479<BR>benchmarks:<BR>  Lesion segmentation - ISIC 2018: Dice Score<BR>  Medical image segmentation - 2018 Data Science Bowl: Dice<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - Kvasir-SEG: Average MAE<BR>  Medical image segmentation - Kvasir-SEG: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: max E-Measure<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): S measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): weighted F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): S-Measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean E-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): weighted F-measure<BR>","<BR>task: Biomedical vision process<BR>date: 2020-07-01<BR>ratio: 0.9995<BR>benchmarks:<BR>  Brain tumor segmentation - BRATS-2013: Dice Score<BR>  Liver segmentation - LiTS2017: Dice<BR>  Liver segmentation - LiTS2017: IoU<BR>","<BR>task: Biomedical vision process<BR>date: 2020-08-01<BR>ratio: 0.6235<BR>benchmarks:<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): S measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): weighted F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Dice<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): S-Measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean E-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): weighted F-measure<BR>","<BR>task: Biomedical vision process<BR>date: 2020-10-01<BR>ratio: 0.7256<BR>benchmarks:<BR>  Liver segmentation - LiTS2017: Dice<BR>  Liver segmentation - LiTS2017: IoU<BR>  Medical image segmentation - RITE: Jaccard Index<BR>","<BR>task: Biomedical vision process<BR>date: 2021-01-01<BR>ratio: 0.7792<BR>benchmarks:<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - CVC-ColonDB: mIoU<BR>  Medical image segmentation - CVC-ColonDB: mean Dice<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: mean Dice<BR>  Medical image segmentation - Kvasir-SEG: Average MAE<BR>  Medical image segmentation - Kvasir-SEG: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: mIoU<BR>  Medical image segmentation - Kvasir-SEG: max E-Measure<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>  Retinal vessel segmentation - CHASE_DB1: F1 score<BR>  Retinal vessel segmentation - DRIVE: AUC<BR>  Retinal vessel segmentation - DRIVE: Accuracy<BR>  Retinal vessel segmentation - DRIVE: F1 score<BR>","<BR>task: Biomedical vision process<BR>date: 2021-02-01<BR>ratio: 0.463<BR>benchmarks:<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - CVC-ColonDB: mIoU<BR>  Medical image segmentation - CVC-ColonDB: mean Dice<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: mean Dice<BR>  Medical image segmentation - Kvasir-SEG: mIoU<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2021-03-01<BR>ratio: 0.9399<BR>benchmarks:<BR>  Medical image segmentation - 2018 Data Science Bowl: Dice<BR>  Medical image segmentation - 2018 Data Science Bowl: Recall<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - Medical Segmentation Decathlon: Dice (Average)<BR>  Medical image segmentation - Medical Segmentation Decathlon: NSD<BR>  Medical image segmentation - Synapse multi-organ CT: Avg DSC<BR>  Retinal vessel segmentation - CHASE_DB1: AUC<BR>","<BR>task: Biomedical vision process<BR>date: 2021-05-01<BR>ratio: 0.5519<BR>benchmarks:<BR>  Medical image segmentation - 2018 Data Science Bowl: Dice<BR>  Medical image segmentation - 2018 Data Science Bowl: Recall<BR>  Medical image segmentation - Automatic Cardiac Diagnosis Challenge (ACDC): Avg DSC<BR>  Medical image segmentation - CVC-ClinicDB: mean Dice<BR>  Medical image segmentation - Kvasir-SEG: mIoU<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): Sensitivity<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Easy (Unseen): weighted F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): mean F-measure<BR>  Video Polyp Segmentation - SUN-SEG-Hard (Unseen): weighted F-measure<BR>","<BR>task: Biomedical vision process<BR>date: 2021-07-01<BR>ratio: 0.8784<BR>benchmarks:<BR>  Medical image segmentation - CVC-ColonDB: Average MAE<BR>  Medical image segmentation - CVC-ColonDB: S-Measure<BR>  Medical image segmentation - CVC-ColonDB: mIoU<BR>  Medical image segmentation - CVC-ColonDB: max E-Measure<BR>  Medical image segmentation - CVC-ColonDB: mean Dice<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: S-Measure<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: mean Dice<BR>","<BR>task: Biomedical vision process<BR>date: 2021-08-01<BR>ratio: 0.3696<BR>benchmarks:<BR>  Medical image segmentation - CVC-ColonDB: Average MAE<BR>  Medical image segmentation - CVC-ColonDB: S-Measure<BR>  Medical image segmentation - CVC-ColonDB: max E-Measure<BR>  Medical image segmentation - ETIS-LARIBPOLYPDB: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: Average MAE<BR>  Medical image segmentation - Kvasir-SEG: S-Measure<BR>  Medical image segmentation - Kvasir-SEG: max E-Measure<BR>","<BR>task: Biomedical vision process<BR>date: 2021-09-01<BR>ratio: 0.8766<BR>benchmarks:<BR>  Lesion segmentation - ISIC 2018: Dice Score<BR>  Liver segmentation - LiTS2017: IoU<BR>  Medical image segmentation - Automatic Cardiac Diagnosis Challenge (ACDC): Avg DSC<BR>","<BR>task: Biomedical vision process<BR>date: 2021-11-01<BR>ratio: 0.9494<BR>benchmarks:<BR>  Medical image segmentation - Kvasir-SEG: mean Dice<BR>  Medical image segmentation - Medical Segmentation Decathlon: Dice (Average)<BR>  Medical image segmentation - Medical Segmentation Decathlon: NSD<BR>  Medical image segmentation - Synapse multi-organ CT: Avg DSC<BR>","<BR>task: Blind face restoration<BR>date: 2020-05-01<BR>ratio: 0.0012<BR>benchmarks:<BR>  Blind face restoration - CelebA-Test: NIQE<BR>","<BR>task: Blind face restoration<BR>date: 2020-09-01<BR>ratio: 0.5065<BR>benchmarks:<BR>  Blind face restoration - CelebA-Test: FID<BR>","<BR>task: Blind face restoration<BR>date: 2021-01-01<BR>ratio: 0.9988<BR>benchmarks:<BR>  Blind face restoration - CelebA-Test: FID<BR>  Blind face restoration - CelebA-Test: NIQE<BR>","<BR>task: Continual learning<BR>date: 2016-11-01<BR>ratio: 0.7858<BR>benchmarks:<BR>  Class-incremental learning - cifar100: 10-stage average accuracy<BR>","<BR>task: Continual learning<BR>date: 2017-05-01<BR>ratio: 0.5012<BR>benchmarks:<BR>  Continual learning - visual domain decathlon (10 tasks): Avg. Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): decathlon discipline (Score)<BR>","<BR>task: Continual learning<BR>date: 2017-11-01<BR>ratio: 0.2602<BR>benchmarks:<BR>  Continual learning - CUBS (Fine-grained 6 Tasks): Accuracy<BR>","<BR>task: Continual learning<BR>date: 2018-01-01<BR>ratio: 0.9942<BR>benchmarks:<BR>  Continual learning - 20Newsgroup (10 tasks): F1 - macro<BR>  Continual learning - ASC (19 tasks): F1 - macro<BR>  Continual learning - CUBS (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - DSC (10 tasks): F1 - macro<BR>  Continual learning - Flowers (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Sketch (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Stanford Cars (Fine-grained 6 Tasks): Accuracy<BR>","<BR>task: Continual learning<BR>date: 2018-03-01<BR>ratio: 0.3911<BR>benchmarks:<BR>  Continual learning - visual domain decathlon (10 tasks): Avg. Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): decathlon discipline (Score)<BR>","<BR>task: Continual learning<BR>date: 2018-09-01<BR>ratio: 0.0846<BR>benchmarks:<BR>  Continual learning - ASC (19 tasks): F1 - macro<BR>","<BR>task: Continual learning<BR>date: 2019-02-01<BR>ratio: 0.0773<BR>benchmarks:<BR>  Continual learning - visual domain decathlon (10 tasks): decathlon discipline (Score)<BR>","<BR>task: Continual learning<BR>date: 2019-06-01<BR>ratio: 0.5793<BR>benchmarks:<BR>  Continual learning - Cifar100 (20 tasks): Average Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): Avg. Accuracy<BR>  Continual learning - visual domain decathlon (10 tasks): decathlon discipline (Score)<BR>","<BR>task: Continual learning<BR>date: 2019-09-01<BR>ratio: 0.0942<BR>benchmarks:<BR>  Continual learning - ASC (19 tasks): F1 - macro<BR>","<BR>task: Continual learning<BR>date: 2019-10-01<BR>ratio: 0.8858<BR>benchmarks:<BR>  Continual learning - Cifar100 (20 tasks): Average Accuracy<BR>  Continual learning - Flowers (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Sketch (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Stanford Cars (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Wikiart (Fine-grained 6 Tasks): Accuracy<BR>","<BR>task: Continual learning<BR>date: 2020-03-01<BR>ratio: 0.3465<BR>benchmarks:<BR>  Continual learning - ImageNet-50 (5 tasks) : Accuracy<BR>","<BR>task: Continual learning<BR>date: 2020-04-01<BR>ratio: 0.1188<BR>benchmarks:<BR>  Class-incremental learning - cifar100: 10-stage average accuracy<BR>","<BR>task: Continual learning<BR>date: 2020-12-01<BR>ratio: 0.5187<BR>benchmarks:<BR>  Continual learning - Flowers (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - ImageNet-50 (5 tasks) : Accuracy<BR>  Continual learning - Sketch (Fine-grained 6 Tasks): Accuracy<BR>  Continual learning - Wikiart (Fine-grained 6 Tasks): Accuracy<BR>","<BR>task: Continual learning<BR>date: 2021-02-01<BR>ratio: 0.1348<BR>benchmarks:<BR>  Continual learning - Cifar100 (20 tasks): Average Accuracy<BR>  Continual learning - ImageNet-50 (5 tasks) : Accuracy<BR>","<BR>task: Continual learning<BR>date: 2021-06-01<BR>ratio: 0.5089<BR>benchmarks:<BR>  Continual learning - Cifar100 (20 tasks): Average Accuracy<BR>","<BR>task: Continual learning<BR>date: 2021-07-01<BR>ratio: 0.0954<BR>benchmarks:<BR>  Class-incremental learning - cifar100: 10-stage average accuracy<BR>","<BR>task: Continual learning<BR>date: 2021-12-01<BR>ratio: 0.5533<BR>benchmarks:<BR>  Continual learning - 20Newsgroup (10 tasks): F1 - macro<BR>  Continual learning - ASC (19 tasks): F1 - macro<BR>  Continual learning - DSC (10 tasks): F1 - macro<BR>","<BR>task: Crowds<BR>date: 2015-11-01<BR>ratio: 0.1962<BR>benchmarks:<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2015-12-01<BR>ratio: 0.3487<BR>benchmarks:<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2016-01-01<BR>ratio: 0.5548<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2016-08-01<BR>ratio: 0.1177<BR>benchmarks:<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2017-07-01<BR>ratio: 0.249<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>","<BR>task: Crowds<BR>date: 2017-08-01<BR>ratio: 0.7414<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - Venice: MAE<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2018-02-01<BR>ratio: 0.3658<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - Venice: MAE<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2018-06-01<BR>ratio: 0.193<BR>benchmarks:<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2018-07-01<BR>ratio: 0.0202<BR>benchmarks:<BR>  Crowd counting - UCF CC 50: MAE<BR>","<BR>task: Crowds<BR>date: 2018-08-01<BR>ratio: 0.1351<BR>benchmarks:<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2018-09-01<BR>ratio: 0.0856<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>","<BR>task: Crowds<BR>date: 2018-11-01<BR>ratio: 0.1796<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - UCF-QNRF: MAE<BR>  Crowd counting - Venice: MAE<BR>  Crowd counting - WorldExpo\u201910: Average MAE<BR>","<BR>task: Crowds<BR>date: 2019-08-01<BR>ratio: 0.0428<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>","<BR>task: Crowds<BR>date: 2019-09-01<BR>ratio: 0.8895<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MSE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>","<BR>task: Crowds<BR>date: 2019-11-01<BR>ratio: 0.0846<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech B: MAE<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2020-03-01<BR>ratio: 0.1939<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - UCF CC 50: MAE<BR>  Crowd counting - UCF-QNRF: MAE<BR>","<BR>task: Crowds<BR>date: 2021-07-01<BR>ratio: 0.1105<BR>benchmarks:<BR>  Crowd counting - ShanghaiTech A: MAE<BR>  Crowd counting - ShanghaiTech A: MSE<BR>","<BR>task: Deception detection<BR>date: 2015-11-01<BR>ratio: 0.9206<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Deception detection<BR>date: 2019-01-01<BR>ratio: 0.0794<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Deception detection<BR>date: 2021-03-01<BR>ratio: 0.4221<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Deception detection<BR>date: 2021-04-01<BR>ratio: 0.5779<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Dehazing<BR>date: 2016-05-01<BR>ratio: 0.8795<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Dehazing<BR>date: 2017-08-01<BR>ratio: 0.1205<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Dehazing<BR>date: 2018-03-01<BR>ratio: 0.3678<BR>benchmarks:<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>","<BR>task: Dehazing<BR>date: 2018-10-01<BR>ratio: 0.7065<BR>benchmarks:<BR>  Image dehazing - SOTS Outdoor: PSNR<BR>  Image dehazing - SOTS Outdoor: SSIM<BR>","<BR>task: Dehazing<BR>date: 2018-11-01<BR>ratio: 0.5747<BR>benchmarks:<BR>  Image dehazing - KITTI: PSNR<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>","<BR>task: Dehazing<BR>date: 2019-04-01<BR>ratio: 0.5269<BR>benchmarks:<BR>  Image dehazing - O-Haze: PSNR<BR>","<BR>task: Dehazing<BR>date: 2019-08-01<BR>ratio: 0.2935<BR>benchmarks:<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>  Image dehazing - SOTS Outdoor: PSNR<BR>  Image dehazing - SOTS Outdoor: SSIM<BR>","<BR>task: Dehazing<BR>date: 2019-11-01<BR>ratio: 0.6854<BR>benchmarks:<BR>  Image dehazing - KITTI: PSNR<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>  Image dehazing - SOTS Outdoor: PSNR<BR>","<BR>task: Dehazing<BR>date: 2020-08-01<BR>ratio: 0.4731<BR>benchmarks:<BR>  Image dehazing - O-Haze: PSNR<BR>","<BR>task: Dehazing<BR>date: 2020-10-01<BR>ratio: 0.2673<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Dehazing<BR>date: 2021-03-01<BR>ratio: 0.5644<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Dehazing<BR>date: 2021-04-01<BR>ratio: 0.3791<BR>benchmarks:<BR>  Image dehazing - SOTS Indoor: PSNR<BR>  Image dehazing - SOTS Indoor: SSIM<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>","<BR>task: Dehazing<BR>date: 2021-05-01<BR>ratio: 0.0693<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Dehazing<BR>date: 2021-06-01<BR>ratio: 0.159<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>","<BR>task: Dehazing<BR>date: 2021-07-01<BR>ratio: 0.841<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Dehazing<BR>date: 2021-11-01<BR>ratio: 0.0693<BR>benchmarks:<BR>  Image dehazing - SOTS Indoor: PSNR<BR>","<BR>task: Denoising<BR>date: 2016-08-01<BR>ratio: 0.6327<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>","<BR>task: Denoising<BR>date: 2017-04-01<BR>ratio: 0.9412<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>","<BR>task: Denoising<BR>date: 2017-05-01<BR>ratio: 0.7853<BR>benchmarks:<BR>  Denoising - Darmstadt Noise Dataset: PSNR<BR>","<BR>task: Denoising<BR>date: 2017-10-01<BR>ratio: 0.2941<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-02-01<BR>ratio: 0.2391<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-05-01<BR>ratio: 0.9987<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-06-01<BR>ratio: 0.1772<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-07-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>  Denoising - Darmstadt Noise Dataset: PSNR<BR>","<BR>task: Denoising<BR>date: 2018-11-01<BR>ratio: 0.3418<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2019-04-01<BR>ratio: 0.9434<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Denoising - Darmstadt Noise Dataset: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2019-07-01<BR>ratio: 0.011<BR>benchmarks:<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Denoising<BR>date: 2019-08-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2019-10-01<BR>ratio: 0.9673<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>","<BR>task: Denoising<BR>date: 2020-01-01<BR>ratio: 0.1066<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2020-03-01<BR>ratio: 0.2143<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2020-12-01<BR>ratio: 0.7609<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>","<BR>task: Denoising<BR>date: 2021-05-01<BR>ratio: 0.0292<BR>benchmarks:<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>","<BR>task: Denoising<BR>date: 2021-06-01<BR>ratio: 0.0457<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Denoising<BR>date: 2021-08-01<BR>ratio: 0.4949<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Denoising<BR>date: 2021-11-01<BR>ratio: 0.7097<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>","<BR>task: Denoising<BR>date: 2021-12-01<BR>ratio: 0.1899<BR>benchmarks:<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Depth completion<BR>date: 2018-08-01<BR>ratio: 0.8742<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: MAE<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>  Depth completion - KITTI Depth Completion: Runtime [ms]<BR>","<BR>task: Depth completion<BR>date: 2018-11-01<BR>ratio: 0.0991<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: MAE<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>","<BR>task: Depth completion<BR>date: 2019-01-01<BR>ratio: 0.1774<BR>benchmarks:<BR>  Depth completion - VOID: MAE<BR>  Depth completion - VOID: RMSE<BR>","<BR>task: Depth completion<BR>date: 2019-02-01<BR>ratio: 0.0644<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>","<BR>task: Depth completion<BR>date: 2019-05-01<BR>ratio: 0.4392<BR>benchmarks:<BR>  Depth completion - VOID: MAE<BR>  Depth completion - VOID: RMSE<BR>","<BR>task: Depth completion<BR>date: 2020-07-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: MAE<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>  Depth completion - KITTI Depth Completion: Runtime [ms]<BR>  Depth completion - VOID: MAE<BR>  Depth completion - VOID: RMSE<BR>","<BR>task: Depth completion<BR>date: 2021-03-01<BR>ratio: 0.0131<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>","<BR>task: Depth completion<BR>date: 2021-07-01<BR>ratio: 0.0187<BR>benchmarks:<BR>  Depth completion - KITTI Depth Completion: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2015-11-01<BR>ratio: 0.9206<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Depth estimation<BR>date: 2016-07-01<BR>ratio: 0.0211<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2017-04-01<BR>ratio: 0.1725<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2018-03-01<BR>ratio: 0.373<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>  Stereo-lidar fusion - KITTI Depth Completion Validation: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2018-06-01<BR>ratio: 0.5366<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split: absolute relative error<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2018-10-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2018-12-01<BR>ratio: 0.1549<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2019-01-01<BR>ratio: 0.0794<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Depth estimation<BR>date: 2019-04-01<BR>ratio: 0.3408<BR>benchmarks:<BR>  Stereo-lidar fusion - KITTI Depth Completion Validation: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2019-05-01<BR>ratio: 0.119<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2019-06-01<BR>ratio: 0.632<BR>benchmarks:<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE log<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE<BR>  Monocular depth estimation - Mid-Air Dataset: SQ Rel<BR>","<BR>task: Depth estimation<BR>date: 2019-07-01<BR>ratio: 0.5753<BR>benchmarks:<BR>  Depth estimation - DCM: RMSE log<BR>  Depth estimation - DCM: RMSE<BR>  Depth estimation - eBDtheque: RMSE log<BR>  Depth estimation - eBDtheque: RMSE<BR>  Monocular depth estimation - KITTI Eigen split: absolute relative error<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2019-10-01<BR>ratio: 0.0238<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2019-11-01<BR>ratio: 0.0604<BR>benchmarks:<BR>  Stereo-lidar fusion - KITTI Depth Completion Validation: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2020-06-01<BR>ratio: 0.5392<BR>benchmarks:<BR>  Monocular depth estimation - Make3D: Abs Rel<BR>","<BR>task: Depth estimation<BR>date: 2020-07-01<BR>ratio: 0.949<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^3<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Sq Rel<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>  Stereo depth estimation - KITTI2015: three pixel error<BR>","<BR>task: Depth estimation<BR>date: 2020-08-01<BR>ratio: 0.8252<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>","<BR>task: Depth estimation<BR>date: 2020-11-01<BR>ratio: 0.9167<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split: RMSE<BR>  Monocular depth estimation - KITTI Eigen split: absolute relative error<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25^2<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>  Monocular depth estimation - NYU-Depth V2: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2020-12-01<BR>ratio: 0.65<BR>benchmarks:<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25<BR>  Monocular depth estimation - NYU-Depth V2: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2021-03-01<BR>ratio: 0.4221<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Sq Rel<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split: RMSE<BR>  Monocular depth estimation - KITTI Eigen split: absolute relative error<BR>  Monocular depth estimation - NYU-Depth V2: Delta < 1.25^2<BR>  Monocular depth estimation - NYU-Depth V2: RMSE<BR>  Stereo-lidar fusion - KITTI Depth Completion Validation: RMSE<BR>","<BR>task: Depth estimation<BR>date: 2021-04-01<BR>ratio: 0.5779<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Depth estimation<BR>date: 2021-05-01<BR>ratio: 0.7895<BR>benchmarks:<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE log<BR>  Monocular depth estimation - Mid-Air Dataset: RMSE<BR>  Monocular depth estimation - Mid-Air Dataset: SQ Rel<BR>","<BR>task: Depth estimation<BR>date: 2021-08-01<BR>ratio: 0.051<BR>benchmarks:<BR>  Stereo depth estimation - KITTI2015: three pixel error<BR>","<BR>task: Depth estimation<BR>date: 2021-09-01<BR>ratio: 0.4444<BR>benchmarks:<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^3<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: RMSE log<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: absolute relative error<BR>","<BR>task: Depth estimation<BR>date: 2021-10-01<BR>ratio: 0.6364<BR>benchmarks:<BR>  Depth estimation - DCM: RMSE log<BR>  Depth estimation - DCM: RMSE<BR>  Depth estimation - eBDtheque: RMSE log<BR>  Depth estimation - eBDtheque: RMSE<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^2<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Delta < 1.25^3<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: RMSE log<BR>","<BR>task: Depth estimation<BR>date: 2021-12-01<BR>ratio: 0.9993<BR>benchmarks:<BR>  3D Depth Estimation - Relative Human: PCDR-Kid<BR>  3D Depth Estimation - Relative Human: PCDR<BR>  3D Depth Estimation - Relative Human: mPCDK<BR>  Monocular depth estimation - KITTI Eigen split unsupervised: Sq Rel<BR>  Monocular depth estimation - Make3D: Abs Rel<BR>","<BR>task: Domain adaptation<BR>date: 2014-09-01<BR>ratio: 0.9656<BR>benchmarks:<BR>  Domain adaptation - HMDBsmall-to-UCF: Accuracy<BR>  Domain adaptation - Olympic-to-HMDBsmall: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2015-02-01<BR>ratio: 0.3958<BR>benchmarks:<BR>  Domain adaptation - Office-Caltech: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2015-05-01<BR>ratio: 0.7187<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-MNIST-M: Accuracy<BR>  Domain adaptation - Synth Digits-to-SVHN: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2016-05-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Domain adaptation - HMDBfull-to-UCF: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2016-08-01<BR>ratio: 0.4171<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-MNIST-M: Accuracy<BR>  Domain adaptation - SVNH-to-MNIST: Accuracy<BR>  Domain adaptation - Synth Digits-to-SVHN: Accuracy<BR>  Domain adaptation - Synth Signs-to-GTSRB: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-04-01<BR>ratio: 0.6303<BR>benchmarks:<BR>  Domain adaptation - Office-31: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-05-01<BR>ratio: 0.5116<BR>benchmarks:<BR>  Domain adaptation - SVHN-to-MNIST: Accuracy<BR>  Domain adaptation - VisDA2017: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-06-01<BR>ratio: 0.96<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-USPS: Accuracy<BR>  Domain adaptation - SVHN-to-MNIST: Accuracy<BR>  Domain adaptation - Synth Signs-to-GTSRB: Accuracy<BR>  Domain adaptation - USPS-to-MNIST: Accuracy<BR>  Domain adaptation - VisDA2017: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-08-01<BR>ratio: 0.0028<BR>benchmarks:<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>","<BR>task: Domain adaptation<BR>date: 2017-09-01<BR>ratio: 0.4231<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2017-10-01<BR>ratio: 0.0303<BR>benchmarks:<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>","<BR>task: Domain adaptation<BR>date: 2017-12-01<BR>ratio: 0.5156<BR>benchmarks:<BR>  Domain adaptation - SYNSIG-to-GTSRB: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2018-03-01<BR>ratio: 0.2009<BR>benchmarks:<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2018-04-01<BR>ratio: 0.2923<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2018-06-01<BR>ratio: 0.1372<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2018-07-01<BR>ratio: 0.5987<BR>benchmarks:<BR>  Domain adaptation - Office-Caltech: Average Accuracy<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2018-11-01<BR>ratio: 0.8955<BR>benchmarks:<BR>  Domain adaptation - ImageCLEF-DA: Accuracy<BR>  Domain adaptation - Office-31: Average Accuracy<BR>  Domain generalization - ImageNet-R: Top-1 Error Rate<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>","<BR>task: Domain adaptation<BR>date: 2018-12-01<BR>ratio: 0.4555<BR>benchmarks:<BR>  Unsupervised domain adaptation - Cityscapes to Foggy Cityscapes: mAP-at-0.5<BR>","<BR>task: Domain adaptation<BR>date: 2019-01-01<BR>ratio: 0.2121<BR>benchmarks:<BR>  Domain adaptation - Office-31: Average Accuracy<BR>  Domain adaptation - VisDA2017: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2019-03-01<BR>ratio: 0.5789<BR>benchmarks:<BR>  Domain adaptation - SVNH-to-MNIST: Accuracy<BR>  Domain generalization - NICO Animal: Accuracy<BR>  Domain generalization - NICO Vehicle: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2019-05-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Domain adaptation - HMDBfull-to-UCF: Accuracy<BR>  Domain adaptation - HMDBsmall-to-UCF: Accuracy<BR>  Domain adaptation - Olympic-to-HMDBsmall: Accuracy<BR>  Domain adaptation - SVNH-to-MNIST: Accuracy<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>  Partial domain adaptation - Office-31: Accuracy (%)<BR>  Partial domain adaptation - Office-Home: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2019-06-01<BR>ratio: 0.6682<BR>benchmarks:<BR>  Domain adaptation - Office-Home: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2019-07-01<BR>ratio: 0.0718<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>","<BR>task: Domain adaptation<BR>date: 2019-08-01<BR>ratio: 0.4444<BR>benchmarks:<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>  Unsupervised domain adaptation - Market to MSMT: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2019-09-01<BR>ratio: 0.575<BR>benchmarks:<BR>  Domain adaptation - USPS-to-MNIST: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2019-11-01<BR>ratio: 0.1045<BR>benchmarks:<BR>  Domain adaptation - ImageCLEF-DA: Accuracy<BR>  Domain adaptation - Office-Caltech: Average Accuracy<BR>  Domain adaptation - Office-Home: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2019-12-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>  Partial domain adaptation - Office-Home: Accuracy (%)<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>","<BR>task: Domain adaptation<BR>date: 2020-01-01<BR>ratio: 0.2646<BR>benchmarks:<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>  Unsupervised domain adaptation - Market to MSMT: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2020-02-01<BR>ratio: 0.381<BR>benchmarks:<BR>  Domain adaptation - Office-Home: Accuracy<BR>  Domain adaptation - USPS-to-MNIST: Accuracy<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>  Partial domain adaptation - Office-Home: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2020-03-01<BR>ratio: 0.8229<BR>benchmarks:<BR>  Partial domain adaptation - DomainNet: Accuracy (%)<BR>  Partial domain adaptation - ImageNet-Caltech: Accuracy (%)<BR>  Partial domain adaptation - Office-31: Accuracy (%)<BR>  Unsupervised domain adaptation - Cityscapes to Foggy Cityscapes: mAP-at-0.5<BR>  Unsupervised domain adaptation - Cityscapes-to-OxfordCar: mIoU<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>","<BR>task: Domain adaptation<BR>date: 2020-04-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Domain adaptation - Synscapes-to-Cityscapes: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2020-05-01<BR>ratio: 0.3125<BR>benchmarks:<BR>  Domain adaptation - Cityscapes to ACDC: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2020-06-01<BR>ratio: 0.4844<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-USPS: Accuracy<BR>  Domain adaptation - SYNSIG-to-GTSRB: Accuracy<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>  Domain generalization - ImageNet-R: Top-1 Error Rate<BR>  Domain generalization - PACS: Average Accuracy<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Duke to Market: rank-10<BR>  Unsupervised domain adaptation - Duke to Market: rank-1<BR>  Unsupervised domain adaptation - Duke to Market: rank-5<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>  Unsupervised domain adaptation - Market to MSMT: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2020-07-01<BR>ratio: 0.0109<BR>benchmarks:<BR>  Domain generalization - PACS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2020-11-01<BR>ratio: 0.0838<BR>benchmarks:<BR>  Domain adaptation - Office-31: Average Accuracy<BR>  Domain adaptation - Office-Home: Accuracy<BR>  Unsupervised domain adaptation - Cityscapes to Foggy Cityscapes: mAP-at-0.5<BR>","<BR>task: Domain adaptation<BR>date: 2020-12-01<BR>ratio: 0.3452<BR>benchmarks:<BR>  Domain generalization - NICO Animal: Accuracy<BR>  Domain generalization - NICO Vehicle: Accuracy<BR>  Partial domain adaptation - Office-31: Accuracy (%)<BR>","<BR>task: Domain adaptation<BR>date: 2021-02-01<BR>ratio: 0.6473<BR>benchmarks:<BR>  Domain generalization - DomainNet: Average Accuracy<BR>  Domain generalization - Office-Home: Average Accuracy<BR>  Domain generalization - PACS: Average Accuracy<BR>  Domain generalization - VLCS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2021-03-01<BR>ratio: 0.711<BR>benchmarks:<BR>  Domain adaptation - MNIST-to-MNIST-M: Accuracy<BR>  Domain adaptation - Office-Home: Accuracy<BR>  Partial domain adaptation - Office-Home: Accuracy (%)<BR>  Unsupervised domain adaptation - Cityscapes-to-OxfordCar: mIoU<BR>  Unsupervised domain adaptation - Duke to MSMT: mAP<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-10<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-1<BR>  Unsupervised domain adaptation - Duke to MSMT: rank-5<BR>  Unsupervised domain adaptation - Duke to Market: mAP<BR>  Unsupervised domain adaptation - Market to Duke: mAP<BR>  Unsupervised domain adaptation - Market to Duke: rank-10<BR>  Unsupervised domain adaptation - Market to Duke: rank-1<BR>  Unsupervised domain adaptation - Market to Duke: rank-5<BR>  Unsupervised domain adaptation - Market to MSMT: mAP<BR>  Unsupervised domain adaptation - Market to MSMT: rank-10<BR>  Unsupervised domain adaptation - Market to MSMT: rank-1<BR>  Unsupervised domain adaptation - Market to MSMT: rank-5<BR>","<BR>task: Domain adaptation<BR>date: 2021-04-01<BR>ratio: 0.1929<BR>benchmarks:<BR>  Domain adaptation - Cityscapes to ACDC: mIoU<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>","<BR>task: Domain adaptation<BR>date: 2021-05-01<BR>ratio: 0.3831<BR>benchmarks:<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>  Domain generalization - ImageNet-R: Top-1 Error Rate<BR>","<BR>task: Domain adaptation<BR>date: 2021-06-01<BR>ratio: 0.1211<BR>benchmarks:<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>","<BR>task: Domain adaptation<BR>date: 2021-08-01<BR>ratio: 0.2589<BR>benchmarks:<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>  Unsupervised domain adaptation - Cityscapes to Foggy Cityscapes: mAP-at-0.5<BR>","<BR>task: Domain adaptation<BR>date: 2021-09-01<BR>ratio: 0.8<BR>benchmarks:<BR>  Domain adaptation - Office-31: Average Accuracy<BR>  Domain adaptation - Office-Home: Accuracy<BR>  Domain adaptation - Synscapes-to-Cityscapes: mIoU<BR>  Domain adaptation - VisDA2017: Accuracy<BR>  Domain generalization - NICO Animal: Accuracy<BR>  Domain generalization - NICO Vehicle: Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2021-10-01<BR>ratio: 0.8338<BR>benchmarks:<BR>  Domain generalization - DomainNet: Average Accuracy<BR>  Domain generalization - Office-Home: Average Accuracy<BR>  Domain generalization - PACS: Average Accuracy<BR>  Domain generalization - VLCS: Average Accuracy<BR>","<BR>task: Domain adaptation<BR>date: 2021-11-01<BR>ratio: 0.6648<BR>benchmarks:<BR>  Domain adaptation - Cityscapes to ACDC: mIoU<BR>  Domain adaptation - SYNTHIA-to-Cityscapes: mIoU<BR>  Domain generalization - ImageNet-A: Top-1 accuracy %<BR>  Domain generalization - ImageNet-C: mean Corruption Error (mCE)<BR>  Domain generalization - ImageNet-R: Top-1 Error Rate<BR>","<BR>task: Domain adaptation<BR>date: 2021-12-01<BR>ratio: 0.181<BR>benchmarks:<BR>  Partial domain adaptation - DomainNet: Accuracy (%)<BR>  Partial domain adaptation - ImageNet-Caltech: Accuracy (%)<BR>","<BR>task: Edge detection<BR>date: 2019-02-01<BR>ratio: 0.8913<BR>benchmarks:<BR>  Edge detection - BIPED: Number of parameters (M)<BR>  Edge detection - BIPED: ODS<BR>  Edge detection - MDBD: ODS<BR>","<BR>task: Edge detection<BR>date: 2020-11-01<BR>ratio: 0.2667<BR>benchmarks:<BR>  Edge detection - MDBD: ODS<BR>","<BR>task: Edge detection<BR>date: 2021-12-01<BR>ratio: 0.9257<BR>benchmarks:<BR>  Edge detection - BIPED: Number of parameters (M)<BR>  Edge detection - BIPED: ODS<BR>  Edge detection - MDBD: ODS<BR>","<BR>task: Emotion classification<BR>date: 2020-08-01<BR>ratio: 0.1176<BR>benchmarks:<BR>  Emotion classification - SemEval 2018 Task 1E-c: Macro-F1<BR>","<BR>task: Emotion classification<BR>date: 2021-01-01<BR>ratio: 0.8824<BR>benchmarks:<BR>  Emotion classification - SemEval 2018 Task 1E-c: Macro-F1<BR>","<BR>task: Emotion recognition<BR>date: 2015-01-01<BR>ratio: 0.2844<BR>benchmarks:<BR>  Emotion recognition in conversation - CPED: Accuracy of Sentiment<BR>  Emotion recognition in conversation - CPED: Macro-F1 of Sentiment<BR>","<BR>task: Emotion recognition<BR>date: 2017-07-01<BR>ratio: 0.5917<BR>benchmarks:<BR>  Emotion recognition in conversation - CPED: Accuracy of Sentiment<BR>  Emotion recognition in conversation - CPED: Macro-F1 of Sentiment<BR>","<BR>task: Emotion recognition<BR>date: 2018-06-01<BR>ratio: 0.023<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2018-10-01<BR>ratio: 0.377<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>  Emotion recognition in conversation - IEMOCAP: Macro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Expectancy)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Power)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Valence)<BR>","<BR>task: Emotion recognition<BR>date: 2018-11-01<BR>ratio: 0.6316<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>  Emotion recognition in conversation - IEMOCAP: Macro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Accuracy<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Expectancy)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Power)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Valence)<BR>","<BR>task: Emotion recognition<BR>date: 2019-04-01<BR>ratio: 0.3923<BR>benchmarks:<BR>  Speech emotion recognition - IEMOCAP: UA<BR>","<BR>task: Emotion recognition<BR>date: 2019-07-01<BR>ratio: 0.0367<BR>benchmarks:<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2019-08-01<BR>ratio: 0.3438<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>  Emotion recognition in conversation - IEMOCAP: Macro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Expectancy)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Power)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Valence)<BR>","<BR>task: Emotion recognition<BR>date: 2019-09-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2020-02-01<BR>ratio: 0.7735<BR>benchmarks:<BR>  Emotion-cause pair extraction - ECPE: F1<BR>  Speech emotion recognition - CREMA-D: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2020-03-01<BR>ratio: 0.369<BR>benchmarks:<BR>  Emotion recognition in conversation - EmoryNLP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2020-05-01<BR>ratio: 0.0824<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2020-07-01<BR>ratio: 0.9112<BR>benchmarks:<BR>  Emotion recognition in conversation - DailyDialog: Micro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion-cause pair extraction - ECPE: F1<BR>","<BR>task: Emotion recognition<BR>date: 2020-09-01<BR>ratio: 0.2667<BR>benchmarks:<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>  Emotion recognition in conversation - SEMAINE: MAE (Expectancy)<BR>","<BR>task: Emotion recognition<BR>date: 2020-10-01<BR>ratio: 0.4515<BR>benchmarks:<BR>  Emotion recognition in conversation - EmoryNLP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>  Speech emotion recognition - IEMOCAP: UA<BR>","<BR>task: Emotion recognition<BR>date: 2020-12-01<BR>ratio: 0.6795<BR>benchmarks:<BR>  Emotion recognition in conversation - CPED: Accuracy of Sentiment<BR>  Emotion recognition in conversation - CPED: Macro-F1 of Sentiment<BR>  Emotion recognition in conversation - IEMOCAP: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2021-03-01<BR>ratio: 0.2265<BR>benchmarks:<BR>  Speech emotion recognition - CREMA-D: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2021-05-01<BR>ratio: 0.1876<BR>benchmarks:<BR>  Emotion recognition in conversation - EmoryNLP: Weighted-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2021-06-01<BR>ratio: 0.3684<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Macro-F1<BR>  Emotion recognition in conversation - MELD: Accuracy<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>  Emotion recognition in conversation - SEMAINE: MAE (Arousal)<BR>","<BR>task: Emotion recognition<BR>date: 2021-08-01<BR>ratio: 0.1042<BR>benchmarks:<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>  Emotion recognition in conversation - MELD: Weighted-F1<BR>","<BR>task: Emotion recognition<BR>date: 2021-09-01<BR>ratio: 0.3077<BR>benchmarks:<BR>  Emotion recognition in conversation - EmoryNLP: Weighted-F1<BR>  Speech emotion recognition - IEMOCAP: UA<BR>","<BR>task: Emotion recognition<BR>date: 2021-11-01<BR>ratio: 0.4437<BR>benchmarks:<BR>  Emotion recognition - RAVDESS: Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2021-12-01<BR>ratio: 0.5563<BR>benchmarks:<BR>  Emotion recognition - RAVDESS: Accuracy<BR>  Emotion recognition in conversation - DailyDialog: Micro-F1<BR>  Emotion recognition in conversation - IEMOCAP: Weighted-F1<BR>","<BR>task: Gesture recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Gesture recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Gesture recognition<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Gesture recognition<BR>date: 2017-05-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Gesture recognition<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Gesture recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Gesture recognition<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Gesture recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Gesture recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-12-01<BR>ratio: 0.872<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-01-01<BR>ratio: 0.128<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Gesture recognition<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-07-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Gesture recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Gesture recognition<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Gesture recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Gesture recognition<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Hand-related vision process<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-05-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-07-01<BR>ratio: 0.5558<BR>benchmarks:<BR>  Gesture-to-gesture translation - NTU Hand Digit: IS<BR>  Gesture-to-gesture translation - Senz3D: PSNR<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>  Hand pose estimation - MSRA Hands: Average 3D Error<BR>  Hand pose estimation - NYU Hands: Average 3D Error<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2017-11-01<BR>ratio: 0.6475<BR>benchmarks:<BR>  Hand pose estimation - HANDS 2017: Average 3D Error<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>  Hand pose estimation - MSRA Hands: Average 3D Error<BR>  Hand pose estimation - NYU Hands: Average 3D Error<BR>","<BR>task: Hand-related vision process<BR>date: 2017-12-01<BR>ratio: 0.5452<BR>benchmarks:<BR>  Gesture-to-gesture translation - NTU Hand Digit: AMT<BR>  Gesture-to-gesture translation - NTU Hand Digit: PSNR<BR>  Gesture-to-gesture translation - Senz3D: AMT<BR>  Gesture-to-gesture translation - Senz3D: IS<BR>  Gesture-to-gesture translation - Senz3D: PSNR<BR>","<BR>task: Hand-related vision process<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Hand-related vision process<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Hand-related vision process<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2018-08-01<BR>ratio: 0.6512<BR>benchmarks:<BR>  Gesture-to-gesture translation - NTU Hand Digit: AMT<BR>  Gesture-to-gesture translation - NTU Hand Digit: IS<BR>  Gesture-to-gesture translation - NTU Hand Digit: PSNR<BR>  Gesture-to-gesture translation - Senz3D: AMT<BR>  Gesture-to-gesture translation - Senz3D: IS<BR>  Gesture-to-gesture translation - Senz3D: PSNR<BR>","<BR>task: Hand-related vision process<BR>date: 2018-12-01<BR>ratio: 0.872<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Hand gesture recognition - VIVA Hand Gestures Dataset: Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-01-01<BR>ratio: 0.128<BR>benchmarks:<BR>  Hand gesture recognition - EgoGesture: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Hand-related vision process<BR>date: 2019-05-01<BR>ratio: 0.0826<BR>benchmarks:<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>","<BR>task: Hand-related vision process<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-07-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Hand gesture recognition - DHG-14: Accuracy<BR>  Hand gesture recognition - DHG-28: Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Hand-related vision process<BR>date: 2019-08-01<BR>ratio: 0.4409<BR>benchmarks:<BR>  Hand pose estimation - HANDS 2017: Average 3D Error<BR>","<BR>task: Hand-related vision process<BR>date: 2019-09-01<BR>ratio: 0.2958<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  3D hand pose estimation - FreiHAND: PA-MPVPE<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Hand-related vision process<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Gesture-to-gesture translation - NTU Hand Digit: AMT<BR>  Gesture-to-gesture translation - NTU Hand Digit: PSNR<BR>  Gesture-to-gesture translation - Senz3D: AMT<BR>  Gesture-to-gesture translation - Senz3D: PSNR<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Hand gesture recognition - NVGesture: Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2020-07-01<BR>ratio: 0.1801<BR>benchmarks:<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>  Hand pose estimation - MSRA Hands: Average 3D Error<BR>  Hand pose estimation - NYU Hands: Average 3D Error<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2020-08-01<BR>ratio: 0.72<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  3D hand pose estimation - FreiHAND: PA-MPJPE<BR>  3D hand pose estimation - FreiHAND: PA-MPVPE<BR>","<BR>task: Hand-related vision process<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2020-12-01<BR>ratio: 0.1268<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  3D hand pose estimation - FreiHAND: PA-MPJPE<BR>  3D hand pose estimation - FreiHAND: PA-MPVPE<BR>","<BR>task: Hand-related vision process<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  3D hand pose estimation - FreiHAND: PA-F@15mm<BR>  3D hand pose estimation - FreiHAND: PA-F@5mm<BR>  3D hand pose estimation - FreiHAND: PA-MPJPE<BR>  3D hand pose estimation - FreiHAND: PA-MPVPE<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Hand-related vision process<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Hand-related vision process<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Hand-related vision process<BR>date: 2021-08-01<BR>ratio: 0.0194<BR>benchmarks:<BR>  Hand pose estimation - ICVL Hands: Average 3D Error<BR>","<BR>task: Horizon line estimation<BR>date: 2016-08-01<BR>ratio: 0.96<BR>benchmarks:<BR>  Horizon line estimation - Eurasian Cities Dataset: AUC (horizon error)<BR>  Horizon line estimation - York Urban Dataset: AUC (horizon error)<BR>","<BR>task: Horizon line estimation<BR>date: 2018-09-01<BR>ratio: 0.0638<BR>benchmarks:<BR>  Horizon line estimation - Eurasian Cities Dataset: AUC (horizon error)<BR>  Horizon line estimation - York Urban Dataset: AUC (horizon error)<BR>","<BR>task: Human interaction recognition<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Human interaction recognition<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-06-01<BR>ratio: 0.9173<BR>benchmarks:<BR>  Human interaction recognition - BIT: Accuracy<BR>  Human interaction recognition - UT: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Human interaction recognition<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Human interaction recognition<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Human interaction recognition<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-11-01<BR>ratio: 0.662<BR>benchmarks:<BR>  Human interaction recognition - BIT: Accuracy<BR>  Human interaction recognition - UT: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2018-12-01<BR>ratio: 0.4719<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Human interaction recognition<BR>date: 2019-05-01<BR>ratio: 0.2124<BR>benchmarks:<BR>  One-shot 3D action recognition - NTU RGB+D 120: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2019-06-01<BR>ratio: 0.024<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Human interaction recognition<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Human interaction recognition<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2020-04-01<BR>ratio: 0.3805<BR>benchmarks:<BR>  One-shot 3D action recognition - NTU RGB+D 120: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2020-07-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2020-12-01<BR>ratio: 0.4071<BR>benchmarks:<BR>  One-shot 3D action recognition - NTU RGB+D 120: Accuracy<BR>","<BR>task: Human interaction recognition<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Human interaction recognition<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2021-06-01<BR>ratio: 0.0079<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Human interaction recognition<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Human parsing<BR>date: 2017-05-01<BR>ratio: 0.3018<BR>benchmarks:<BR>  Multi-human parsing - MHP v2.0: AP 0.5<BR>","<BR>task: Human parsing<BR>date: 2017-09-01<BR>ratio: 0.0861<BR>benchmarks:<BR>  Multi-human parsing - PASCAL-Part: AP 0.5<BR>","<BR>task: Human parsing<BR>date: 2018-04-01<BR>ratio: 0.9139<BR>benchmarks:<BR>  Multi-human parsing - MHP v2.0: AP 0.5<BR>  Multi-human parsing - PASCAL-Part: AP 0.5<BR>","<BR>task: Human-object interaction detection<BR>date: 2016-04-01<BR>ratio: 0.2049<BR>benchmarks:<BR>  Human-object interaction detection - HICO: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2018-07-01<BR>ratio: 0.1024<BR>benchmarks:<BR>  Human-object interaction detection - HICO: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2018-08-01<BR>ratio: 0.1896<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2018-11-01<BR>ratio: 0.2238<BR>benchmarks:<BR>  Human-object interaction detection - Ambiguious-HOI: mAP<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>","<BR>task: Human-object interaction detection<BR>date: 2019-04-01<BR>ratio: 0.1941<BR>benchmarks:<BR>  Human-object interaction detection - HICO: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2019-12-01<BR>ratio: 0.1579<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-03-01<BR>ratio: 0.0344<BR>benchmarks:<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-04-01<BR>ratio: 0.9641<BR>benchmarks:<BR>  Human-object interaction detection - Ambiguious-HOI: mAP<BR>  Human-object interaction detection - HICO-DET: mAP<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-08-01<BR>ratio: 0.1273<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-10-01<BR>ratio: 0.3267<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Human-object interaction detection<BR>date: 2020-12-01<BR>ratio: 0.1149<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Human-object interaction detection<BR>date: 2021-03-01<BR>ratio: 0.1406<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Human-object interaction detection<BR>date: 2021-04-01<BR>ratio: 0.3366<BR>benchmarks:<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Human-object interaction detection<BR>date: 2021-08-01<BR>ratio: 0.266<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - V-COCO: AP(S1)<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Human-object interaction detection<BR>date: 2021-12-01<BR>ratio: 0.4987<BR>benchmarks:<BR>  Human-object interaction detection - HICO-DET: mAP<BR>  Human-object interaction detection - HICO: mAP<BR>  Human-object interaction detection - V-COCO: AP(S2)<BR>","<BR>task: Image classification<BR>date: 2013-02-01<BR>ratio: 0.1657<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2013-12-01<BR>ratio: 0.1657<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2014-06-01<BR>ratio: 0.3717<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2014-09-01<BR>ratio: 0.2955<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2014-12-01<BR>ratio: 0.3164<BR>benchmarks:<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2015-02-01<BR>ratio: 0.0146<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-06-01<BR>ratio: 0.0525<BR>benchmarks:<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2015-09-01<BR>ratio: 0.1105<BR>benchmarks:<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2015-12-01<BR>ratio: 0.3983<BR>benchmarks:<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-02-01<BR>ratio: 0.051<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-03-01<BR>ratio: 0.0415<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-05-01<BR>ratio: 0.0998<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2016-08-01<BR>ratio: 0.0433<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2016-10-01<BR>ratio: 0.0169<BR>benchmarks:<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2016-11-01<BR>ratio: 0.1225<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2017-01-01<BR>ratio: 0.0373<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2017-07-01<BR>ratio: 0.2633<BR>benchmarks:<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-08-01<BR>ratio: 0.3312<BR>benchmarks:<BR>  Image classification - STL-10: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2017-09-01<BR>ratio: 0.0972<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2017-10-01<BR>ratio: 0.9513<BR>benchmarks:<BR>  Image classification - EMNIST-Balanced: Accuracy<BR>  Image classification - Kuzushiji-MNIST: Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-11-01<BR>ratio: 0.0515<BR>benchmarks:<BR>  Image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-12-01<BR>ratio: 0.2743<BR>benchmarks:<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-01-01<BR>ratio: 0.8515<BR>benchmarks:<BR>  Image classification - ImageNet: GFLOPs<BR>  Image classification - smallNORB: Classification Error<BR>","<BR>task: Image classification<BR>date: 2018-02-01<BR>ratio: 0.0461<BR>benchmarks:<BR>  Image classification - ImageNet: Number of params<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-04-01<BR>ratio: 0.2203<BR>benchmarks:<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-05-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - ImageNet: Number of params<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - MNIST: Accuracy<BR>  Image classification - MNIST: Percentage error<BR>","<BR>task: Image classification<BR>date: 2018-06-01<BR>ratio: 0.018<BR>benchmarks:<BR>  Image classification - CIFAR-10: PARAMS<BR>","<BR>task: Image classification<BR>date: 2018-07-01<BR>ratio: 0.0539<BR>benchmarks:<BR>  Image classification - ImageNet: GFLOPs<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2018-08-01<BR>ratio: 0.4601<BR>benchmarks:<BR>  Image classification - EMNIST-Digits: Accuracy (%)<BR>  Image classification - EMNIST-Letters: Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-1 Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-11-01<BR>ratio: 0.1266<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2018-12-01<BR>ratio: 0.3493<BR>benchmarks:<BR>  Image classification - Clothing1M: Accuracy<BR>  Image classification - ImageNet: GFLOPs<BR>","<BR>task: Image classification<BR>date: 2019-01-01<BR>ratio: 0.4511<BR>benchmarks:<BR>  Image classification - CIFAR-10: PARAMS<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - Kuzushiji-MNIST: Accuracy<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-02-01<BR>ratio: 0.7889<BR>benchmarks:<BR>  Image classification - Clothing1M: Accuracy<BR>  Image classification - OmniBenchmark: Average Top-1 Accuracy<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2019-04-01<BR>ratio: 0.5542<BR>benchmarks:<BR>  Image classification - EMNIST-Letters: Accuracy<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2019-05-01<BR>ratio: 0.9485<BR>benchmarks:<BR>  Image classification - CIFAR-100: PARAMS<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: PARAMS<BR>  Image classification - STL-10: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>  Image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>  Image classification - smallNORB: Classification Error<BR>","<BR>task: Image classification<BR>date: 2019-06-01<BR>ratio: 0.5031<BR>benchmarks:<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - STL-10: Percentage correct<BR>  Image classification - WebVision-1000: ImageNet Top-1 Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-5 Accuracy<BR>  Image classification - iNaturalist: Top 1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-07-01<BR>ratio: 0.9486<BR>benchmarks:<BR>  Image classification - Colored-MNIST(with spurious correlation): Accuracy<BR>  Image classification - ImageNet: GFLOPs<BR>  Image classification - Kuzushiji-MNIST: Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-11-01<BR>ratio: 0.5803<BR>benchmarks:<BR>  Image classification - Clothing1M: Accuracy<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: GFLOPs<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - STL-10: Percentage correct<BR>  Image classification - WebVision-1000: ImageNet Top-1 Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-5 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-12-01<BR>ratio: 0.9343<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - Flowers-102: Accuracy<BR>  Image classification - ImageNet: Number of params<BR>  Image classification - ObjectNet: Top-1 Accuracy<BR>  Image classification - ObjectNet: Top-5 Accuracy<BR>  Image classification - Tiny ImageNet Classification: Validation Acc<BR>  Image classification - VTAB-1k: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-01-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Image classification - MNIST: Accuracy<BR>  Image classification - MNIST: Percentage error<BR>","<BR>task: Image classification<BR>date: 2020-02-01<BR>ratio: 0.1259<BR>benchmarks:<BR>  Image classification - Fashion-MNIST: Percentage error<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-03-01<BR>ratio: 0.067<BR>benchmarks:<BR>  Image classification - Colored-MNIST(with spurious correlation): Accuracy<BR>  Image classification - ImageNet ReaL: Accuracy<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-05-01<BR>ratio: 0.0847<BR>benchmarks:<BR>  Image classification - STL-10: Percentage correct<BR>","<BR>task: Image classification<BR>date: 2020-06-01<BR>ratio: 0.9821<BR>benchmarks:<BR>  Image classification - EMNIST-Digits: Accuracy (%)<BR>  Image classification - Fashion-MNIST: Percentage error<BR>  Image classification - Places205: Top 1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-07-01<BR>ratio: 0.0752<BR>benchmarks:<BR>  Image classification - EMNIST-Balanced: Accuracy<BR>  Image classification - EMNIST-Letters: Accuracy<BR>  Image classification - Kuzushiji-MNIST: Accuracy<BR>  Image classification - STL-10: Percentage correct<BR>  Image classification - SVHN: Percentage error<BR>","<BR>task: Image classification<BR>date: 2020-08-01<BR>ratio: 0.3873<BR>benchmarks:<BR>  Image classification - MNIST: Accuracy<BR>  Image classification - MNIST: Percentage error<BR>  Image classification - WebVision-1000: ImageNet Top-1 Accuracy<BR>  Image classification - WebVision-1000: ImageNet Top-5 Accuracy<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-10-01<BR>ratio: 0.9367<BR>benchmarks:<BR>  Image classification - CIFAR-100: Percentage correct<BR>  Image classification - CIFAR-10: PARAMS<BR>  Image classification - CIFAR-10: Percentage correct<BR>  Image classification - Flowers-102: Accuracy<BR>  Image classification - ObjectNet: Top-1 Accuracy<BR>  Image classification - ObjectNet: Top-5 Accuracy<BR>  Image classification - OmniBenchmark: Average Top-1 Accuracy<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-11-01<BR>ratio: 0.3383<BR>benchmarks:<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2020-12-01<BR>ratio: 0.3284<BR>benchmarks:<BR>  Image classification - CIFAR-100: PARAMS<BR>  Image classification - Colored-MNIST(with spurious correlation): Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-01-01<BR>ratio: 0.0236<BR>benchmarks:<BR>  Image classification - smallNORB: Classification Error<BR>","<BR>task: Image classification<BR>date: 2021-02-01<BR>ratio: 0.208<BR>benchmarks:<BR>  Image classification - ObjectNet: Top-1 Accuracy<BR>  Image classification - VTAB-1k: Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-03-01<BR>ratio: 0.6889<BR>benchmarks:<BR>  Image classification - Flowers-102: Accuracy<BR>  Image classification - OmniBenchmark: Average Top-1 Accuracy<BR>  Image classification - Places205: Top 1 Accuracy<BR>  Image classification - Stanford Cars: Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-04-01<BR>ratio: 0.955<BR>benchmarks:<BR>  Image classification - Flowers-102: Accuracy<BR>  Image classification - Food-101: Accuracy (%)<BR>  Image classification - Stanford Cars: Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-06-01<BR>ratio: 0.8889<BR>benchmarks:<BR>  Image classification - ImageNet: Number of params<BR>  Image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-09-01<BR>ratio: 0.3011<BR>benchmarks:<BR>  Image classification - mini WebVision 1.0: ImageNet Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: ImageNet Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-10-01<BR>ratio: 0.7147<BR>benchmarks:<BR>  Image classification - Food-101: Accuracy (%)<BR>  Image classification - Tiny ImageNet Classification: Validation Acc<BR>","<BR>task: Image classification<BR>date: 2021-11-01<BR>ratio: 0.4969<BR>benchmarks:<BR>  Image classification - ImageNet: Top 5 Accuracy<BR>  Image classification - ObjectNet: Top-1 Accuracy<BR>  Image classification - Places205: Top 1 Accuracy<BR>  Image classification - iNaturalist 2018: Top-1 Accuracy<BR>  Image classification - iNaturalist: Top 1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2021-12-01<BR>ratio: 0.0524<BR>benchmarks:<BR>  Image classification - mini WebVision 1.0: Top-1 Accuracy<BR>  Image classification - mini WebVision 1.0: Top-5 Accuracy<BR>","<BR>task: Image classification // Document image classification<BR>date: 2018-01-01<BR>ratio: 0.2385<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Document image classification<BR>date: 2018-02-01<BR>ratio: 0.2385<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Document image classification<BR>date: 2019-12-01<BR>ratio: 0.1865<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Document image classification<BR>date: 2020-12-01<BR>ratio: 0.2346<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Document image classification<BR>date: 2021-06-01<BR>ratio: 0.1019<BR>benchmarks:<BR>  Document image classification - RVL-CDIP: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2014-09-01<BR>ratio: 0.8273<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2016-03-01<BR>ratio: 0.1186<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2016-05-01<BR>ratio: 0.1778<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-03-01<BR>ratio: 0.8222<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-06-01<BR>ratio: 0.267<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2017-11-01<BR>ratio: 0.5012<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-05-01<BR>ratio: 0.234<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-06-01<BR>ratio: 0.4298<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-10-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-11-01<BR>ratio: 0.6674<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2018-12-01<BR>ratio: 0.5693<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-01-01<BR>ratio: 0.9015<BR>benchmarks:<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-02-01<BR>ratio: 0.2783<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-03-01<BR>ratio: 0.4148<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-04-01<BR>ratio: 0.7432<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-05-01<BR>ratio: 0.0836<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-06-01<BR>ratio: 0.7342<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-07-01<BR>ratio: 0.2956<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-08-01<BR>ratio: 0.2676<BR>benchmarks:<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-10-01<BR>ratio: 0.95<BR>benchmarks:<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-11-01<BR>ratio: 0.9242<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2019-12-01<BR>ratio: 0.8565<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-01-01<BR>ratio: 0.1703<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-02-01<BR>ratio: 0.3689<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-03-01<BR>ratio: 0.366<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-04-01<BR>ratio: 0.7<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-06-01<BR>ratio: 0.7127<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-08-01<BR>ratio: 0.3986<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-09-01<BR>ratio: 0.012<BR>benchmarks:<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-10-01<BR>ratio: 0.5799<BR>benchmarks:<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-11-01<BR>ratio: 0.7432<BR>benchmarks:<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2020-12-01<BR>ratio: 0.5706<BR>benchmarks:<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-01-01<BR>ratio: 0.1191<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-02-01<BR>ratio: 0.9984<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-03-01<BR>ratio: 0.268<BR>benchmarks:<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-04-01<BR>ratio: 0.5568<BR>benchmarks:<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-05-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-07-01<BR>ratio: 0.303<BR>benchmarks:<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-10-01<BR>ratio: 0.1543<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Image classification // Few-shot image classification<BR>date: 2021-11-01<BR>ratio: 0.55<BR>benchmarks:<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2014-07-01<BR>ratio: 0.6227<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2015-12-01<BR>ratio: 0.2131<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2016-11-01<BR>ratio: 0.499<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>  Fine-grained image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2017-05-01<BR>ratio: 0.2974<BR>benchmarks:<BR>  Fine-grained image classification - NABirds: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2017-12-01<BR>ratio: 0.0318<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-01-01<BR>ratio: 0.4482<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>  Fine-grained image classification - NABirds: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-05-01<BR>ratio: 0.286<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>  Fine-grained image classification - Oxford 102 Flowers: Accuracy<BR>  Fine-grained image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-06-01<BR>ratio: 0.0098<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2018-11-01<BR>ratio: 0.2687<BR>benchmarks:<BR>  Fine-grained image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-01-01<BR>ratio: 0.1061<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-03-01<BR>ratio: 0.0196<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-05-01<BR>ratio: 0.7906<BR>benchmarks:<BR>  Fine-grained image classification - Birdsnap: Accuracy<BR>  Fine-grained image classification - Oxford-IIIT Pets: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-06-01<BR>ratio: 0.7167<BR>benchmarks:<BR>  Fine-grained image classification - NABirds: Accuracy<BR>  Fine-grained image classification - Oxford 102 Flowers: Accuracy<BR>  Fine-grained image classification - Oxford 102 Flowers: Top-1 Error Rate<BR>  Fine-grained image classification - Oxford-IIIT Pets: Top-1 Error Rate<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-10-01<BR>ratio: 0.1608<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2019-12-01<BR>ratio: 0.9204<BR>benchmarks:<BR>  Fine-grained image classification - Oxford 102 Flowers: Accuracy<BR>  Fine-grained image classification - Oxford 102 Flowers: Top-1 Error Rate<BR>  Fine-grained image classification - Oxford-IIIT Pets: Accuracy<BR>  Fine-grained image classification - Oxford-IIIT Pets: Top-1 Error Rate<BR>  Fine-grained image classification - Stanford Dogs: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2020-02-01<BR>ratio: 0.3355<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>  Fine-grained image classification - Stanford Dogs: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2020-03-01<BR>ratio: 0.2572<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2020-07-01<BR>ratio: 0.9349<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>  Fine-grained image classification - Caltech-101: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2020-10-01<BR>ratio: 0.8918<BR>benchmarks:<BR>  Fine-grained image classification - Birdsnap: Accuracy<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>  Fine-grained image classification - Oxford-IIIT Pets: Accuracy<BR>  Fine-grained image classification - Oxford-IIIT Pets: Top-1 Error Rate<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2021-01-01<BR>ratio: 0.0643<BR>benchmarks:<BR>  Fine-grained image classification - CUB-200-2011: Accuracy<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2021-03-01<BR>ratio: 0.9815<BR>benchmarks:<BR>  Fine-grained image classification - Bird-225: Accuracy<BR>  Fine-grained image classification - Caltech-101: Accuracy<BR>  Fine-grained image classification - NABirds: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2021-07-01<BR>ratio: 0.1548<BR>benchmarks:<BR>  Fine-grained image classification - FGVC Aircraft: Accuracy<BR>  Fine-grained image classification - Stanford Dogs: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2021-10-01<BR>ratio: 0.0185<BR>benchmarks:<BR>  Fine-grained image classification - Bird-225: Accuracy<BR>","<BR>task: Image classification // Fine-grained image classification<BR>date: 2021-11-01<BR>ratio: 0.0403<BR>benchmarks:<BR>  Fine-grained image classification - Stanford Cars: Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2018-07-01<BR>ratio: 0.7769<BR>benchmarks:<BR>  Hyperspectral image classification - Pavia University: Overall Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2019-02-01<BR>ratio: 0.9838<BR>benchmarks:<BR>  Hyperspectral image classification - Indian Pines: Overall Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2020-11-01<BR>ratio: 0.1514<BR>benchmarks:<BR>  Hyperspectral image classification - Pavia University: Overall Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2020-12-01<BR>ratio: 0.0159<BR>benchmarks:<BR>  Hyperspectral image classification - Pavia University: Overall Accuracy<BR>","<BR>task: Image classification // Hyperspectral image classification<BR>date: 2021-04-01<BR>ratio: 0.0558<BR>benchmarks:<BR>  Hyperspectral image classification - Indian Pines: Overall Accuracy<BR>  Hyperspectral image classification - Pavia University: Overall Accuracy<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2018-04-01<BR>ratio: 0.4674<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-100N: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Aggregate: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random1: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random2: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random3: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Worst: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2020-02-01<BR>ratio: 0.7691<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-100N: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random2: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Worst: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2020-03-01<BR>ratio: 0.0528<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-10N-Aggregate: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2020-06-01<BR>ratio: 0.5609<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-10N-Aggregate: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random1: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random2: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random3: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2020-10-01<BR>ratio: 0.0791<BR>benchmarks:<BR>  Learning with noisy labels - CIFAR-10N-Aggregate: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random1: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random2: Accuracy (mean)<BR>  Learning with noisy labels - CIFAR-10N-Random3: Accuracy (mean)<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2021-03-01<BR>ratio: 0.2388<BR>benchmarks:<BR>  Learning with noisy labels - ANIMAL: Accuracy<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2021-04-01<BR>ratio: 0.1045<BR>benchmarks:<BR>  Learning with noisy labels - ANIMAL: Accuracy<BR>","<BR>task: Image classification // Learning with noisy labels<BR>date: 2021-11-01<BR>ratio: 0.6567<BR>benchmarks:<BR>  Learning with noisy labels - ANIMAL: Accuracy<BR>","<BR>task: Image classification // Photo geolocation estimation<BR>date: 2017-05-01<BR>ratio: 0.7063<BR>benchmarks:<BR>  Photo geolocation estimation - Im2GPS: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS: Street level (1 km)<BR>","<BR>task: Image classification // Photo geolocation estimation<BR>date: 2018-08-01<BR>ratio: 0.9091<BR>benchmarks:<BR>  Photo geolocation estimation - Im2GPS3k: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS3k: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS3k: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS3k: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS3k: Street level (1 km)<BR>  Photo geolocation estimation - Im2GPS: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS: Street level (1 km)<BR>","<BR>task: Image classification // Photo geolocation estimation<BR>date: 2018-09-01<BR>ratio: 0.3588<BR>benchmarks:<BR>  Photo geolocation estimation - Im2GPS3k: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS3k: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS3k: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS3k: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS3k: Street level (1 km)<BR>  Photo geolocation estimation - Im2GPS: City level (25 km)<BR>  Photo geolocation estimation - Im2GPS: Continent level (2500 km)<BR>  Photo geolocation estimation - Im2GPS: Country level (750 km)<BR>  Photo geolocation estimation - Im2GPS: Region level (200 km)<BR>  Photo geolocation estimation - Im2GPS: Street level (1 km)<BR>","<BR>task: Image classification // Satellite image classification<BR>date: 2015-12-01<BR>ratio: 0.4051<BR>benchmarks:<BR>  Satellite image classification - SAT-4: Accuracy<BR>","<BR>task: Image classification // Satellite image classification<BR>date: 2019-11-01<BR>ratio: 0.5949<BR>benchmarks:<BR>  Satellite image classification - SAT-4: Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2018-03-01<BR>ratio: 0.4289<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-01-01<BR>ratio: 0.1963<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-03-01<BR>ratio: 0.1019<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-05-01<BR>ratio: 0.5571<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Number of Params<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-06-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Number of Params<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2019-11-01<BR>ratio: 0.186<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2020-02-01<BR>ratio: 0.1416<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2020-05-01<BR>ratio: 0.1555<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2020-06-01<BR>ratio: 0.4496<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Number of Params<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2020-07-01<BR>ratio: 0.8944<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Number of Params<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2020-12-01<BR>ratio: 0.4024<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2021-03-01<BR>ratio: 0.0853<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2021-04-01<BR>ratio: 0.1799<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2021-06-01<BR>ratio: 0.1628<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy (kNN, k=20)<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 5 Accuracy<BR>","<BR>task: Image classification // Self-supervised image classification<BR>date: 2021-11-01<BR>ratio: 0.1163<BR>benchmarks:<BR>  Self-supervised image classification - ImageNet (finetuned): Top 1 Accuracy<BR>  Self-supervised image classification - ImageNet: Top 1 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2016-06-01<BR>ratio: 0.2965<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2016-10-01<BR>ratio: 0.6485<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2016-11-01<BR>ratio: 0.1793<BR>benchmarks:<BR>  Semi-supervised image classification - STL-10, 1000 Labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2017-03-01<BR>ratio: 0.7756<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 250 Labels: Percentage error<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>  Semi-supervised image classification - SVHN, 250 Labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2017-04-01<BR>ratio: 0.241<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 250 Labels: Percentage error<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2018-06-01<BR>ratio: 0.0789<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2018-07-01<BR>ratio: 0.4581<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 1% labeled data: Top 5 Accuracy<BR>  Semi-supervised image classification - STL-10: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-03-01<BR>ratio: 0.058<BR>benchmarks:<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-04-01<BR>ratio: 0.188<BR>benchmarks:<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-05-01<BR>ratio: 0.6158<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 250 Labels: Percentage error<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 5 Accuracy<BR>  Semi-supervised image classification - STL-10, 1000 Labels: Accuracy<BR>  Semi-supervised image classification - SVHN, 250 Labels: Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-08-01<BR>ratio: 0.3341<BR>benchmarks:<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-09-01<BR>ratio: 0.0058<BR>benchmarks:<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2019-11-01<BR>ratio: 0.8127<BR>benchmarks:<BR>  Semi-supervised image classification - CIFAR-10, 250 Labels: Percentage error<BR>  Semi-supervised image classification - CIFAR-10, 4000 Labels: Percentage error<BR>  Semi-supervised image classification - STL-10, 1000 Labels: Accuracy<BR>  Semi-supervised image classification - STL-10: Accuracy<BR>  Semi-supervised image classification - SVHN, 1000 labels: Accuracy<BR>  Semi-supervised image classification - SVHN, 250 Labels: Accuracy<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2020-01-01<BR>ratio: 0.0185<BR>benchmarks:<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2020-02-01<BR>ratio: 0.4017<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 1% labeled data: Top 5 Accuracy<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 5 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2020-03-01<BR>ratio: 0.0949<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 1 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2020-06-01<BR>ratio: 0.9051<BR>benchmarks:<BR>  Semi-supervised image classification - ImageNet - 1% labeled data: Top 5 Accuracy<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 1 Accuracy<BR>  Semi-supervised image classification - ImageNet - 10% labeled data: Top 5 Accuracy<BR>","<BR>task: Image classification // Semi-supervised image classification<BR>date: 2021-03-01<BR>ratio: 0.041<BR>benchmarks:<BR>  Semi-supervised image classification - cifar-100, 10000 Labels: Percentage error<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2015-11-01<BR>ratio: 0.4563<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2016-03-01<BR>ratio: 0.4415<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2017-10-01<BR>ratio: 0.076<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2018-03-01<BR>ratio: 0.1074<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2018-10-01<BR>ratio: 0.3878<BR>benchmarks:<BR>  Sequential image classification - Sequential CIFAR-10: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2019-10-01<BR>ratio: 0.1065<BR>benchmarks:<BR>  Sequential image classification - Sequential CIFAR-10: Unpermuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2020-06-01<BR>ratio: 0.2835<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>  Sequential image classification - noise padded CIFAR-10: % Test Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2020-08-01<BR>ratio: 0.028<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2021-02-01<BR>ratio: 0.0143<BR>benchmarks:<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2021-03-01<BR>ratio: 0.2241<BR>benchmarks:<BR>  Sequential image classification - noise padded CIFAR-10: % Test Accuracy<BR>","<BR>task: Image classification // Sequential image classification<BR>date: 2021-10-01<BR>ratio: 0.5783<BR>benchmarks:<BR>  Sequential image classification - Sequential CIFAR-10: Unpermuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Permuted Accuracy<BR>  Sequential image classification - Sequential MNIST: Unpermuted Accuracy<BR>  Sequential image classification - noise padded CIFAR-10: % Test Accuracy<BR>","<BR>task: Image classification // Superpixel image classification<BR>date: 2017-11-01<BR>ratio: 0.5176<BR>benchmarks:<BR>  Superpixel image classification - 75 Superpixel MNIST: Classification Error<BR>","<BR>task: Image classification // Superpixel image classification<BR>date: 2019-05-01<BR>ratio: 0.073<BR>benchmarks:<BR>  Superpixel image classification - 75 Superpixel MNIST: Classification Error<BR>","<BR>task: Image classification // Superpixel image classification<BR>date: 2020-02-01<BR>ratio: 0.0491<BR>benchmarks:<BR>  Superpixel image classification - 75 Superpixel MNIST: Classification Error<BR>","<BR>task: Image classification // Superpixel image classification<BR>date: 2020-03-01<BR>ratio: 0.3602<BR>benchmarks:<BR>  Superpixel image classification - 75 Superpixel MNIST: Classification Error<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2017-02-01<BR>ratio: 0.6995<BR>benchmarks:<BR>  Unsupervised image classification - SVHN: Acc<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2018-02-01<BR>ratio: 0.7118<BR>benchmarks:<BR>  Unsupervised image classification - MNIST: Accuracy<BR>  Unsupervised image classification - SVHN: Acc<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2018-07-01<BR>ratio: 0.2882<BR>benchmarks:<BR>  Unsupervised image classification - MNIST: Accuracy<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2020-05-01<BR>ratio: 0.9301<BR>benchmarks:<BR>  Unsupervised image classification - CIFAR-10: Accuracy<BR>  Unsupervised image classification - CIFAR-20: Accuracy<BR>  Unsupervised image classification - STL-10: Accuracy<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2020-12-01<BR>ratio: 0.2257<BR>benchmarks:<BR>  Unsupervised image classification - CIFAR-10: Accuracy<BR>  Unsupervised image classification - CIFAR-20: Accuracy<BR>  Unsupervised image classification - STL-10: Accuracy<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2021-03-01<BR>ratio: 0.3774<BR>benchmarks:<BR>  Unsupervised image classification - ImageNet: ARI<BR>  Unsupervised image classification - ImageNet: Accuracy (%)<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2021-08-01<BR>ratio: 0.2486<BR>benchmarks:<BR>  Unsupervised image classification - CIFAR-20: Accuracy<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2021-10-01<BR>ratio: 0.0943<BR>benchmarks:<BR>  Unsupervised image classification - ImageNet: ARI<BR>","<BR>task: Image classification // Unsupervised image classification<BR>date: 2021-11-01<BR>ratio: 0.6571<BR>benchmarks:<BR>  Unsupervised image classification - ImageNet: ARI<BR>  Unsupervised image classification - ImageNet: Accuracy (%)<BR>","<BR>task: Image clustering<BR>date: 2015-11-01<BR>ratio: 0.1213<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2016-05-01<BR>ratio: 0.4138<BR>benchmarks:<BR>  Image clustering - MNIST-full: NMI<BR>","<BR>task: Image clustering<BR>date: 2017-03-01<BR>ratio: 0.3043<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2017-04-01<BR>ratio: 0.8033<BR>benchmarks:<BR>  Image clustering - CMU-PIE: Accuracy<BR>  Image clustering - CUB Birds: Accuracy<BR>  Image clustering - CUB Birds: NMI<BR>  Image clustering - Stanford Cars: Accuracy<BR>  Image clustering - Stanford Cars: NMI<BR>  Image clustering - Stanford Dogs: Accuracy<BR>  Image clustering - Stanford Dogs: NMI<BR>","<BR>task: Image clustering<BR>date: 2017-09-01<BR>ratio: 0.912<BR>benchmarks:<BR>  Image clustering - Extended Yale-B: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2017-10-01<BR>ratio: 0.3301<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2018-04-01<BR>ratio: 0.8631<BR>benchmarks:<BR>  Image clustering - Extended Yale-B: Accuracy<BR>  Image clustering - USPS: Accuracy<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2018-07-01<BR>ratio: 0.1835<BR>benchmarks:<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>","<BR>task: Image clustering<BR>date: 2018-10-01<BR>ratio: 0.5149<BR>benchmarks:<BR>  Image clustering - CMU-PIE: Accuracy<BR>  Image clustering - USPS: Accuracy<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2018-11-01<BR>ratio: 0.7927<BR>benchmarks:<BR>  Image clustering - CUB Birds: Accuracy<BR>  Image clustering - CUB Birds: NMI<BR>  Image clustering - Stanford Cars: Accuracy<BR>  Image clustering - Stanford Cars: NMI<BR>  Image clustering - Stanford Dogs: Accuracy<BR>  Image clustering - Stanford Dogs: NMI<BR>","<BR>task: Image clustering<BR>date: 2018-12-01<BR>ratio: 0.8629<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>  Image clustering - MNIST-full: NMI<BR>  Image clustering - MNIST-test: Accuracy<BR>  Image clustering - MNIST-test: NMI<BR>  Image clustering - USPS: Accuracy<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2019-01-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>  Image clustering - MNIST-full: NMI<BR>  Image clustering - MNIST-test: Accuracy<BR>  Image clustering - MNIST-test: NMI<BR>  Image clustering - USPS: Accuracy<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2019-04-01<BR>ratio: 0.2916<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2019-08-01<BR>ratio: 0.0235<BR>benchmarks:<BR>  Image clustering - Fashion-MNIST: NMI<BR>","<BR>task: Image clustering<BR>date: 2019-09-01<BR>ratio: 0.1304<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>  Image clustering - MNIST-full: NMI<BR>","<BR>task: Image clustering<BR>date: 2019-12-01<BR>ratio: 0.3288<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2020-03-01<BR>ratio: 0.1213<BR>benchmarks:<BR>  Image clustering - CIFAR-10: ARI<BR>","<BR>task: Image clustering<BR>date: 2020-05-01<BR>ratio: 0.4192<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>","<BR>task: Image clustering<BR>date: 2020-06-01<BR>ratio: 0.9765<BR>benchmarks:<BR>  Image clustering - Fashion-MNIST: NMI<BR>","<BR>task: Image clustering<BR>date: 2020-09-01<BR>ratio: 0.1968<BR>benchmarks:<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2020-12-01<BR>ratio: 0.0833<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - STL-10: Accuracy<BR>","<BR>task: Image clustering<BR>date: 2021-03-01<BR>ratio: 0.6284<BR>benchmarks:<BR>  Image clustering - CIFAR-100: Accuracy<BR>  Image clustering - CIFAR-100: NMI<BR>  Image clustering - CIFAR-10: ARI<BR>  Image clustering - CIFAR-10: Accuracy<BR>  Image clustering - CIFAR-10: NMI<BR>  Image clustering - ImageNet-10: Accuracy<BR>  Image clustering - ImageNet-10: NMI<BR>  Image clustering - Imagenet-dog-15: ARI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>  Image clustering - STL-10: Accuracy<BR>  Image clustering - STL-10: NMI<BR>  Image clustering - Tiny-ImageNet: Accuracy<BR>  Image clustering - Tiny-ImageNet: NMI<BR>","<BR>task: Image clustering<BR>date: 2021-05-01<BR>ratio: 0.0336<BR>benchmarks:<BR>  Image clustering - Imagenet-dog-15: ARI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>","<BR>task: Image clustering<BR>date: 2021-07-01<BR>ratio: 0.1463<BR>benchmarks:<BR>  Image clustering - MNIST-full: Accuracy<BR>  Image clustering - MNIST-full: NMI<BR>  Image clustering - USPS: Accuracy<BR>  Image clustering - USPS: NMI<BR>","<BR>task: Image clustering<BR>date: 2021-11-01<BR>ratio: 0.3591<BR>benchmarks:<BR>  Image clustering - Imagenet-dog-15: ARI<BR>  Image clustering - Imagenet-dog-15: Accuracy<BR>  Image clustering - Imagenet-dog-15: NMI<BR>","<BR>task: Image denoising<BR>date: 2016-08-01<BR>ratio: 0.6327<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>","<BR>task: Image denoising<BR>date: 2017-04-01<BR>ratio: 0.9412<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>","<BR>task: Image denoising<BR>date: 2017-10-01<BR>ratio: 0.2941<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-02-01<BR>ratio: 0.2391<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-05-01<BR>ratio: 0.9987<BR>benchmarks:<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - BSD68 sigma25: PSNR<BR>  Grayscale image denoising - BSD68 sigma50: PSNR<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-06-01<BR>ratio: 0.1772<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - Set12 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2018-07-01<BR>ratio: 0.8352<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2018-11-01<BR>ratio: 0.3418<BR>benchmarks:<BR>  Color image denoising - Darmstadt Noise Dataset: PSNR (sRGB)<BR>  Color image denoising - Darmstadt Noise Dataset: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2019-04-01<BR>ratio: 0.9434<BR>benchmarks:<BR>  Color image denoising - BSD68 sigma15: PSNR<BR>  Color image denoising - BSD68 sigma25: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2019-07-01<BR>ratio: 0.011<BR>benchmarks:<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>","<BR>task: Image denoising<BR>date: 2019-08-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2019-10-01<BR>ratio: 0.9673<BR>benchmarks:<BR>  Grayscale image denoising - BSD200 sigma30: PSNR<BR>  Grayscale image denoising - BSD200 sigma50: PSNR<BR>  Grayscale image denoising - BSD200 sigma70: PSNR<BR>","<BR>task: Image denoising<BR>date: 2020-01-01<BR>ratio: 0.1066<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2020-03-01<BR>ratio: 0.2143<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - DND: SSIM (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2020-12-01<BR>ratio: 0.7609<BR>benchmarks:<BR>  Color image denoising - CBSD68 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2021-05-01<BR>ratio: 0.0292<BR>benchmarks:<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2021-06-01<BR>ratio: 0.0457<BR>benchmarks:<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: SSIM (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2021-08-01<BR>ratio: 0.4949<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - BSD68 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Image denoising<BR>date: 2021-11-01<BR>ratio: 0.7097<BR>benchmarks:<BR>  Color image denoising - Kodak24 sigma50: PSNR<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - Urban100 sigma15: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>  Image denoising - DND: PSNR (sRGB)<BR>  Image denoising - SIDD: PSNR (sRGB)<BR>","<BR>task: Image denoising<BR>date: 2021-12-01<BR>ratio: 0.1899<BR>benchmarks:<BR>  Color image denoising - Urban100 sigma50: PSNR<BR>  Grayscale image denoising - Urban100 sigma25: PSNR<BR>  Grayscale image denoising - Urban100 sigma50: PSNR<BR>","<BR>task: Image enhancement<BR>date: 2019-06-01<BR>ratio: 0.8409<BR>benchmarks:<BR>  Low-light image enhancement - DICM: User Study Score<BR>  Low-light image enhancement - MEF: User Study Score<BR>  Low-light image enhancement - VV: User Study Score<BR>","<BR>task: Image enhancement<BR>date: 2019-08-01<BR>ratio: 0.396<BR>benchmarks:<BR>  Image relighting - VIDIT\u201920 validation set: LPIPS<BR>","<BR>task: Image enhancement<BR>date: 2019-10-01<BR>ratio: 0.5217<BR>benchmarks:<BR>  Image enhancement - TIP 2018: PSNR<BR>  Image enhancement - TIP 2018: SSIM<BR>","<BR>task: Image enhancement<BR>date: 2020-01-01<BR>ratio: 0.4176<BR>benchmarks:<BR>  Low-light image enhancement - DICM: User Study Score<BR>  Low-light image enhancement - MEF: User Study Score<BR>  Low-light image enhancement - VV: User Study Score<BR>","<BR>task: Image enhancement<BR>date: 2020-08-01<BR>ratio: 0.9559<BR>benchmarks:<BR>  Image relighting - VIDIT\u201920 validation set: PSNR<BR>  Image relighting - VIDIT\u201920 validation set: SSIM<BR>","<BR>task: Image enhancement<BR>date: 2020-09-01<BR>ratio: 0.6924<BR>benchmarks:<BR>  Image relighting - VIDIT\u201920 validation set: LPIPS<BR>  Image relighting - VIDIT\u201920 validation set: SSIM<BR>","<BR>task: Image enhancement<BR>date: 2021-05-01<BR>ratio: 0.0441<BR>benchmarks:<BR>  Image relighting - VIDIT\u201920 validation set: LPIPS<BR>  Image relighting - VIDIT\u201920 validation set: PSNR<BR>  Image relighting - VIDIT\u201920 validation set: SSIM<BR>","<BR>task: Image enhancement<BR>date: 2021-06-01<BR>ratio: 0.6096<BR>benchmarks:<BR>  Image enhancement - TIP 2018: PSNR<BR>  Image enhancement - TIP 2018: SSIM<BR>","<BR>task: Image generation<BR>date: 2014-06-01<BR>ratio: 0.3202<BR>benchmarks:<BR>  Image generation - Binarized MNIST: nats<BR>","<BR>task: Image generation<BR>date: 2015-12-01<BR>ratio: 0.0148<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2016-01-01<BR>ratio: 0.4807<BR>benchmarks:<BR>  Image generation - Binarized MNIST: nats<BR>","<BR>task: Image generation<BR>date: 2016-06-01<BR>ratio: 0.3842<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Image generation - ImageNet 32x32: bpd<BR>","<BR>task: Image generation<BR>date: 2016-10-01<BR>ratio: 0.0407<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2016-11-01<BR>ratio: 0.7294<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: Image generation<BR>date: 2016-12-01<BR>ratio: 0.1552<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-02-01<BR>ratio: 0.0646<BR>benchmarks:<BR>  Image generation - CIFAR-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-03-01<BR>ratio: 0.2431<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Image generation - CIFAR-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-06-01<BR>ratio: 0.1654<BR>benchmarks:<BR>  Image generation - CIFAR-10: FID<BR>","<BR>task: Image generation<BR>date: 2017-07-01<BR>ratio: 0.1864<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2017-09-01<BR>ratio: 0.0509<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Image generation - CIFAR-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-10-01<BR>ratio: 0.877<BR>benchmarks:<BR>  Image generation - CIFAR-10: Inception score<BR>  Text-to-image generation - CUB: FID<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Image generation<BR>date: 2017-11-01<BR>ratio: 0.949<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: Inception score<BR>  Unsupervised image-to-image translation - SVNH-to-MNIST: Classification Accuracy<BR>","<BR>task: Image generation<BR>date: 2017-12-01<BR>ratio: 0.9041<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>  Pose transfer - Deep-Fashion: IS<BR>","<BR>task: Image generation<BR>date: 2018-02-01<BR>ratio: 0.375<BR>benchmarks:<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - ImageNet 32x32: bpd<BR>  Image generation - STL-10: Inception score<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2018-03-01<BR>ratio: 0.4841<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - STL-10: FID<BR>","<BR>task: Image generation<BR>date: 2018-04-01<BR>ratio: 0.7947<BR>benchmarks:<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Handbags: Diversity<BR>  Multimodal unsupervised image-to-image translation - Edge-to-Shoes: Diversity<BR>","<BR>task: Image generation<BR>date: 2018-05-01<BR>ratio: 0.3697<BR>benchmarks:<BR>  Conditional image generation - ImageNet 128x128: FID<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>","<BR>task: Image generation<BR>date: 2018-07-01<BR>ratio: 0.8489<BR>benchmarks:<BR>  Image generation - CAT 256x256: FID<BR>","<BR>task: Image generation<BR>date: 2018-08-01<BR>ratio: 0.0643<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2018-09-01<BR>ratio: 0.5338<BR>benchmarks:<BR>  Conditional image generation - ImageNet 128x128: FID<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - CIFAR-10: Inception score<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>","<BR>task: Image generation<BR>date: 2018-10-01<BR>ratio: 0.1206<BR>benchmarks:<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2018-11-01<BR>ratio: 0.7979<BR>benchmarks:<BR>  Image generation - CUB 128 x 128: FID<BR>  Image generation - Stanford Cars: FID<BR>  Image generation - Stanford Dogs: FID<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Layout-to-image generation - COCO-Stuff 64x64: FID<BR>  Layout-to-image generation - COCO-Stuff 64x64: Inception Score<BR>  Layout-to-image generation - Visual Genome 64x64: FID<BR>  Layout-to-image generation - Visual Genome 64x64: Inception Score<BR>","<BR>task: Image generation<BR>date: 2018-12-01<BR>ratio: 0.7805<BR>benchmarks:<BR>  Image generation - CelebA-HQ 1024x1024: FID<BR>  Image generation - FFHQ: FID<BR>  Image generation - ImageNet 64x64: Bits per dim<BR>  Image generation - LSUN Churches 256 x 256: FID<BR>  Image generation - STL-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2019-01-01<BR>ratio: 0.4013<BR>benchmarks:<BR>  Image generation - CelebA-HQ 128x128: FID<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: Image generation<BR>date: 2019-03-01<BR>ratio: 0.8755<BR>benchmarks:<BR>  Image generation - CelebA-HQ 128x128: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Image generation<BR>date: 2019-04-01<BR>ratio: 0.9951<BR>benchmarks:<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Dayton (256\u00d7256) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64x64) - ground-to-aerial: SSIM<BR>  Cross-view image-to-image translation - Dayton (64\u00d764) - aerial-to-ground: SSIM<BR>  Cross-view image-to-image translation - Ego2Top: SSIM<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Image generation - FFHQ: FID<BR>  Image generation - ImageNet 64x64: Bits per dim<BR>  Image generation - LSUN Bedroom 256 x 256: FID<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Pose transfer - Deep-Fashion: SSIM<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - CUB: Inception score<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: Image generation<BR>date: 2019-05-01<BR>ratio: 0.625<BR>benchmarks:<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>","<BR>task: Image generation<BR>date: 2019-07-01<BR>ratio: 0.6195<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - CelebA 256x256: FID<BR>","<BR>task: Image generation<BR>date: 2019-08-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - STL-10: FID<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Layout-to-image generation - COCO-Stuff 64x64: FID<BR>  Layout-to-image generation - COCO-Stuff 64x64: Inception Score<BR>  Layout-to-image generation - Visual Genome 64x64: Inception Score<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>","<BR>task: Image generation<BR>date: 2019-09-01<BR>ratio: 0.1429<BR>benchmarks:<BR>  Layout-to-image generation - COCO-Stuff 64x64: Inception Score<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Image generation<BR>date: 2019-10-01<BR>ratio: 0.3825<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>","<BR>task: Image generation<BR>date: 2019-11-01<BR>ratio: 0.2232<BR>benchmarks:<BR>  Image generation - CIFAR-10: FID<BR>  Synthetic-to-real translation - Syn2Real-C: Accuracy<BR>","<BR>task: Image generation<BR>date: 2019-12-01<BR>ratio: 0.9091<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Conditional image generation - ImageNet 128x128: FID<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>  Cross-view image-to-image translation - cvusa: SSIM<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Image generation - CIFAR-100: FID<BR>  Image generation - FFHQ: FID<BR>  Image generation - LSUN Cat 256 x 256: FID<BR>  Image generation - LSUN Churches 256 x 256: FID<BR>  Image generation - STL-10: FID<BR>  Image generation - STL-10: Inception score<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Text-to-image generation - CUB: Inception score<BR>","<BR>task: Image generation<BR>date: 2020-02-01<BR>ratio: 0.1661<BR>benchmarks:<BR>  Image generation - CelebA-HQ 128x128: FID<BR>","<BR>task: Image generation<BR>date: 2020-03-01<BR>ratio: 0.7742<BR>benchmarks:<BR>  Image generation - ImageNet 64x64: Bits per dim<BR>  Image-to-image translation - Cityscapes-to-Foggy Cityscapes: mAP<BR>  Layout-to-image generation - COCO-Stuff 128x128: FID<BR>  Layout-to-image generation - COCO-Stuff 128x128: Inception Score<BR>  Layout-to-image generation - COCO-Stuff 64x64: FID<BR>  Layout-to-image generation - COCO-Stuff 64x64: Inception Score<BR>  Layout-to-image generation - Visual Genome 128x128: FID<BR>  Layout-to-image generation - Visual Genome 128x128: Inception Score<BR>  Layout-to-image generation - Visual Genome 64x64: FID<BR>  Layout-to-image generation - Visual Genome 64x64: Inception Score<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: Image generation<BR>date: 2020-04-01<BR>ratio: 0.3693<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Image generation - STL-10: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: Accuracy<BR>  Image-to-image translation - ADE20K Labels-to-Photos: FID<BR>  Image-to-image translation - ADE20K Labels-to-Photos: mIoU<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: Per-pixel Accuracy<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>","<BR>task: Image generation<BR>date: 2020-05-01<BR>ratio: 0.7978<BR>benchmarks:<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: FID<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: LPIPS<BR>  Face hallucination - FFHQ 512 x 512 - 16x upscaling: NIQE<BR>  Pose transfer - Deep-Fashion: SSIM<BR>","<BR>task: Image generation<BR>date: 2020-06-01<BR>ratio: 0.4545<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Image generation - Binarized MNIST: nats<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - CIFAR-10: Inception score<BR>  Image generation - LSUN Bedroom 256 x 256: FID<BR>","<BR>task: Image generation<BR>date: 2020-07-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Image generation - CIFAR-10: Inception score<BR>  Pose transfer - Deep-Fashion: IS<BR>  Pose transfer - Deep-Fashion: SSIM<BR>  Pose transfer - Market-1501: SSIM<BR>  Pose transfer - Market-1501: mask-SSIM<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: Image generation<BR>date: 2020-08-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Pose transfer - Market-1501: SSIM<BR>  Pose transfer - Market-1501: mask-SSIM<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>","<BR>task: Image generation<BR>date: 2020-10-01<BR>ratio: 0.4878<BR>benchmarks:<BR>  Image generation - CelebA 256x256: FID<BR>  Image generation - CelebA 64x64: FID<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: Inception score<BR>  Text-to-image generation - CUB: FID<BR>","<BR>task: Image generation<BR>date: 2020-11-01<BR>ratio: 0.6638<BR>benchmarks:<BR>  Conditional image generation - ImageNet 128x128: Inception score<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - CIFAR-10: Inception score<BR>  Image generation - FFHQ 256 x 256: FID<BR>  Image generation - LSUN Churches 256 x 256: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: Accuracy<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>","<BR>task: Image generation<BR>date: 2020-12-01<BR>ratio: 0.6488<BR>benchmarks:<BR>  Image generation - ImageNet 256x256: FID<BR>  Image-to-image translation - ADE20K-Outdoor Labels-to-Photos: mIoU<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: mIoU<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: FID<BR>  Image-to-image translation - Cityscapes Labels-to-Photo: mIoU<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Image generation<BR>date: 2021-01-01<BR>ratio: 0.6992<BR>benchmarks:<BR>  Image generation - ADE-Indoor: FID<BR>  Image generation - ImageNet 32x32: bpd<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>  Text-to-image generation - COCO: FID<BR>","<BR>task: Image generation<BR>date: 2021-02-01<BR>ratio: 0.1321<BR>benchmarks:<BR>  Image generation - STL-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2021-03-01<BR>ratio: 0.8549<BR>benchmarks:<BR>  Image generation - FFHQ 256 x 256: FID<BR>  Image generation - FFHQ: FID<BR>  Layout-to-image generation - COCO-Stuff 128x128: FID<BR>  Layout-to-image generation - COCO-Stuff 128x128: Inception Score<BR>  Layout-to-image generation - Visual Genome 128x128: FID<BR>  Layout-to-image generation - Visual Genome 128x128: Inception Score<BR>","<BR>task: Image generation<BR>date: 2021-04-01<BR>ratio: 0.6364<BR>benchmarks:<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: FID<BR>  Fundus to angiography generation - Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients: Kernel Inception Distance<BR>  Image generation - CAT 256x256: FID<BR>  Image generation - CIFAR-100: FID<BR>  Image generation - STL-10: Inception score<BR>  Pose transfer - Deep-Fashion: SSIM<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: Acc<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Image generation<BR>date: 2021-05-01<BR>ratio: 0.8612<BR>benchmarks:<BR>  Image generation - Binarized MNIST: nats<BR>  Image generation - CIFAR-10: Inception score<BR>  Image generation - ImageNet 128x128: FID<BR>  Image generation - ImageNet 256x256: FID<BR>  Image generation - LSUN Bedroom 256 x 256: FID<BR>  Image generation - LSUN Cat 256 x 256: FID<BR>  Image generation - LSUN Horse 256 x 256: FID<BR>","<BR>task: Image generation<BR>date: 2021-06-01<BR>ratio: 0.5652<BR>benchmarks:<BR>  Image generation - CIFAR-10: FID<BR>  Image generation - CIFAR-10: Inception score<BR>  Image generation - CelebA 256x256: FID<BR>  Image generation - CelebA 64x64: FID<BR>  Image generation - CelebA-HQ 256x256: FID<BR>  Image generation - FFHQ 256 x 256: FID<BR>  Image generation - ImageNet 32x32: bpd<BR>  Image generation - ImageNet 64x64: Bits per dim<BR>  Image generation - STL-10: FID<BR>  Image generation - STL-10: Inception score<BR>","<BR>task: Image generation<BR>date: 2021-07-01<BR>ratio: 0.037<BR>benchmarks:<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Image generation<BR>date: 2021-08-01<BR>ratio: 0.1388<BR>benchmarks:<BR>  Image generation - ImageNet 128x128: FID<BR>  Image generation - ImageNet 256x256: FID<BR>","<BR>task: Image generation<BR>date: 2021-09-01<BR>ratio: 0.0854<BR>benchmarks:<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>","<BR>task: Image generation<BR>date: 2021-11-01<BR>ratio: 0.897<BR>benchmarks:<BR>  Conditional image generation - CIFAR-10: FID<BR>  Conditional image generation - CIFAR-10: Inception score<BR>  Image generation - ADE-Indoor: FID<BR>  Image generation - CUB 128 x 128: FID<BR>  Image generation - LSUN Bedroom 256 x 256: FID<BR>  Image generation - LSUN Cat 256 x 256: FID<BR>  Image generation - LSUN Churches 256 x 256: FID<BR>  Image generation - Stanford Cars: FID<BR>  Image generation - Stanford Dogs: FID<BR>  Image-to-image translation - GTAV-to-Cityscapes Labels: mIoU<BR>  Image-to-image translation - SYNTHIA-to-Cityscapes: mIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (13 classes)<BR>  Synthetic-to-real translation - SYNTHIA-to-Cityscapes: MIoU (16 classes)<BR>  Text-to-image generation - COCO: FID<BR>  Text-to-image generation - COCO: SOA-C<BR>  Text-to-image generation - Multi-Modal-CelebA-HQ: FID<BR>","<BR>task: Image generation<BR>date: 2021-12-01<BR>ratio: 0.818<BR>benchmarks:<BR>  Image generation - CelebA-HQ 1024x1024: FID<BR>  Image generation - CelebA-HQ 256x256: FID<BR>  Image generation - FFHQ 256 x 256: FID<BR>  Image generation - LSUN Cat 256 x 256: FID<BR>  Image generation - LSUN Horse 256 x 256: FID<BR>  Image-to-image translation - COCO-Stuff Labels-to-Photos: FID<BR>  Text-to-image generation - COCO: Inception score<BR>","<BR>task: Image matching<BR>date: 2019-11-01<BR>ratio: 0.2414<BR>benchmarks:<BR>  Image matching - IMC PhotoTourism: mean average accuracy @ 10<BR>","<BR>task: Image matching<BR>date: 2020-03-01<BR>ratio: 0.093<BR>benchmarks:<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2020-06-01<BR>ratio: 0.383<BR>benchmarks:<BR>  Image matching - IMC PhotoTourism: mean average accuracy @ 10<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - PF-WILLOW: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2020-07-01<BR>ratio: 0.4419<BR>benchmarks:<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2021-03-01<BR>ratio: 0.3462<BR>benchmarks:<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - PF-WILLOW: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2021-04-01<BR>ratio: 0.715<BR>benchmarks:<BR>  Image matching - IMC PhotoTourism: mean average accuracy @ 10<BR>","<BR>task: Image matching<BR>date: 2021-06-01<BR>ratio: 0.2326<BR>benchmarks:<BR>  Semantic correspondence - PF-PASCAL: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2021-07-01<BR>ratio: 0.0192<BR>benchmarks:<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matching<BR>date: 2021-12-01<BR>ratio: 0.3404<BR>benchmarks:<BR>  Semantic correspondence - PF-WILLOW: PCK<BR>  Semantic correspondence - SPair-71k: PCK<BR>","<BR>task: Image matting<BR>date: 2019-01-01<BR>ratio: 0.7539<BR>benchmarks:<BR>  Image matting - AIM-500: MSE<BR>  Image matting - AIM-500: SAD<BR>","<BR>task: Image matting<BR>date: 2019-06-01<BR>ratio: 0.4252<BR>benchmarks:<BR>  Image matting - AM-2K: MAD<BR>","<BR>task: Image matting<BR>date: 2019-08-01<BR>ratio: 0.2257<BR>benchmarks:<BR>  Image matting - Composition-1K: Conn<BR>  Image matting - Composition-1K: Grad<BR>  Image matting - Composition-1K: MSE<BR>  Semantic image matting - Semantic Image Matting Dataset: MSE(10^3)<BR>","<BR>task: Image matting<BR>date: 2019-09-01<BR>ratio: 0.4848<BR>benchmarks:<BR>  Image matting - Composition-1K: Conn<BR>  Image matting - Composition-1K: Grad<BR>  Image matting - Composition-1K: MSE<BR>  Image matting - Composition-1K: SAD<BR>","<BR>task: Image matting<BR>date: 2020-01-01<BR>ratio: 0.4351<BR>benchmarks:<BR>  Semantic image matting - Semantic Image Matting Dataset: Conn<BR>  Semantic image matting - Semantic Image Matting Dataset: Grad<BR>  Semantic image matting - Semantic Image Matting Dataset: MSE(10^3)<BR>  Semantic image matting - Semantic Image Matting Dataset: SAD<BR>","<BR>task: Image matting<BR>date: 2020-03-01<BR>ratio: 0.4234<BR>benchmarks:<BR>  Image matting - Composition-1K: Conn<BR>  Image matting - Composition-1K: Grad<BR>  Image matting - Composition-1K: MSE<BR>  Image matting - Composition-1K: SAD<BR>","<BR>task: Image matting<BR>date: 2020-04-01<BR>ratio: 0.5748<BR>benchmarks:<BR>  Image matting - AM-2K: MAD<BR>","<BR>task: Image matting<BR>date: 2020-06-01<BR>ratio: 0.6216<BR>benchmarks:<BR>  Image matting - AM-2K: MSE<BR>  Image matting - P3M-10k: MSE<BR>","<BR>task: Image matting<BR>date: 2020-10-01<BR>ratio: 0.7045<BR>benchmarks:<BR>  Image matting - AIM-500: MSE<BR>  Image matting - AIM-500: SAD<BR>  Image matting - AM-2K: MSE<BR>  Image matting - P3M-10k: MSE<BR>  Image matting - P3M-10k: SAD<BR>","<BR>task: Image matting<BR>date: 2021-04-01<BR>ratio: 0.8522<BR>benchmarks:<BR>  Image matting - P3M-10k: MSE<BR>  Image matting - P3M-10k: SAD<BR>  Semantic image matting - Semantic Image Matting Dataset: Conn<BR>  Semantic image matting - Semantic Image Matting Dataset: Grad<BR>  Semantic image matting - Semantic Image Matting Dataset: MSE(10^3)<BR>  Semantic image matting - Semantic Image Matting Dataset: SAD<BR>","<BR>task: Image matting<BR>date: 2021-07-01<BR>ratio: 0.0691<BR>benchmarks:<BR>  Image matting - AIM-500: MSE<BR>  Image matting - AIM-500: SAD<BR>","<BR>task: Image matting<BR>date: 2021-09-01<BR>ratio: 0.1261<BR>benchmarks:<BR>  Image matting - Composition-1K: Conn<BR>  Image matting - Composition-1K: Grad<BR>  Image matting - Composition-1K: MSE<BR>  Image matting - Composition-1K: SAD<BR>","<BR>task: Image quality assessment<BR>date: 2016-04-01<BR>ratio: 0.4868<BR>benchmarks:<BR>  Aesthetics quality assessment - AVA: Accuracy<BR>","<BR>task: Image quality assessment<BR>date: 2017-04-01<BR>ratio: 0.4474<BR>benchmarks:<BR>  Aesthetics quality assessment - AVA: Accuracy<BR>","<BR>task: Image quality assessment<BR>date: 2018-10-01<BR>ratio: 0.0658<BR>benchmarks:<BR>  Aesthetics quality assessment - AVA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2015-11-01<BR>ratio: 0.0843<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 open ended: Percentage correct<BR>","<BR>task: Image question answering<BR>date: 2016-03-01<BR>ratio: 0.2881<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 open ended: Percentage correct<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>  Image question answering - VQA v1 test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2016-04-01<BR>ratio: 0.1571<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 multiple choice: Percentage correct<BR>","<BR>task: Image question answering<BR>date: 2016-05-01<BR>ratio: 0.2982<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 multiple choice: Percentage correct<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 open ended: Percentage correct<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>  Image question answering - VQA v1 test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2016-06-01<BR>ratio: 0.5714<BR>benchmarks:<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 multiple choice: Percentage correct<BR>  Image question answering - COCO Visual Question Answering (VQA) real images 1.0 open ended: Percentage correct<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>  Image question answering - VQA v1 test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2016-11-01<BR>ratio: 0.0169<BR>benchmarks:<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2017-04-01<BR>ratio: 0.2456<BR>benchmarks:<BR>  Image question answering - VQA v1 test-dev: Accuracy<BR>  Image question answering - VQA v1 test-std: Accuracy<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2017-05-01<BR>ratio: 0.2696<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2017-07-01<BR>ratio: 0.1545<BR>benchmarks:<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2017-08-01<BR>ratio: 0.1355<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2017-09-01<BR>ratio: 0.6159<BR>benchmarks:<BR>  Image question answering - CLEVR-Humans: Accuracy<BR>  Image question answering - CLEVR: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2018-03-01<BR>ratio: 0.4828<BR>benchmarks:<BR>  Image question answering - CLEVR-Humans: Accuracy<BR>  Image question answering - CLEVR: Accuracy<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>  Image question answering - MSVD-QA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2018-05-01<BR>ratio: 0.0094<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2018-10-01<BR>ratio: 0.2414<BR>benchmarks:<BR>  Image question answering - CLEVR: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2019-02-01<BR>ratio: 0.3594<BR>benchmarks:<BR>  Image question answering - VQA-CP: Score<BR>","<BR>task: Image question answering<BR>date: 2019-04-01<BR>ratio: 0.1333<BR>benchmarks:<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>  Image question answering - MSVD-QA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2019-05-01<BR>ratio: 0.3271<BR>benchmarks:<BR>  Image question answering - GQA test-std: Accuracy<BR>  Image question answering - VQA-CP: Score<BR>","<BR>task: Image question answering<BR>date: 2019-06-01<BR>ratio: 0.0326<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2019-07-01<BR>ratio: 0.7877<BR>benchmarks:<BR>  Image question answering - GQA Test2019: Accuracy<BR>  Image question answering - GQA Test2019: Binary<BR>  Image question answering - GQA Test2019: Consistency<BR>  Image question answering - GQA Test2019: Open<BR>  Image question answering - GQA Test2019: Validity<BR>  Image question answering - GQA test-dev: Accuracy<BR>  Image question answering - GQA test-std: Accuracy<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2019-08-01<BR>ratio: 0.1784<BR>benchmarks:<BR>  Image question answering - GQA Test2019: Accuracy<BR>  Image question answering - GQA Test2019: Binary<BR>  Image question answering - GQA Test2019: Consistency<BR>  Image question answering - GQA Test2019: Open<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2019-09-01<BR>ratio: 0.6897<BR>benchmarks:<BR>  Image question answering - PlotQA-D2: 1:1 Accuracy<BR>  Image question answering - VCR (Q-A) test: Accuracy<BR>  Image question answering - VCR (Q-AR) test: Accuracy<BR>  Image question answering - VCR (QA-R) test: Accuracy<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA-CP: Score<BR>","<BR>task: Image question answering<BR>date: 2020-02-01<BR>ratio: 0.2321<BR>benchmarks:<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>  Image question answering - MSVD-QA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2020-03-01<BR>ratio: 0.2277<BR>benchmarks:<BR>  Image question answering - VQA-CP: Score<BR>","<BR>task: Image question answering<BR>date: 2020-04-01<BR>ratio: 0.0321<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2020-06-01<BR>ratio: 0.713<BR>benchmarks:<BR>  Image question answering - VCR (Q-A) test: Accuracy<BR>  Image question answering - VCR (Q-AR) test: Accuracy<BR>  Image question answering - VCR (QA-R) test: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2020-12-01<BR>ratio: 0.68<BR>benchmarks:<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>  Image question answering - MSVD-QA: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2021-01-01<BR>ratio: 0.5909<BR>benchmarks:<BR>  Image question answering - GQA Test2019: Accuracy<BR>  Image question answering - GQA Test2019: Binary<BR>  Image question answering - GQA Test2019: Consistency<BR>  Image question answering - GQA Test2019: Open<BR>  Image question answering - GQA Test2019: Validity<BR>  Image question answering - VQA v2 test-std: number<BR>  Image question answering - VQA v2 test-std: other<BR>  Image question answering - VQA v2 test-std: overall<BR>  Image question answering - VQA v2 test-std: yes/no<BR>","<BR>task: Image question answering<BR>date: 2021-04-01<BR>ratio: 0.0132<BR>benchmarks:<BR>  Image question answering - CLEVR-Humans: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2021-07-01<BR>ratio: 0.1117<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2021-08-01<BR>ratio: 0.2317<BR>benchmarks:<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA v2 test-std: overall<BR>","<BR>task: Image question answering<BR>date: 2021-10-01<BR>ratio: 0.5613<BR>benchmarks:<BR>  Image question answering - GQA test-dev: Accuracy<BR>  Image question answering - GQA test-std: Accuracy<BR>","<BR>task: Image question answering<BR>date: 2021-11-01<BR>ratio: 0.7684<BR>benchmarks:<BR>  Image question answering - PlotQA-D2: 1:1 Accuracy<BR>  Image question answering - VQA v2 test-dev: Accuracy<BR>  Image question answering - VQA v2 test-std: number<BR>  Image question answering - VQA v2 test-std: other<BR>  Image question answering - VQA v2 test-std: overall<BR>  Image question answering - VQA v2 test-std: yes/no<BR>","<BR>task: Image question answering<BR>date: 2021-12-01<BR>ratio: 0.0536<BR>benchmarks:<BR>  Image question answering - MSRVTT-QA: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2015-03-01<BR>ratio: 0.5888<BR>benchmarks:<BR>  Face recognition - CASIA-WebFace+masks: Accuracy<BR>  Face recognition - CelebA+masks: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2015-11-01<BR>ratio: 0.9003<BR>benchmarks:<BR>  Age-invariant face recognition - CACDVS: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2016-06-01<BR>ratio: 0.0303<BR>benchmarks:<BR>  Age-invariant face recognition - CACDVS: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2017-03-01<BR>ratio: 0.0347<BR>benchmarks:<BR>  Age-invariant face recognition - CACDVS: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2018-01-01<BR>ratio: 0.3164<BR>benchmarks:<BR>  Face recognition - CASIA-WebFace+masks: Accuracy<BR>  Face recognition - CelebA+masks: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2018-09-01<BR>ratio: 0.0347<BR>benchmarks:<BR>  Age-invariant face recognition - CACDVS: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2021-09-01<BR>ratio: 0.4444<BR>benchmarks:<BR>  Face recognition - AgeDB-30: Accuracy<BR>  Face recognition - CASIA-WebFace+masks: Accuracy<BR>  Face recognition - CelebA+masks: Accuracy<BR>","<BR>task: Image recognition<BR>date: 2021-11-01<BR>ratio: 0.5556<BR>benchmarks:<BR>  Face recognition - AgeDB-30: Accuracy<BR>","<BR>task: Image reconstruction<BR>date: 2017-06-01<BR>ratio: 0.9545<BR>benchmarks:<BR>  Image reconstruction - Edge-to-Handbags: FID<BR>  Image reconstruction - Edge-to-Handbags: LPIPS<BR>  Image reconstruction - Edge-to-Shoes: FID<BR>  Image reconstruction - Edge-to-Shoes: LPIPS<BR>","<BR>task: Image reconstruction<BR>date: 2019-03-01<BR>ratio: 0.6315<BR>benchmarks:<BR>  Image reconstruction - Edge-to-Handbags: FID<BR>  Image reconstruction - Edge-to-Handbags: LPIPS<BR>  Image reconstruction - Edge-to-Shoes: FID<BR>  Image reconstruction - Edge-to-Shoes: LPIPS<BR>","<BR>task: Image restoration<BR>date: 2016-06-01<BR>ratio: 0.5238<BR>benchmarks:<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>","<BR>task: Image restoration<BR>date: 2017-08-01<BR>ratio: 0.7059<BR>benchmarks:<BR>  JPEG artifact correction - Classic5 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2018-05-01<BR>ratio: 0.8942<BR>benchmarks:<BR>  JPEG artifact correction - Classic5 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 40 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 30 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 40 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2018-06-01<BR>ratio: 0.6842<BR>benchmarks:<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2018-10-01<BR>ratio: 0.5635<BR>benchmarks:<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2018-12-01<BR>ratio: 0.1607<BR>benchmarks:<BR>  JPEG artifact correction - Classic5 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 40 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 40 Grayscale): PSNR<BR>","<BR>task: Image restoration<BR>date: 2020-04-01<BR>ratio: 0.519<BR>benchmarks:<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Grayscale): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): SSIM<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR-B<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 30 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): SSIM<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): SSIM<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): SSIM<BR>","<BR>task: Image restoration<BR>date: 2021-04-01<BR>ratio: 0.714<BR>benchmarks:<BR>  Underwater Image Restoration - LSUI: PSNR<BR>","<BR>task: Image restoration<BR>date: 2021-09-01<BR>ratio: 0.1765<BR>benchmarks:<BR>  JPEG artifact correction - Classic5 (Quality 10 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 30 Grayscale): PSNR<BR>  JPEG artifact correction - Classic5 (Quality 40 Grayscale): PSNR<BR>  JPEG artifact correction - ICB (Quality 10 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 20 Color): PSNR<BR>  JPEG artifact correction - ICB (Quality 30 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 10 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Color): PSNR<BR>  JPEG artifact correction - LIVE1 (Quality 20 Grayscale): PSNR<BR>  JPEG artifact correction - Live1 (Quality 10 Grayscale): PSNR<BR>","<BR>task: Image restoration<BR>date: 2021-11-01<BR>ratio: 0.286<BR>benchmarks:<BR>  Underwater Image Restoration - LSUI: PSNR<BR>","<BR>task: Image retrieval<BR>date: 2015-04-01<BR>ratio: 0.3963<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2015-11-01<BR>ratio: 0.0927<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2016-04-01<BR>ratio: 0.6636<BR>benchmarks:<BR>  Image retrieval - Oxf105k: MAP<BR>  Image retrieval - Par106k: mAP<BR>  Image retrieval - Par6k: mAP<BR>","<BR>task: Image retrieval<BR>date: 2016-08-01<BR>ratio: 0.0879<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2016-11-01<BR>ratio: 0.222<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2016-12-01<BR>ratio: 0.1681<BR>benchmarks:<BR>  Image retrieval - Oxf105k: MAP<BR>  Image retrieval - Oxf5k: MAP<BR>  Image retrieval - Par106k: mAP<BR>  Image retrieval - Par6k: mAP<BR>","<BR>task: Image retrieval<BR>date: 2017-06-01<BR>ratio: 0.0667<BR>benchmarks:<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-10<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-1<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-50<BR>","<BR>task: Image retrieval<BR>date: 2017-07-01<BR>ratio: 0.2778<BR>benchmarks:<BR>  Image retrieval - Oxf5k: MAP<BR>","<BR>task: Image retrieval<BR>date: 2017-11-01<BR>ratio: 0.9402<BR>benchmarks:<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-10<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-1<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-5<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@10<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@1<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@5<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>","<BR>task: Image retrieval<BR>date: 2017-12-01<BR>ratio: 0.0317<BR>benchmarks:<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2018-01-01<BR>ratio: 0.2848<BR>benchmarks:<BR>  Image retrieval - SOP: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2018-03-01<BR>ratio: 0.4922<BR>benchmarks:<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2018-04-01<BR>ratio: 0.1273<BR>benchmarks:<BR>  Image retrieval - SOP: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2018-11-01<BR>ratio: 0.5833<BR>benchmarks:<BR>  Image retrieval - CARS196: R-at-1<BR>  Image retrieval - In-Shop: R-at-1<BR>  Image retrieval - Oxf105k: MAP<BR>  Image retrieval - Oxf5k: MAP<BR>  Image retrieval - Par106k: mAP<BR>  Image retrieval - Par6k: mAP<BR>  Image retrieval - SOP: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2018-12-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-10<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-1<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-50<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-10<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-1<BR>  Image retrieval with multi-modal query - MIT-States: Recall@5<BR>","<BR>task: Image retrieval<BR>date: 2019-03-01<BR>ratio: 0.6962<BR>benchmarks:<BR>  Image retrieval - CARS196: R-at-1<BR>  Image retrieval - In-Shop: R-at-1<BR>  Image retrieval - SOP: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2019-05-01<BR>ratio: 0.5316<BR>benchmarks:<BR>  Cross-modal retrieval - Recipe1M: Image-to-text R-at-1<BR>  Cross-modal retrieval - Recipe1M: Text-to-image R@1<BR>","<BR>task: Image retrieval<BR>date: 2019-08-01<BR>ratio: 0.9532<BR>benchmarks:<BR>  Text-image retrieval - COCO (image as query): Recall-at-10<BR>","<BR>task: Image retrieval<BR>date: 2019-09-01<BR>ratio: 0.1854<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2020-03-01<BR>ratio: 0.9624<BR>benchmarks:<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>  Cross-modal retrieval - Recipe1M: Image-to-text R-at-1<BR>  Cross-modal retrieval - Recipe1M: Text-to-image R@1<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-20<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-50<BR>","<BR>task: Image retrieval<BR>date: 2020-04-01<BR>ratio: 0.273<BR>benchmarks:<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-10<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-1<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-5<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@1<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@5<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>  Text-image retrieval - COCO (image as query): Recall-at-10<BR>","<BR>task: Image retrieval<BR>date: 2020-06-01<BR>ratio: 0.85<BR>benchmarks:<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-10<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-1<BR>  Image retrieval with multi-modal query - Fashion200k: Recall-at-50<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-10<BR>  Image retrieval with multi-modal query - MIT-States: Recall-at-1<BR>  Image retrieval with multi-modal query - MIT-States: Recall@5<BR>","<BR>task: Image retrieval<BR>date: 2020-08-01<BR>ratio: 0.0317<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2020-12-01<BR>ratio: 0.4132<BR>benchmarks:<BR>  Cross-modal retrieval - Recipe1M: Image-to-text R-at-1<BR>  Cross-modal retrieval - Recipe1M: Text-to-image R@1<BR>","<BR>task: Image retrieval<BR>date: 2021-01-01<BR>ratio: 0.0317<BR>benchmarks:<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>","<BR>task: Image retrieval<BR>date: 2021-02-01<BR>ratio: 0.5581<BR>benchmarks:<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-10<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-1<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-5<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@1<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@5<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-10<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@5<BR>","<BR>task: Image retrieval<BR>date: 2021-04-01<BR>ratio: 0.09<BR>benchmarks:<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-20<BR>  Image retrieval - DeepFashion - Consumer-to-shop: Rank-50<BR>","<BR>task: Image retrieval<BR>date: 2021-06-01<BR>ratio: 0.0244<BR>benchmarks:<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2021-08-01<BR>ratio: 0.7188<BR>benchmarks:<BR>  Image retrieval - iNaturalist: R-at-16<BR>  Image retrieval - iNaturalist: R-at-1<BR>  Image retrieval - iNaturalist: R-at-32<BR>  Image retrieval - iNaturalist: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2021-10-01<BR>ratio: 0.4286<BR>benchmarks:<BR>  Image retrieval - SOP: R-at-1<BR>  Image retrieval - iNaturalist: R-at-16<BR>  Image retrieval - iNaturalist: R-at-1<BR>  Image retrieval - iNaturalist: R-at-32<BR>  Image retrieval - iNaturalist: R-at-5<BR>","<BR>task: Image retrieval<BR>date: 2021-11-01<BR>ratio: 0.3961<BR>benchmarks:<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-10<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-1<BR>  Cross-modal retrieval - COCO 2014: Image-to-text R-at-5<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@10<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@1<BR>  Cross-modal retrieval - COCO 2014: Text-to-image R@5<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-1<BR>  Cross-modal retrieval - Flickr30k: Image-to-text R-at-5<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@10<BR>  Cross-modal retrieval - Flickr30k: Text-to-image R@1<BR>  Image retrieval - Flickr30K 1K test: R-at-10<BR>  Image retrieval - Flickr30K 1K test: R-at-1<BR>  Image retrieval - Flickr30K 1K test: R-at-5<BR>","<BR>task: Image tagging<BR>date: 2017-06-01<BR>ratio: 0.6104<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>","<BR>task: Image tagging<BR>date: 2018-02-01<BR>ratio: 0.929<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: METEOR<BR>","<BR>task: Image tagging<BR>date: 2018-05-01<BR>ratio: 0.6436<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Image tagging<BR>date: 2019-08-01<BR>ratio: 0.8881<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Image tagging<BR>date: 2019-09-01<BR>ratio: 0.6111<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Image tagging<BR>date: 2019-12-01<BR>ratio: 0.1119<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>","<BR>task: Image tagging<BR>date: 2020-02-01<BR>ratio: 0.8421<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Image tagging<BR>date: 2020-03-01<BR>ratio: 0.1579<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>","<BR>task: Image tagging<BR>date: 2020-04-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Image tagging<BR>date: 2021-01-01<BR>ratio: 0.4667<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Image tagging<BR>date: 2021-02-01<BR>ratio: 0.0479<BR>benchmarks:<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>","<BR>task: Image tagging<BR>date: 2021-04-01<BR>ratio: 0.7906<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Image tagging<BR>date: 2021-08-01<BR>ratio: 0.933<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Image tagging<BR>date: 2021-11-01<BR>ratio: 0.9521<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: SPICE<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Image tagging<BR>date: 2021-12-01<BR>ratio: 0.2366<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Image/document clustering<BR>date: 2021-04-01<BR>ratio: 0.9856<BR>benchmarks:<BR>  Image/document clustering - pendigits: Accuracy (%)<BR>  Image/document clustering - pendigits: NMI<BR>","<BR>task: Image/document clustering<BR>date: 2021-05-01<BR>ratio: 0.0779<BR>benchmarks:<BR>  Image/document clustering - pendigits: Accuracy (%)<BR>  Image/document clustering - pendigits: NMI<BR>","<BR>task: Instance segmentation<BR>date: 2016-11-01<BR>ratio: 0.328<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2017-03-01<BR>ratio: 0.3822<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2017-04-01<BR>ratio: 0.1932<BR>benchmarks:<BR>  Instance segmentation - Cityscapes test: Average Precision<BR>","<BR>task: Instance segmentation<BR>date: 2017-07-01<BR>ratio: 0.3494<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: Precision@0.8<BR>","<BR>task: Instance segmentation<BR>date: 2017-12-01<BR>ratio: 0.0357<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2018-03-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: mask AP<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: IoU mean<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>","<BR>task: Instance segmentation<BR>date: 2018-12-01<BR>ratio: 0.5107<BR>benchmarks:<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  3D semantic instance segmentation - ScanNetV2: mAP-at-0.50<BR>","<BR>task: Instance segmentation<BR>date: 2019-02-01<BR>ratio: 0.2026<BR>benchmarks:<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  Instance segmentation - COCO minival: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2019-04-01<BR>ratio: 0.8223<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Instance segmentation - COCO minival: mask AP<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2019-06-01<BR>ratio: 0.3568<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mPrec<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  3D semantic instance segmentation - ScanNetV2: mAP-at-0.50<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>  Instance segmentation - Cityscapes test: Average Precision<BR>","<BR>task: Instance segmentation<BR>date: 2019-07-01<BR>ratio: 0.6051<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>","<BR>task: Instance segmentation<BR>date: 2019-08-01<BR>ratio: 0.6452<BR>benchmarks:<BR>  Instance segmentation - BDD100K val: AP<BR>  Instance segmentation - COCO minival: AP50<BR>  Instance segmentation - COCO minival: AP75<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>","<BR>task: Instance segmentation<BR>date: 2019-09-01<BR>ratio: 0.0464<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2019-10-01<BR>ratio: 0.2098<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: IoU mean<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2019-11-01<BR>ratio: 0.5459<BR>benchmarks:<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>  Instance segmentation - Cityscapes test: Average Precision<BR>  Real-time instance segmentation - MSCOCO: APL<BR>  Real-time instance segmentation - MSCOCO: APM<BR>  Real-time instance segmentation - MSCOCO: APS<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2019-12-01<BR>ratio: 0.7196<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mPrec<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: mask AP<BR>  Instance segmentation - Cityscapes test: Average Precision<BR>  Real-time instance segmentation - MSCOCO: AP50<BR>  Real-time instance segmentation - MSCOCO: AP75<BR>  Real-time instance segmentation - MSCOCO: APL<BR>  Real-time instance segmentation - MSCOCO: APM<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2020-01-01<BR>ratio: 0.4379<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2020-03-01<BR>ratio: 0.75<BR>benchmarks:<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  3D instance segmentation - ScanNet(v2): mAP<BR>  3D semantic instance segmentation - ScanNetV2: mAP-at-0.50<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Real-time instance segmentation - MSCOCO: AP50<BR>  Real-time instance segmentation - MSCOCO: AP75<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2020-04-01<BR>ratio: 0.2727<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mPrec<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  Instance segmentation - COCO minival: APS<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>","<BR>task: Instance segmentation<BR>date: 2020-06-01<BR>ratio: 0.2746<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mCov<BR>  3D instance segmentation - S3DIS: mWCov<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>  Instance segmentation - COCO test-dev: mask AP<BR>  Referring expression segmentation - RefCOCO testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2020-07-01<BR>ratio: 0.7976<BR>benchmarks:<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.7<BR>  Real-time instance segmentation - MSCOCO: APL<BR>  Real-time instance segmentation - MSCOCO: APM<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>","<BR>task: Instance segmentation<BR>date: 2020-09-01<BR>ratio: 0.64<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - COCO test-dev: AP@50<BR>","<BR>task: Instance segmentation<BR>date: 2020-10-01<BR>ratio: 0.7407<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F 1st frame<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F Full video<BR>  Referring expression segmentation - RefCOCO testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2020-11-01<BR>ratio: 0.1158<BR>benchmarks:<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>","<BR>task: Instance segmentation<BR>date: 2020-12-01<BR>ratio: 0.3899<BR>benchmarks:<BR>  Instance segmentation - COCO minival: AP50<BR>  Instance segmentation - COCO minival: AP75<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2021-02-01<BR>ratio: 0.2903<BR>benchmarks:<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: Precision@0.8<BR>","<BR>task: Instance segmentation<BR>date: 2021-03-01<BR>ratio: 0.9182<BR>benchmarks:<BR>  Instance segmentation - COCO minival: AP75<BR>  Instance segmentation - COCO minival: APS<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: mask AP<BR>  Referring expression segmentation - A2D Sentences: IoU mean<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - A2D Sentences: Precision@0.8<BR>  Referring expression segmentation - A2D Sentences: Precision@0.9<BR>  Referring expression segmentation - J-HMDB: IoU mean<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: Precision@0.8<BR>  Referring expression segmentation - J-HMDB: Precision@0.9<BR>","<BR>task: Instance segmentation<BR>date: 2021-04-01<BR>ratio: 0.444<BR>benchmarks:<BR>  Instance segmentation - COCO minival: APL<BR>  Referring expression segmentation - RefCOCO testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2021-05-01<BR>ratio: 0.7615<BR>benchmarks:<BR>  Instance segmentation - COCO minival: AP50<BR>  Instance segmentation - COCO minival: AP75<BR>  Instance segmentation - COCO minival: APL<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - J-HMDB: AP<BR>","<BR>task: Instance segmentation<BR>date: 2021-06-01<BR>ratio: 0.1721<BR>benchmarks:<BR>  Instance segmentation - COCO minival: mask AP<BR>  Instance segmentation - COCO test-dev: mask AP<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F 1st frame<BR>","<BR>task: Instance segmentation<BR>date: 2021-07-01<BR>ratio: 0.1579<BR>benchmarks:<BR>  Instance segmentation - COCO test-dev: AP50<BR>  Instance segmentation - COCO test-dev: AP75<BR>  Instance segmentation - COCO test-dev: APL<BR>  Instance segmentation - COCO test-dev: APM<BR>  Instance segmentation - COCO test-dev: APS<BR>","<BR>task: Instance segmentation<BR>date: 2021-08-01<BR>ratio: 0.9758<BR>benchmarks:<BR>  3D instance segmentation - S3DIS: mCov<BR>  3D instance segmentation - S3DIS: mPrec<BR>  3D instance segmentation - S3DIS: mWCov<BR>  3D instance segmentation - ScanNet(v2): mAP @ 50<BR>  3D instance segmentation - ScanNet(v2): mAP<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2021-09-01<BR>ratio: 0.36<BR>benchmarks:<BR>  Image-level Supervised Instance Segmentation - COCO test-dev: AP@50<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.25<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Image-level Supervised Instance Segmentation - PASCAL VOC 2012 val: mAP@0.7<BR>","<BR>task: Instance segmentation<BR>date: 2021-10-01<BR>ratio: 0.5843<BR>benchmarks:<BR>  Real-time instance segmentation - MSCOCO: AP50<BR>  Real-time instance segmentation - MSCOCO: AP75<BR>  Real-time instance segmentation - MSCOCO: APM<BR>  Real-time instance segmentation - MSCOCO: APS<BR>  Real-time instance segmentation - MSCOCO: mask AP<BR>","<BR>task: Instance segmentation<BR>date: 2021-11-01<BR>ratio: 0.9975<BR>benchmarks:<BR>  Instance segmentation - BDD100K val: AP<BR>  Referring expression segmentation - A2D Sentences: AP<BR>  Referring expression segmentation - A2D Sentences: IoU overall<BR>  Referring expression segmentation - A2D Sentences: Precision@0.5<BR>  Referring expression segmentation - A2D Sentences: Precision@0.6<BR>  Referring expression segmentation - A2D Sentences: Precision@0.7<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F 1st frame<BR>  Referring expression segmentation - DAVIS 2017 (val): J&F Full video<BR>  Referring expression segmentation - J-HMDB: AP<BR>  Referring expression segmentation - J-HMDB: IoU mean<BR>  Referring expression segmentation - J-HMDB: IoU overall<BR>  Referring expression segmentation - J-HMDB: Precision@0.5<BR>  Referring expression segmentation - J-HMDB: Precision@0.6<BR>  Referring expression segmentation - J-HMDB: Precision@0.7<BR>  Referring expression segmentation - J-HMDB: Precision@0.8<BR>  Referring expression segmentation - J-HMDB: Precision@0.9<BR>  Referring expression segmentation - RefCOCO testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO testB: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ test B: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>  Referring expression segmentation - RefCOCO+ val: Overall IoU<BR>  Referring expression segmentation - RefCoCo val: Overall IoU<BR>","<BR>task: Instance segmentation<BR>date: 2021-12-01<BR>ratio: 0.3302<BR>benchmarks:<BR>  Instance segmentation - COCO minival: APL<BR>  Instance segmentation - COCO test-dev: APL<BR>  Referring expression segmentation - RefCOCO+ testA: Overall IoU<BR>","<BR>task: Intelligent surveillance<BR>date: 2020-04-01<BR>ratio: 0.844<BR>benchmarks:<BR>  Vehicle re-identification - VeRi-776: mAP<BR>","<BR>task: Intelligent surveillance<BR>date: 2021-02-01<BR>ratio: 0.6667<BR>benchmarks:<BR>  Vehicle re-identification - VehicleID Small: Rank-5<BR>","<BR>task: Intelligent surveillance<BR>date: 2021-04-01<BR>ratio: 0.1443<BR>benchmarks:<BR>  Vehicle re-identification - VeRi-776: mAP<BR>","<BR>task: Intelligent surveillance<BR>date: 2021-08-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Vehicle re-identification - VehicleID Small: Rank-5<BR>","<BR>task: Intelligent surveillance<BR>date: 2021-10-01<BR>ratio: 0.0117<BR>benchmarks:<BR>  Vehicle re-identification - VeRi-776: mAP<BR>","<BR>task: Interest point detection<BR>date: 2017-09-01<BR>ratio: 0.9167<BR>benchmarks:<BR>  Homography estimation - S-COCO: MACE<BR>","<BR>task: Interest point detection<BR>date: 2019-09-01<BR>ratio: 0.0833<BR>benchmarks:<BR>  Homography estimation - S-COCO: MACE<BR>","<BR>task: Keyword spotting<BR>date: 2018-08-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>","<BR>task: Keyword spotting<BR>date: 2019-01-01<BR>ratio: 0.6364<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 20<BR>","<BR>task: Keyword spotting<BR>date: 2020-01-01<BR>ratio: 0.4444<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>","<BR>task: Keyword spotting<BR>date: 2020-04-01<BR>ratio: 0.5222<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>","<BR>task: Keyword spotting<BR>date: 2020-05-01<BR>ratio: 0.1667<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>","<BR>task: Keyword spotting<BR>date: 2020-08-01<BR>ratio: 0.0889<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>","<BR>task: Keyword spotting<BR>date: 2021-03-01<BR>ratio: 0.0278<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>","<BR>task: Keyword spotting<BR>date: 2021-05-01<BR>ratio: 0.3636<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 20<BR>","<BR>task: Keyword spotting<BR>date: 2021-06-01<BR>ratio: 0.1111<BR>benchmarks:<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V1 12<BR>  Keyword spotting - Google Speech Commands: Google Speech Commands V2 12<BR>","<BR>task: Line segment detection<BR>date: 2018-12-01<BR>ratio: 0.0093<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2019-05-01<BR>ratio: 0.9048<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP5<BR>  Line segment detection - wireframe dataset: sAP10<BR>  Line segment detection - wireframe dataset: sAP15<BR>  Line segment detection - wireframe dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2020-03-01<BR>ratio: 0.8986<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: F1 score<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP15<BR>  Line segment detection - York Urban Dataset: sAP5<BR>  Line segment detection - wireframe dataset: sAP10<BR>  Line segment detection - wireframe dataset: sAP15<BR>  Line segment detection - wireframe dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2020-07-01<BR>ratio: 0.0071<BR>benchmarks:<BR>  Line segment detection - wireframe dataset: sAP10<BR>  Line segment detection - wireframe dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2020-09-01<BR>ratio: 0.0704<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: sAP5<BR>","<BR>task: Line segment detection<BR>date: 2021-01-01<BR>ratio: 0.2609<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: F1 score<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP15<BR>","<BR>task: Line segment detection<BR>date: 2021-04-01<BR>ratio: 0.0648<BR>benchmarks:<BR>  Line segment detection - York Urban Dataset: sAP10<BR>  Line segment detection - York Urban Dataset: sAP15<BR>  Line segment detection - York Urban Dataset: sAP5<BR>  Line segment detection - wireframe dataset: sAP10<BR>  Line segment detection - wireframe dataset: sAP15<BR>  Line segment detection - wireframe dataset: sAP5<BR>","<BR>task: Material property prediction<BR>date: 2017-04-01<BR>ratio: 0.2032<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2017-06-01<BR>ratio: 0.4063<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2017-09-01<BR>ratio: 0.1219<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2017-12-01<BR>ratio: 0.2367<BR>benchmarks:<BR>  Formation energy prediction - Materials Project: MAE<BR>","<BR>task: Material property prediction<BR>date: 2018-06-01<BR>ratio: 0.7278<BR>benchmarks:<BR>  Formation energy prediction - Materials Project: MAE<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2018-12-01<BR>ratio: 0.0722<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2019-02-01<BR>ratio: 0.158<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2020-09-01<BR>ratio: 0.0045<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2020-11-01<BR>ratio: 0.0023<BR>benchmarks:<BR>  Formation energy prediction - QM9: MAE<BR>","<BR>task: Material property prediction<BR>date: 2021-06-01<BR>ratio: 0.0355<BR>benchmarks:<BR>  Formation energy prediction - Materials Project: MAE<BR>","<BR>task: Medical diagnosis<BR>date: 2017-07-01<BR>ratio: 0.874<BR>benchmarks:<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Medical diagnosis<BR>date: 2019-10-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Retinal OCT disease classification - OCT2017: Acc<BR>  Retinal OCT disease classification - OCT2017: Sensitivity<BR>  Retinal OCT disease classification - Srinivasan2014: Acc<BR>","<BR>task: Meta-learning<BR>date: 2014-09-01<BR>ratio: 0.8273<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2016-03-01<BR>ratio: 0.1186<BR>benchmarks:<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2016-05-01<BR>ratio: 0.1778<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>  Few-shot image classification - CUB-200-2011 - 0-Shot: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-03-01<BR>ratio: 0.8222<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 50-way (0-shot): Accuracy<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-06-01<BR>ratio: 0.267<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2017-11-01<BR>ratio: 0.5012<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-05-01<BR>ratio: 0.234<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-06-01<BR>ratio: 0.4298<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-10-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-11-01<BR>ratio: 0.6674<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2018-12-01<BR>ratio: 0.5693<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-01-01<BR>ratio: 0.9015<BR>benchmarks:<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>","<BR>task: Meta-learning<BR>date: 2019-02-01<BR>ratio: 0.2783<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 5-way: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 5-Shot, 20-way: Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-03-01<BR>ratio: 0.4292<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2019-04-01<BR>ratio: 0.7432<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-05-01<BR>ratio: 0.0836<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-06-01<BR>ratio: 0.7342<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-07-01<BR>ratio: 0.2956<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-08-01<BR>ratio: 0.2676<BR>benchmarks:<BR>  Few-shot image classification - OMNIGLOT - 1-Shot, 20-way: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-09-01<BR>ratio: 0.1143<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2019-10-01<BR>ratio: 0.95<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-11-01<BR>ratio: 0.9242<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2019-12-01<BR>ratio: 0.8565<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2020-01-01<BR>ratio: 0.1703<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-02-01<BR>ratio: 0.3689<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (1-shot): Accuracy<BR>  Few-shot image classification - FC100 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-03-01<BR>ratio: 0.366<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 1-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2020-04-01<BR>ratio: 0.7<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - CUB-LT: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - ImageNet-LT-d: Per-Class Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Long-Tailed Accuracy<BR>  Long-tail learning with class descriptors - SUN-LT: Per-Class Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-06-01<BR>ratio: 0.7127<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Dirichlet Mini-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Dirichlet Tiered-Imagenet (5-way, 5-shot): 1:1 Accuracy<BR>  Few-shot image classification - Meta-Dataset Rank: Mean Rank<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Few-shot image classification - Mini-ImageNet - 1-Shot Learning: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (10-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 10-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-07-01<BR>ratio: 0.6116<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2020-08-01<BR>ratio: 0.3986<BR>benchmarks:<BR>  Few-shot image classification - Mini-Imagenet 20-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 20-way (5-shot): Accuracy<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>","<BR>task: Meta-learning<BR>date: 2020-09-01<BR>ratio: 0.012<BR>benchmarks:<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-10-01<BR>ratio: 0.5799<BR>benchmarks:<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - EGTEA: Average Precision<BR>  Long-tail learning - EGTEA: Average Recall<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-11-01<BR>ratio: 0.7432<BR>benchmarks:<BR>  Few-shot image classification - Stanford Cars 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Cars 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2020-12-01<BR>ratio: 0.7059<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>","<BR>task: Meta-learning<BR>date: 2021-01-01<BR>ratio: 0.1191<BR>benchmarks:<BR>  Few-shot image classification - Meta-Dataset: Accuracy<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-02-01<BR>ratio: 0.9984<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - ImageNet - 0-Shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-03-01<BR>ratio: 0.268<BR>benchmarks:<BR>  Few-shot image classification - Stanford Dogs 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Stanford Dogs 5-way (5-shot): Accuracy<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-04-01<BR>ratio: 0.5568<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-05-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Few-shot image classification - CUB 200 5-way 1-shot: Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-07-01<BR>ratio: 0.303<BR>benchmarks:<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-10-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=10): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=100): Error Rate<BR>  Long-tail learning - CIFAR-100-LT (\u03c1=50): Error Rate<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-10-01<BR>ratio: 0.1543<BR>benchmarks:<BR>  Few-shot image classification - CIFAR-FS 5-way (1-shot): Accuracy<BR>  Few-shot image classification - CIFAR-FS 5-way (5-shot): Accuracy<BR>  Few-shot image classification - CUB 200 5-way 5-shot: Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-ImageNet-CUB 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Mini-Imagenet 5-way (5-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (1-shot): Accuracy<BR>  Few-shot image classification - Tiered ImageNet 5-way (5-shot): Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-11-01<BR>ratio: 0.55<BR>benchmarks:<BR>  Long-tail learning - ImageNet-LT: Top-1 Accuracy<BR>  Long-tail learning - Places-LT: Top-1 Accuracy<BR>  Long-tail learning - iNaturalist 2018: Top-1 Accuracy<BR>","<BR>task: Meta-learning<BR>date: 2021-12-01<BR>ratio: 0.0613<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2019-07-01<BR>ratio: 0.5126<BR>benchmarks:<BR>  Multi-target domain adaptation - Office-31: Accuracy<BR>  Multi-target domain adaptation - Office-Home: Accuracy<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2019-12-01<BR>ratio: 0.5659<BR>benchmarks:<BR>  Multi-target domain adaptation - DomainNet: Accuracy<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2020-07-01<BR>ratio: 0.3247<BR>benchmarks:<BR>  Multi-target domain adaptation - Office-31: Accuracy<BR>","<BR>task: Multi-target domain adaptation<BR>date: 2021-04-01<BR>ratio: 0.4874<BR>benchmarks:<BR>  Multi-target domain adaptation - DomainNet: Accuracy<BR>  Multi-target domain adaptation - Office-31: Accuracy<BR>  Multi-target domain adaptation - Office-Home: Accuracy<BR>","<BR>task: Multimodal machine translation<BR>date: 2018-11-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: BLEU (EN-DE)<BR>  Multimodal machine translation - Multi30K: Meteor (EN-DE)<BR>","<BR>task: Multimodal machine translation<BR>date: 2019-06-01<BR>ratio: 0.7049<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: BLEU (EN-DE)<BR>  Multimodal machine translation - Multi30K: Meteor (EN-FR)<BR>","<BR>task: Multimodal machine translation<BR>date: 2019-11-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: BLEU (EN-DE)<BR>  Multimodal machine translation - Multi30K: Meteor (EN-DE)<BR>","<BR>task: Multimodal machine translation<BR>date: 2020-09-01<BR>ratio: 0.2951<BR>benchmarks:<BR>  Multimodal machine translation - Multi30K: BLEU (EN-DE)<BR>  Multimodal machine translation - Multi30K: Meteor (EN-FR)<BR>","<BR>task: Multispectral Object Detection<BR>date: 2021-01-01<BR>ratio: 0.0794<BR>benchmarks:<BR>  Multispectral Object Detection - FLIR-aligned: mAP50<BR>","<BR>task: Multispectral Object Detection<BR>date: 2021-10-01<BR>ratio: 0.9206<BR>benchmarks:<BR>  Multispectral Object Detection - FLIR-aligned: mAP50<BR>","<BR>task: Object counting<BR>date: 2016-09-01<BR>ratio: 0.4767<BR>benchmarks:<BR>  Object counting - CARPK: MAE<BR>  Object counting - CARPK: RMSE<BR>","<BR>task: Object counting<BR>date: 2017-07-01<BR>ratio: 0.3232<BR>benchmarks:<BR>  Object counting - CARPK: MAE<BR>  Object counting - CARPK: RMSE<BR>","<BR>task: Object counting<BR>date: 2019-04-01<BR>ratio: 0.3086<BR>benchmarks:<BR>  Object counting - CARPK: MAE<BR>  Object counting - CARPK: RMSE<BR>","<BR>task: Object counting<BR>date: 2021-07-01<BR>ratio: 0.1232<BR>benchmarks:<BR>  Object counting - CARPK: MAE<BR>  Object counting - CARPK: RMSE<BR>","<BR>task: Object detection<BR>date: 2014-06-01<BR>ratio: 0.0779<BR>benchmarks:<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2015-04-01<BR>ratio: 0.2955<BR>benchmarks:<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2015-06-01<BR>ratio: 0.2457<BR>benchmarks:<BR>  Object detection - PASCAL VOC 2007: MAP<BR>  Object detection - PASCAL VOC 2012: MAP<BR>","<BR>task: Object detection<BR>date: 2015-12-01<BR>ratio: 0.7543<BR>benchmarks:<BR>  Object detection - COCO test-dev: box AP<BR>  Object detection - PASCAL VOC 2007: MAP<BR>  Object detection - PASCAL VOC 2012: MAP<BR>","<BR>task: Object detection<BR>date: 2016-05-01<BR>ratio: 0.3852<BR>benchmarks:<BR>  Object detection - UA-DETRAC: mAP<BR>","<BR>task: Object detection<BR>date: 2016-09-01<BR>ratio: 0.783<BR>benchmarks:<BR>  Object detection - KITTI Cars Easy: AP<BR>  Object detection - KITTI Cars Hard: AP<BR>  Object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection<BR>date: 2016-12-01<BR>ratio: 0.0438<BR>benchmarks:<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2017-03-01<BR>ratio: 0.2773<BR>benchmarks:<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2017-08-01<BR>ratio: 0.0649<BR>benchmarks:<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2017-11-01<BR>ratio: 0.2518<BR>benchmarks:<BR>  Object detection - COCO minival: AP50<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2017-12-01<BR>ratio: 0.2222<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>","<BR>task: Object detection<BR>date: 2018-03-01<BR>ratio: 0.4562<BR>benchmarks:<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>  Object detection - iSAID: Average Precision<BR>","<BR>task: Object detection<BR>date: 2018-11-01<BR>ratio: 0.2514<BR>benchmarks:<BR>  Object detection - COCO minival: AP75<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - KITTI Cars Easy: AP<BR>","<BR>task: Object detection<BR>date: 2018-12-01<BR>ratio: 0.2536<BR>benchmarks:<BR>  Object detection - KITTI Cars Easy: AP<BR>  Object detection - KITTI Cars Hard: AP<BR>  Object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection<BR>date: 2019-01-01<BR>ratio: 0.0805<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2019-03-01<BR>ratio: 0.0091<BR>benchmarks:<BR>  Object detection - COCO minival: APM<BR>","<BR>task: Object detection<BR>date: 2019-04-01<BR>ratio: 0.459<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>  Object detection - UA-DETRAC: mAP<BR>","<BR>task: Object detection<BR>date: 2019-05-01<BR>ratio: 0.5438<BR>benchmarks:<BR>  Object detection - iSAID: Average Precision<BR>","<BR>task: Object detection<BR>date: 2019-06-01<BR>ratio: 0.1511<BR>benchmarks:<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2019-08-01<BR>ratio: 0.2115<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APS<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2019-10-01<BR>ratio: 0.0701<BR>benchmarks:<BR>  Object detection - KITTI Cars Easy: AP<BR>  Object detection - KITTI Cars Hard: AP<BR>  Object detection - KITTI Cars Moderate: AP<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2019-11-01<BR>ratio: 0.4746<BR>benchmarks:<BR>  Object detection - COCO minival: AP50<BR>  Object detection - COCO minival: AP75<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>","<BR>task: Object detection<BR>date: 2019-12-01<BR>ratio: 0.2222<BR>benchmarks:<BR>  Object detection - COCO minival: box AP<BR>","<BR>task: Object detection<BR>date: 2020-02-01<BR>ratio: 0.9505<BR>benchmarks:<BR>  Object detection - UA-DETRAC: mAP<BR>  Object detection - UAVDT: mAP<BR>","<BR>task: Object detection<BR>date: 2020-04-01<BR>ratio: 0.0167<BR>benchmarks:<BR>  Object detection - COCO minival: box AP<BR>","<BR>task: Object detection<BR>date: 2020-06-01<BR>ratio: 0.6829<BR>benchmarks:<BR>  Object detection - AI-TOD: AP50<BR>  Object detection - AI-TOD: AP75<BR>  Object detection - AI-TOD: AP<BR>  Object detection - AI-TOD: APm<BR>  Object detection - AI-TOD: APs<BR>  Object detection - AI-TOD: APt<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2020-11-01<BR>ratio: 0.0168<BR>benchmarks:<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2020-12-01<BR>ratio: 0.2045<BR>benchmarks:<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: box AP<BR>  Object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection<BR>date: 2021-01-01<BR>ratio: 0.4615<BR>benchmarks:<BR>  Object detection - AI-TOD: AP50<BR>  Object detection - AI-TOD: APt<BR>","<BR>task: Object detection<BR>date: 2021-03-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2021-05-01<BR>ratio: 0.1727<BR>benchmarks:<BR>  Object detection - COCO minival: AP50<BR>  Object detection - COCO minival: AP75<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>","<BR>task: Object detection<BR>date: 2021-06-01<BR>ratio: 0.2911<BR>benchmarks:<BR>  Object detection - COCO minival: AP50<BR>  Object detection - COCO minival: AP75<BR>  Object detection - COCO minival: APL<BR>  Object detection - COCO minival: APM<BR>  Object detection - COCO minival: APS<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2021-09-01<BR>ratio: 0.0495<BR>benchmarks:<BR>  Object detection - UA-DETRAC: mAP<BR>  Object detection - UAVDT: mAP<BR>","<BR>task: Object detection<BR>date: 2021-10-01<BR>ratio: 0.8803<BR>benchmarks:<BR>  Object detection - AI-TOD: AP50<BR>  Object detection - AI-TOD: AP75<BR>  Object detection - AI-TOD: AP<BR>  Object detection - AI-TOD: APm<BR>  Object detection - AI-TOD: APs<BR>  Object detection - AI-TOD: APt<BR>","<BR>task: Object detection<BR>date: 2021-11-01<BR>ratio: 0.1111<BR>benchmarks:<BR>  Object detection - COCO minival: box AP<BR>  Object detection - COCO test-dev: box AP<BR>","<BR>task: Object detection<BR>date: 2021-12-01<BR>ratio: 0.1169<BR>benchmarks:<BR>  Object detection - COCO test-dev: AP50<BR>  Object detection - COCO test-dev: AP75<BR>  Object detection - COCO test-dev: APL<BR>  Object detection - COCO test-dev: APM<BR>  Object detection - COCO test-dev: APS<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2017-06-01<BR>ratio: 0.6104<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2018-02-01<BR>ratio: 0.929<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: METEOR<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2018-05-01<BR>ratio: 0.6436<BR>benchmarks:<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2019-08-01<BR>ratio: 0.8881<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2019-09-01<BR>ratio: 0.6111<BR>benchmarks:<BR>  Image captioning - Flickr30k Captions test: BLEU-4<BR>  Image captioning - Flickr30k Captions test: CIDEr<BR>  Image captioning - Flickr30k Captions test: METEOR<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2019-12-01<BR>ratio: 0.1119<BR>benchmarks:<BR>  Image captioning - COCO: CIDEr<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2020-02-01<BR>ratio: 0.8421<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2020-03-01<BR>ratio: 0.1579<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDEr-D<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2020-04-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-01-01<BR>ratio: 0.4667<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-02-01<BR>ratio: 0.0479<BR>benchmarks:<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-04-01<BR>ratio: 0.7906<BR>benchmarks:<BR>  Open Vocabulary Object Detection - MSCOCO: AP 0.5<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-08-01<BR>ratio: 0.933<BR>benchmarks:<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: METEOR<BR>  Image captioning - COCO Captions: SPICE<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-11-01<BR>ratio: 0.9521<BR>benchmarks:<BR>  Image captioning - COCO Captions: BLEU-4<BR>  Image captioning - COCO Captions: CIDER<BR>  Image captioning - COCO Captions: SPICE<BR>  Image captioning - nocaps-val-in-domain: CIDEr<BR>  Image captioning - nocaps-val-in-domain: Pre-train (#images)<BR>  Image captioning - nocaps-val-near-domain: CIDEr<BR>  Image captioning - nocaps-val-overall: CIDEr<BR>","<BR>task: Object detection // 2D object detection<BR>date: 2021-12-01<BR>ratio: 0.7091<BR>benchmarks:<BR>  Open Vocabulary Object Detection - MSCOCO: AP 0.5<BR>  Phrase grounding - Flickr30k Entities Test: R-at-10<BR>  Phrase grounding - Flickr30k Entities Test: R-at-1<BR>  Phrase grounding - Flickr30k Entities Test: R-at-5<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2016-06-01<BR>ratio: 0.2489<BR>benchmarks:<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2017-11-01<BR>ratio: 0.4954<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy val: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2017-12-01<BR>ratio: 0.2507<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Pedestrians Hard: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-02-01<BR>ratio: 0.2323<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-10-01<BR>ratio: 0.2776<BR>benchmarks:<BR>  Monocular 3D object detection - SUN RGB-D: AP@0.15<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2018-12-01<BR>ratio: 0.3839<BR>benchmarks:<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclists Easy: AP<BR>  3D object detection - KITTI Cyclists Hard: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>  3D object detection - KITTI Pedestrians Hard: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-03-01<BR>ratio: 0.3144<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclists Easy: AP<BR>  3D object detection - KITTI Cyclists Hard: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-04-01<BR>ratio: 0.3622<BR>benchmarks:<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>  3D object detection - ScanNetV2: mAP-at-0.5<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-06-01<BR>ratio: 0.2738<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-07-01<BR>ratio: 0.4776<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy val: AP<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclist Easy val: AP<BR>  3D object detection - KITTI Cyclist Hard val: AP<BR>  3D object detection - KITTI Cyclist Moderate val: AP<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-09-01<BR>ratio: 0.8205<BR>benchmarks:<BR>  3D object detection - OPV2V: AP@0.7@CulverCity<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2019-12-01<BR>ratio: 0.4446<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclists Hard: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>  Monocular 3D object detection - SUN RGB-D: AP@0.15<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-01-01<BR>ratio: 0.3185<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>  3D Object Detection From Stereo Images - KITTI Cyclists Moderate: AP50<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-03-01<BR>ratio: 0.6062<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>  3D Object Detection From Stereo Images - KITTI Pedestrians Moderate: AP50<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>  3D object detection - ScanNetV2: mAP-at-0.5<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-04-01<BR>ratio: 0.308<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cyclists Moderate: AP50<BR>  3D Object Detection From Stereo Images - KITTI Pedestrians Moderate: AP50<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-06-01<BR>ratio: 0.5835<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy val: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>  3D object detection - KITTI Pedestrians Hard: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>  3D object detection - SUN-RGBD val: mAP-at-0.5<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>  3D object detection - waymo cyclist: APH/L2<BR>  3D object detection - waymo pedestrian: APH/L2<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-07-01<BR>ratio: 0.0933<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-08-01<BR>ratio: 0.9157<BR>benchmarks:<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>  3D object detection - OPV2V: AP@0.7@CulverCity<BR>  3D object detection - V2XSet: AP0.5 (Noisy)<BR>  3D object detection - V2XSet: AP0.7 (Noisy)<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2020-12-01<BR>ratio: 0.2012<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy val: AP<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate val: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-01-01<BR>ratio: 0.3364<BR>benchmarks:<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Hard: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclists Hard: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-02-01<BR>ratio: 0.4524<BR>benchmarks:<BR>  Monocular 3D object detection - KITTI Cars Hard: AP Hard<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-03-01<BR>ratio: 0.6006<BR>benchmarks:<BR>  Monocular 3D object detection - KITTI Cars Hard: AP Hard<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>  Monocular 3D object detection - KITTI Pedestrian Hard: AP Hard<BR>  Monocular 3D object detection - SUN RGB-D: AP@0.15<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-04-01<BR>ratio: 0.8076<BR>benchmarks:<BR>  3D object detection - KITTI Cars Easy: AP<BR>  3D object detection - KITTI Cars Hard val: AP<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - KITTI Cyclist Easy val: AP<BR>  3D object detection - KITTI Cyclist Hard val: AP<BR>  3D object detection - KITTI Cyclist Moderate val: AP<BR>  3D object detection - KITTI Cyclists Easy: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>  3D object detection - SUN-RGBD val: mAP-at-0.5<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>  3D object detection - ScanNetV2: mAP-at-0.5<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-06-01<BR>ratio: 0.1016<BR>benchmarks:<BR>  Monocular 3D object detection - SUN RGB-D: AP@0.15<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-08-01<BR>ratio: 0.6159<BR>benchmarks:<BR>  3D Object Detection From Stereo Images - KITTI Cars Moderate: AP75<BR>  3D Object Detection From Stereo Images - KITTI Cyclists Moderate: AP50<BR>  3D Object Detection From Stereo Images - KITTI Pedestrians Moderate: AP50<BR>  3D object detection - KITTI Cars Hard: AP<BR>  Monocular 3D object detection - KITTI Cars Hard: AP Hard<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>  Monocular 3D object detection - KITTI Pedestrian Hard: AP Hard<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-09-01<BR>ratio: 0.0256<BR>benchmarks:<BR>  3D object detection - OPV2V: AP@0.7@CulverCity<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-10-01<BR>ratio: 0.6578<BR>benchmarks:<BR>  3D object detection - KITTI Cyclists Easy: AP<BR>  3D object detection - KITTI Cyclists Moderate: AP<BR>  3D object detection - KITTI Pedestrians Moderate: AP<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-11-01<BR>ratio: 0.6667<BR>benchmarks:<BR>  3D object detection - V2XSet: AP0.5 (Noisy)<BR>  3D object detection - V2XSet: AP0.7 (Noisy)<BR>","<BR>task: Object detection // 3D object detection<BR>date: 2021-12-01<BR>ratio: 0.8812<BR>benchmarks:<BR>  3D object detection - KITTI Cars Moderate: AP<BR>  3D object detection - SUN-RGBD val: mAP-at-0.25<BR>  3D object detection - SUN-RGBD val: mAP-at-0.5<BR>  3D object detection - ScanNetV2: mAP-at-0.25<BR>  3D object detection - ScanNetV2: mAP-at-0.5<BR>  3D object detection - waymo cyclist: APH/L2<BR>  3D object detection - waymo pedestrian: APH/L2<BR>  Monocular 3D object detection - KITTI Cars Moderate: AP Medium<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2017-12-01<BR>ratio: 0.7094<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Pedestrians Moderate: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2018-12-01<BR>ratio: 0.287<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Hard: AP<BR>  Birds eye view object detection - KITTI Cars Moderate: AP<BR>  Birds eye view object detection - KITTI Cyclists Moderate: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2019-07-01<BR>ratio: 0.7432<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Easy: AP<BR>  Birds eye view object detection - KITTI Cars Hard: AP<BR>  Birds eye view object detection - KITTI Cars Moderate: AP<BR>  Birds eye view object detection - KITTI Cyclists Moderate: AP<BR>  Birds eye view object detection - KITTI Pedestrians Moderate: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2019-10-01<BR>ratio: 0.0168<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Easy: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2019-12-01<BR>ratio: 0.7273<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Easy: AP<BR>  Birds eye view object detection - KITTI Cars Moderate: AP<BR>  Birds eye view object detection - KITTI Cyclists Moderate: AP<BR>","<BR>task: Object detection // Birds eye view object detection<BR>date: 2021-04-01<BR>ratio: 0.1478<BR>benchmarks:<BR>  Birds eye view object detection - KITTI Cars Easy: AP<BR>  Birds eye view object detection - KITTI Cars Moderate: AP<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2019-10-01<BR>ratio: 0.618<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2020-06-01<BR>ratio: 0.3529<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2020-07-01<BR>ratio: 0.191<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>","<BR>task: Object detection // Camouflaged object segmentation<BR>date: 2021-02-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object detection // Dense object detection<BR>date: 2019-04-01<BR>ratio: 0.2803<BR>benchmarks:<BR>  Dense object detection - SKU-110K: AP<BR>","<BR>task: Object detection // Dense object detection<BR>date: 2019-11-01<BR>ratio: 0.4924<BR>benchmarks:<BR>  Dense object detection - SKU-110K: AP<BR>","<BR>task: Object detection // Dense object detection<BR>date: 2020-07-01<BR>ratio: 0.2273<BR>benchmarks:<BR>  Dense object detection - SKU-110K: AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2015-11-01<BR>ratio: 0.1725<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-03-01<BR>ratio: 0.8233<BR>benchmarks:<BR>  Face detection - Annotated Faces in the Wild: AP<BR>  Face detection - FDDB: AP<BR>  Face detection - PASCAL Face: AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-04-01<BR>ratio: 0.4985<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-06-01<BR>ratio: 0.1463<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-07-01<BR>ratio: 0.2604<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2016-12-01<BR>ratio: 0.0921<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2017-08-01<BR>ratio: 0.6457<BR>benchmarks:<BR>  Face detection - Annotated Faces in the Wild: AP<BR>  Face detection - FDDB: AP<BR>  Face detection - PASCAL Face: AP<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2017-09-01<BR>ratio: 0.807<BR>benchmarks:<BR>  Face detection - FDDB: AP<BR>  Face detection - WIDER Face (Easy): AP<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2018-02-01<BR>ratio: 0.0614<BR>benchmarks:<BR>  Face detection - WIDER Face (Easy): AP<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2018-03-01<BR>ratio: 0.019<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2018-09-01<BR>ratio: 0.0789<BR>benchmarks:<BR>  Face detection - Annotated Faces in the Wild: AP<BR>  Face detection - PASCAL Face: AP<BR>  Face detection - WIDER Face (Easy): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2018-10-01<BR>ratio: 0.0135<BR>benchmarks:<BR>  Face detection - FDDB: AP<BR>  Face detection - WIDER Face (Easy): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2019-05-01<BR>ratio: 0.0439<BR>benchmarks:<BR>  Face detection - WIDER Face (Easy): AP<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2020-11-01<BR>ratio: 0.0157<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>","<BR>task: Object detection // Face detection<BR>date: 2021-07-01<BR>ratio: 0.0056<BR>benchmarks:<BR>  Face detection - WIDER Face (Hard): AP<BR>  Face detection - WIDER Face (Medium): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2018-12-01<BR>ratio: 0.1558<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (10-shot): AP<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2019-08-01<BR>ratio: 0.3571<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (10-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2019-09-01<BR>ratio: 0.2037<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2020-03-01<BR>ratio: 0.0802<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2020-07-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (10-shot): AP<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2021-02-01<BR>ratio: 0.3961<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (10-shot): AP<BR>","<BR>task: Object detection // Few-shot object detection<BR>date: 2021-03-01<BR>ratio: 0.5062<BR>benchmarks:<BR>  Few-shot object detection - MS-COCO (30-shot): AP<BR>","<BR>task: Object detection // Medical object detection<BR>date: 2019-06-01<BR>ratio: 0.6037<BR>benchmarks:<BR>  Medical object detection - DeepLesion: Sensitivity<BR>","<BR>task: Object detection // Medical object detection<BR>date: 2019-08-01<BR>ratio: 0.2535<BR>benchmarks:<BR>  Medical object detection - DeepLesion: Sensitivity<BR>","<BR>task: Object detection // Medical object detection<BR>date: 2020-05-01<BR>ratio: 0.1223<BR>benchmarks:<BR>  Medical object detection - DeepLesion: Sensitivity<BR>","<BR>task: Object detection // Medical object detection<BR>date: 2021-03-01<BR>ratio: 0.0204<BR>benchmarks:<BR>  Medical object detection - DeepLesion: Sensitivity<BR>","<BR>task: Object detection // Object Detection In Indoor Scenes<BR>date: 2017-10-01<BR>ratio: 0.4365<BR>benchmarks:<BR>  Object Detection In Indoor Scenes - SUN RGB-D: AP 0.5<BR>","<BR>task: Object detection // Object Detection In Indoor Scenes<BR>date: 2017-11-01<BR>ratio: 0.5635<BR>benchmarks:<BR>  Object Detection In Indoor Scenes - SUN RGB-D: AP 0.5<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2018-07-01<BR>ratio: 0.5451<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2018-11-01<BR>ratio: 0.1593<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2019-06-01<BR>ratio: 0.1124<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2019-08-01<BR>ratio: 0.0258<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2020-04-01<BR>ratio: 0.0122<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2020-08-01<BR>ratio: 0.0934<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2021-01-01<BR>ratio: 0.029<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2021-06-01<BR>ratio: 0.0143<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object detection in aerial images<BR>date: 2021-08-01<BR>ratio: 0.0086<BR>benchmarks:<BR>  Object detection in aerial images - DOTA: mAP<BR>","<BR>task: Object detection // Object proposal generation<BR>date: 2017-12-01<BR>ratio: 0.5985<BR>benchmarks:<BR>  Object proposal generation - PASCAL VOC 2012, 60 proposals per image: Average Recall<BR>","<BR>task: Object detection // Object proposal generation<BR>date: 2021-11-01<BR>ratio: 0.4015<BR>benchmarks:<BR>  Object proposal generation - PASCAL VOC 2012, 60 proposals per image: Average Recall<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2015-06-01<BR>ratio: 0.9697<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2015-12-01<BR>ratio: 0.1574<BR>benchmarks:<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2016-06-01<BR>ratio: 0.5602<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - ViSal: Average MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-04-01<BR>ratio: 0.0616<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-07-01<BR>ratio: 0.8567<BR>benchmarks:<BR>  RGB salient object detection - ISTD: Balanced Error Rate<BR>  RGB salient object detection - UCF: Balanced Error Rate<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-08-01<BR>ratio: 0.2671<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2017-12-01<BR>ratio: 0.5018<BR>benchmarks:<BR>  RGB salient object detection - ISTD: Balanced Error Rate<BR>  RGB salient object detection - SBU: Balanced Error Rate<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2018-06-01<BR>ratio: 0.785<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: MAE<BR>  RGB salient object detection - DUTS-TE: S-Measure<BR>  RGB salient object detection - DUTS-TE: mean E-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>  RGB salient object detection - ISTD: Balanced Error Rate<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2018-09-01<BR>ratio: 0.9828<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-04-01<BR>ratio: 0.4982<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>  RGB salient object detection - ISTD: Balanced Error Rate<BR>  RGB salient object detection - PASCAL-S: MAE<BR>  RGB salient object detection - SBU: Balanced Error Rate<BR>  RGB salient object detection - UCF: Balanced Error Rate<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-06-01<BR>ratio: 0.9714<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: S-Measure<BR>  RGB salient object detection - DUTS-TE: mean E-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>  RGB salient object detection - SOC: Average MAE<BR>  RGB salient object detection - SOC: S-Measure<BR>  RGB salient object detection - SOC: mean E-Measure<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-08-01<BR>ratio: 0.1748<BR>benchmarks:<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2019-10-01<BR>ratio: 0.4894<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2020-04-01<BR>ratio: 0.8841<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2020-07-01<BR>ratio: 0.9524<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: max E-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2020-09-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  RGB salient object detection - DUT-OMRON: MAE<BR>  RGB salient object detection - DUTS-TE: MAE<BR>  RGB salient object detection - DUTS-TE: S-Measure<BR>  RGB salient object detection - DUTS-TE: mean E-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>  RGB salient object detection - ECSSD: MAE<BR>  RGB salient object detection - HKU-IS: MAE<BR>  RGB salient object detection - SOC: Average MAE<BR>  RGB salient object detection - SOC: S-Measure<BR>  RGB salient object detection - SOC: mean E-Measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2021-03-01<BR>ratio: 0.9556<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max E-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2021-06-01<BR>ratio: 0.137<BR>benchmarks:<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2021-10-01<BR>ratio: 0.5455<BR>benchmarks:<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>  RGB salient object detection - DUT-OMRON: MAE<BR>  RGB salient object detection - DUTS-TE: mean E-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>  RGB salient object detection - ECSSD: MAE<BR>  RGB salient object detection - HKU-IS: MAE<BR>  RGB salient object detection - PASCAL-S: MAE<BR>","<BR>task: Object detection // RGB salient object detection<BR>date: 2021-12-01<BR>ratio: 0.5<BR>benchmarks:<BR>  RGB salient object detection - DUT-OMRON: MAE<BR>  RGB salient object detection - DUTS-TE: F-measure<BR>  RGB salient object detection - DUTS-TE: MAE<BR>  RGB salient object detection - DUTS-TE: S-Measure<BR>  RGB salient object detection - DUTS-TE: mean F-Measure<BR>  RGB salient object detection - ECSSD: MAE<BR>  RGB salient object detection - HKU-IS: MAE<BR>  RGB salient object detection - PASCAL-S: MAE<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2018-06-01<BR>ratio: 0.8053<BR>benchmarks:<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2019-06-01<BR>ratio: 0.0508<BR>benchmarks:<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2019-07-01<BR>ratio: 0.8657<BR>benchmarks:<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>  RGB-D salient object detection - NLPR: Average MAE<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - NLPR: max E-Measure<BR>  RGB-D salient object detection - NLPR: max F-Measure<BR>  RGB-D salient object detection - RGBD135: Average MAE<BR>  RGB-D salient object detection - RGBD135: S-Measure<BR>  RGB-D salient object detection - RGBD135: max E-Measure<BR>  RGB-D salient object detection - RGBD135: max F-Measure<BR>  RGB-D salient object detection - SIP: Average MAE<BR>  RGB-D salient object detection - SIP: S-Measure<BR>  RGB-D salient object detection - SIP: max E-Measure<BR>  RGB-D salient object detection - SIP: max F-Measure<BR>  RGB-D salient object detection - STERE: Average MAE<BR>  RGB-D salient object detection - STERE: S-Measure<BR>  RGB-D salient object detection - STERE: max E-Measure<BR>  RGB-D salient object detection - STERE: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2020-04-01<BR>ratio: 0.9565<BR>benchmarks:<BR>  RGB-D salient object detection - DES: Average MAE<BR>  RGB-D salient object detection - DES: S-Measure<BR>  RGB-D salient object detection - DES: max E-Measure<BR>  RGB-D salient object detection - DES: max F-Measure<BR>  RGB-D salient object detection - LFSD: Average MAE<BR>  RGB-D salient object detection - LFSD: S-Measure<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>  RGB-D salient object detection - NLPR: Average MAE<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - NLPR: max E-Measure<BR>  RGB-D salient object detection - NLPR: max F-Measure<BR>  RGB-D salient object detection - RGBD135: Average MAE<BR>  RGB-D salient object detection - RGBD135: S-Measure<BR>  RGB-D salient object detection - RGBD135: max E-Measure<BR>  RGB-D salient object detection - RGBD135: max F-Measure<BR>  RGB-D salient object detection - SIP: Average MAE<BR>  RGB-D salient object detection - SIP: S-Measure<BR>  RGB-D salient object detection - SIP: max E-Measure<BR>  RGB-D salient object detection - SIP: max F-Measure<BR>  RGB-D salient object detection - STERE: Average MAE<BR>  RGB-D salient object detection - STERE: S-Measure<BR>  RGB-D salient object detection - STERE: max E-Measure<BR>  RGB-D salient object detection - STERE: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2020-05-01<BR>ratio: 0.3415<BR>benchmarks:<BR>  RGB-D salient object detection - DES: max F-Measure<BR>  RGB-D salient object detection - NLPR: Average MAE<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - NLPR: max F-Measure<BR>  RGB-D salient object detection - RGBD135: Average MAE<BR>  RGB-D salient object detection - RGBD135: S-Measure<BR>  RGB-D salient object detection - RGBD135: max F-Measure<BR>  RGB-D salient object detection - STERE: Average MAE<BR>  RGB-D salient object detection - STERE: S-Measure<BR>  RGB-D salient object detection - STERE: max F-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2020-07-01<BR>ratio: 0.8837<BR>benchmarks:<BR>  RGB-D salient object detection - LFSD: max E-Measure<BR>  RGB-D salient object detection - LFSD: max F-Measure<BR>  RGB-D salient object detection - NJU2K: Average MAE<BR>  RGB-D salient object detection - NJU2K: S-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - RGBD135: max E-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2020-08-01<BR>ratio: 0.5217<BR>benchmarks:<BR>  RGB-D salient object detection - DES: S-Measure<BR>  RGB-D salient object detection - DES: max E-Measure<BR>  RGB-D salient object detection - DES: max F-Measure<BR>  RGB-D salient object detection - NLPR: max E-Measure<BR>  RGB-D salient object detection - SIP: Average MAE<BR>  RGB-D salient object detection - SIP: S-Measure<BR>  RGB-D salient object detection - SIP: max E-Measure<BR>  RGB-D salient object detection - SIP: max F-Measure<BR>  RGB-D salient object detection - STERE: S-Measure<BR>  RGB-D salient object detection - STERE: max E-Measure<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2020-09-01<BR>ratio: 0.1364<BR>benchmarks:<BR>  RGB-D salient object detection - DES: Average MAE<BR>  RGB-D salient object detection - DES: S-Measure<BR>  RGB-D salient object detection - LFSD: Average MAE<BR>  RGB-D salient object detection - LFSD: S-Measure<BR>  RGB-D salient object detection - SIP: Average MAE<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2021-02-01<BR>ratio: 0.0952<BR>benchmarks:<BR>  RGB-D salient object detection - SIP: Average MAE<BR>","<BR>task: Object detection // RGB-D salient object detection<BR>date: 2021-04-01<BR>ratio: 0.254<BR>benchmarks:<BR>  RGB-D salient object detection - DES: S-Measure<BR>  RGB-D salient object detection - DES: max E-Measure<BR>  RGB-D salient object detection - DES: max F-Measure<BR>  RGB-D salient object detection - LFSD: max E-Measure<BR>  RGB-D salient object detection - LFSD: max F-Measure<BR>  RGB-D salient object detection - NJU2K: max E-Measure<BR>  RGB-D salient object detection - NJU2K: max F-Measure<BR>  RGB-D salient object detection - NLPR: S-Measure<BR>  RGB-D salient object detection - NLPR: max E-Measure<BR>  RGB-D salient object detection - SIP: S-Measure<BR>  RGB-D salient object detection - SIP: max F-Measure<BR>  RGB-D salient object detection - STERE: S-Measure<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2016-05-01<BR>ratio: 0.8795<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2017-08-01<BR>ratio: 0.1205<BR>benchmarks:<BR>  Real-time object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2020-10-01<BR>ratio: 0.2673<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2021-03-01<BR>ratio: 0.5644<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2021-04-01<BR>ratio: 0.3791<BR>benchmarks:<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2021-05-01<BR>ratio: 0.0693<BR>benchmarks:<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2021-06-01<BR>ratio: 0.159<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>","<BR>task: Object detection // Real-time object detection<BR>date: 2021-07-01<BR>ratio: 0.841<BR>benchmarks:<BR>  Real-time object detection - Argoverse-HD (Detection-Only, Test): AP<BR>  Real-time object detection - COCO: FPS (V100, b=1)<BR>  Real-time object detection - COCO: box AP<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2015-06-01<BR>ratio: 0.9697<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2015-12-01<BR>ratio: 0.1574<BR>benchmarks:<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2016-06-01<BR>ratio: 0.5602<BR>benchmarks:<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - ViSal: Average MAE<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2018-06-01<BR>ratio: 0.785<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2018-09-01<BR>ratio: 0.9828<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2019-06-01<BR>ratio: 0.9714<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2019-08-01<BR>ratio: 0.1748<BR>benchmarks:<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2020-08-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Salient object detection - DUT-OMRON: MAE<BR>  Salient object detection - HKU-IS: E-measure<BR>  Salient object detection - PASCAL-S: max_F1<BR>","<BR>task: Object detection // Salient object detection<BR>date: 2021-05-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Salient object detection - DUT-OMRON: MAE<BR>  Salient object detection - HKU-IS: E-measure<BR>  Salient object detection - PASCAL-S: max_F1<BR>","<BR>task: Object detection // Surgical tool detection<BR>date: 2018-06-01<BR>ratio: 0.5378<BR>benchmarks:<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Object detection // Surgical tool detection<BR>date: 2018-12-01<BR>ratio: 0.4622<BR>benchmarks:<BR>  Surgical tool detection - Cholec80: mAP<BR>","<BR>task: Object detection // Video object detection<BR>date: 2018-11-01<BR>ratio: 0.6415<BR>benchmarks:<BR>  Video object detection - ImageNet VID: MAP<BR>","<BR>task: Object detection // Video object detection<BR>date: 2019-07-01<BR>ratio: 0.1509<BR>benchmarks:<BR>  Video object detection - ImageNet VID: MAP<BR>","<BR>task: Object detection // Video object detection<BR>date: 2020-03-01<BR>ratio: 0.2075<BR>benchmarks:<BR>  Video object detection - ImageNet VID: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2015-11-01<BR>ratio: 0.4689<BR>benchmarks:<BR>  Weakly supervised object detection - HICO-DET: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2016-03-01<BR>ratio: 0.3359<BR>benchmarks:<BR>  Weakly supervised object detection - COCO: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2016-09-01<BR>ratio: 0.0144<BR>benchmarks:<BR>  Weakly supervised object detection - Charades: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2016-11-01<BR>ratio: 0.1421<BR>benchmarks:<BR>  Weakly supervised object detection - COCO test-dev: AP50<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2017-04-01<BR>ratio: 0.2514<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2017-06-01<BR>ratio: 0.6641<BR>benchmarks:<BR>  Weakly supervised object detection - COCO: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2017-07-01<BR>ratio: 0.0164<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2017-08-01<BR>ratio: 0.0951<BR>benchmarks:<BR>  Weakly supervised object detection - Charades: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2017-11-01<BR>ratio: 0.0977<BR>benchmarks:<BR>  Weakly supervised object detection - COCO test-dev: AP50<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2018-02-01<BR>ratio: 0.1186<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2018-04-01<BR>ratio: 0.0055<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2018-06-01<BR>ratio: 0.2678<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2018-07-01<BR>ratio: 0.108<BR>benchmarks:<BR>  Weakly supervised object detection - Charades: MAP<BR>  Weakly supervised object detection - HICO-DET: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2018-11-01<BR>ratio: 0.0929<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2019-04-01<BR>ratio: 0.7965<BR>benchmarks:<BR>  Weakly supervised object detection - Charades: MAP<BR>  Weakly supervised object detection - HICO-DET: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2019-10-01<BR>ratio: 0.0437<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2019-11-01<BR>ratio: 0.0254<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2020-04-01<BR>ratio: 0.8421<BR>benchmarks:<BR>  Weakly supervised object detection - COCO test-dev: AP50<BR>  Weakly supervised object detection - PASCAL VOC 2007: MAP<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object detection // Weakly supervised object detection<BR>date: 2020-10-01<BR>ratio: 0.082<BR>benchmarks:<BR>  Weakly supervised object detection - PASCAL VOC 2012 test: MAP<BR>","<BR>task: Object localization<BR>date: 2018-04-01<BR>ratio: 0.4981<BR>benchmarks:<BR>  Weakly-supervised object localization - ILSVRC 2016: Top-5 Error<BR>","<BR>task: Object localization<BR>date: 2018-07-01<BR>ratio: 0.5019<BR>benchmarks:<BR>  Weakly-supervised object localization - ILSVRC 2016: Top-5 Error<BR>","<BR>task: Object localization<BR>date: 2019-08-01<BR>ratio: 0.8432<BR>benchmarks:<BR>  Weakly-supervised object localization - CUB-200-2011: Top-1 Error Rate<BR>","<BR>task: Object localization<BR>date: 2019-11-01<BR>ratio: 0.1568<BR>benchmarks:<BR>  Weakly-supervised object localization - CUB-200-2011: Top-1 Error Rate<BR>","<BR>task: Object reconstruction<BR>date: 2016-12-01<BR>ratio: 0.7273<BR>benchmarks:<BR>  3D object reconstruction - Data3D\u2212R2N2: 3DIoU<BR>  3D object reconstruction - Data3D\u2212R2N2: Avg F1<BR>","<BR>task: Object reconstruction<BR>date: 2018-02-01<BR>ratio: 0.628<BR>benchmarks:<BR>  3D object reconstruction - Data3D\u2212R2N2: Avg F1<BR>","<BR>task: Object reconstruction<BR>date: 2019-01-01<BR>ratio: 0.1909<BR>benchmarks:<BR>  3D object reconstruction - Data3D\u2212R2N2: 3DIoU<BR>  3D object reconstruction - Data3D\u2212R2N2: Avg F1<BR>","<BR>task: Object reconstruction<BR>date: 2019-05-01<BR>ratio: 0.9956<BR>benchmarks:<BR>  3D object reconstruction from a single image - RenderPeople: Chamfer (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Point-to-surface distance (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Surface normal consistency<BR>","<BR>task: Object reconstruction<BR>date: 2020-04-01<BR>ratio: 0.8408<BR>benchmarks:<BR>  3D object reconstruction from a single image - BUFF: Chamfer (cm)<BR>  3D object reconstruction from a single image - BUFF: Surface normal consistency<BR>","<BR>task: Object reconstruction<BR>date: 2020-06-01<BR>ratio: 0.0818<BR>benchmarks:<BR>  3D object reconstruction - Data3D\u2212R2N2: 3DIoU<BR>","<BR>task: Object reconstruction<BR>date: 2021-08-01<BR>ratio: 0.68<BR>benchmarks:<BR>  3D object reconstruction from a single image - BUFF: Chamfer (cm)<BR>  3D object reconstruction from a single image - BUFF: Surface normal consistency<BR>  3D object reconstruction from a single image - RenderPeople: Chamfer (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Point-to-surface distance (cm)<BR>  3D object reconstruction from a single image - RenderPeople: Surface normal consistency<BR>","<BR>task: Object segmentation<BR>date: 2019-10-01<BR>ratio: 0.618<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object segmentation<BR>date: 2020-06-01<BR>ratio: 0.3529<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Object segmentation<BR>date: 2020-07-01<BR>ratio: 0.191<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>","<BR>task: Object segmentation<BR>date: 2021-02-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Camouflaged object segmentation - CAMO: E-Measure<BR>  Camouflaged object segmentation - CAMO: MAE<BR>  Camouflaged object segmentation - CAMO: S-Measure<BR>  Camouflaged object segmentation - CAMO: Weighted F-Measure<BR>  Camouflaged object segmentation - COD: E-Measure<BR>  Camouflaged object segmentation - COD: MAE<BR>  Camouflaged object segmentation - COD: S-Measure<BR>  Camouflaged object segmentation - COD: Weighted F-Measure<BR>","<BR>task: Optical character recognition<BR>date: 2018-11-01<BR>ratio: 0.0517<BR>benchmarks:<BR>  Optical character recognition - Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study: Accuracy (%)<BR>","<BR>task: Optical character recognition<BR>date: 2021-06-01<BR>ratio: 0.9483<BR>benchmarks:<BR>  Optical character recognition - Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study: Accuracy (%)<BR>","<BR>task: Optical flow estimation<BR>date: 2017-09-01<BR>ratio: 0.0874<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015 (train) : EPE<BR>","<BR>task: Optical flow estimation<BR>date: 2018-12-01<BR>ratio: 0.9126<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015 (train) : EPE<BR>","<BR>task: Optical flow estimation<BR>date: 2019-04-01<BR>ratio: 0.2265<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015: Fl-all<BR>","<BR>task: Optical flow estimation<BR>date: 2020-11-01<BR>ratio: 0.7735<BR>benchmarks:<BR>  Optical flow estimation - KITTI 2015: Fl-all<BR>","<BR>task: Out-of-distribution detection<BR>date: 2018-07-01<BR>ratio: 0.8346<BR>benchmarks:<BR>  Out-of-distribution detection - MS-1M vs. IJB-C: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2018-12-01<BR>ratio: 0.5085<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUPR<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2019-06-01<BR>ratio: 0.7391<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUPR<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUROC<BR>  Out-of-distribution detection - CIFAR-100: FPR95<BR>  Out-of-distribution detection - CIFAR-10: AUROC<BR>  Out-of-distribution detection - CIFAR-10: FPR95<BR>  Out-of-distribution detection - ImageNet dogs vs ImageNet non-dogs: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2020-03-01<BR>ratio: 0.3555<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUROC<BR>  Out-of-distribution detection - CIFAR-100 vs CIFAR-10: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2020-11-01<BR>ratio: 0.5238<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10: AUROC<BR>  Out-of-distribution detection - ImageNet dogs vs ImageNet non-dogs: AUROC<BR>  Out-of-distribution detection - MS-1M vs. IJB-C: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2020-12-01<BR>ratio: 0.4482<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100 vs CIFAR-10: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2021-04-01<BR>ratio: 0.0261<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100 vs SVHN: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2021-05-01<BR>ratio: 0.608<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100: FPR95<BR>  Out-of-distribution detection - CIFAR-10: AUROC<BR>  Out-of-distribution detection - CIFAR-10: FPR95<BR>","<BR>task: Out-of-distribution detection<BR>date: 2021-06-01<BR>ratio: 0.3754<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUPR<BR>  Out-of-distribution detection - CIFAR-10 vs CIFAR-100: AUROC<BR>  Out-of-distribution detection - CIFAR-100 vs CIFAR-10: AUROC<BR>","<BR>task: Out-of-distribution detection<BR>date: 2021-10-01<BR>ratio: 0.9739<BR>benchmarks:<BR>  Out-of-distribution detection - CIFAR-100 vs SVHN: AUROC<BR>","<BR>task: Person re-identification<BR>date: 2016-03-01<BR>ratio: 0.3163<BR>benchmarks:<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2016-04-01<BR>ratio: 0.5759<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>","<BR>task: Person re-identification<BR>date: 2016-07-01<BR>ratio: 0.0892<BR>benchmarks:<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2016-10-01<BR>ratio: 0.1222<BR>benchmarks:<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2016-11-01<BR>ratio: 0.1827<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-01-01<BR>ratio: 0.6234<BR>benchmarks:<BR>  Person re-identification - CUHK03: MAP<BR>  Person re-identification - CUHK03: Rank-1<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-03-01<BR>ratio: 0.2373<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03: Rank-1<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-07-01<BR>ratio: 0.275<BR>benchmarks:<BR>  Person re-identification - CUHK-SYSU: MAP<BR>  Person re-identification - CUHK-SYSU: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2017-08-01<BR>ratio: 0.0401<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2017-09-01<BR>ratio: 0.0244<BR>benchmarks:<BR>  Person re-identification - Market-1501: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2017-10-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Person re-identification - PRID2011: Rank-1<BR>  Person re-identification - PRID2011: Rank-20<BR>  Person re-identification - PRID2011: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2017-11-01<BR>ratio: 0.8088<BR>benchmarks:<BR>  Person re-identification - CUHK-SYSU: MAP<BR>  Person re-identification - CUHK-SYSU: Rank-1<BR>  Person re-identification - CUHK03: Rank-1<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2018-02-01<BR>ratio: 0.0221<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2018-04-01<BR>ratio: 0.4817<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - SYSU-30k: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2018-05-01<BR>ratio: 0.0025<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>","<BR>task: Person re-identification<BR>date: 2018-06-01<BR>ratio: 0.6<BR>benchmarks:<BR>  Person re-identification - PRID2011: Rank-1<BR>  Person re-identification - PRID2011: Rank-20<BR>  Person re-identification - PRID2011: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2018-07-01<BR>ratio: 0.2887<BR>benchmarks:<BR>  Person re-identification - CUHK03: MAP<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2018-09-01<BR>ratio: 0.3871<BR>benchmarks:<BR>  Person re-identification - Market-1501-C: Rank-1<BR>  Person re-identification - Market-1501-C: mAP<BR>  Person re-identification - Market-1501-C: mINP<BR>","<BR>task: Person re-identification<BR>date: 2018-10-01<BR>ratio: 0.2757<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - CUHK03-C: Rank-1<BR>  Person re-identification - CUHK03-C: mAP<BR>  Person re-identification - CUHK03-C: mINP<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2018-11-01<BR>ratio: 0.6483<BR>benchmarks:<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: mAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2018-12-01<BR>ratio: 0.0849<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - Market-1501: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>","<BR>task: Person re-identification<BR>date: 2019-03-01<BR>ratio: 0.2652<BR>benchmarks:<BR>  Person re-identification - UAV-Human: Rank-1<BR>  Person re-identification - UAV-Human: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-04-01<BR>ratio: 0.9199<BR>benchmarks:<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>  Person re-identification - SYSU-30k: Rank-1<BR>  Person re-identification - UAV-Human: Rank-1<BR>  Person re-identification - UAV-Human: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-05-01<BR>ratio: 0.4242<BR>benchmarks:<BR>  Person re-identification - MSMT17-C: Rank-1<BR>  Person re-identification - MSMT17-C: mAP<BR>  Person re-identification - MSMT17-C: mINP<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>","<BR>task: Person re-identification<BR>date: 2019-08-01<BR>ratio: 0.9865<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-10<BR>  Person re-identification - MARS: Rank-1<BR>  Person re-identification - MARS: Rank-5<BR>  Person re-identification - MARS: mAP<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>  Person re-identification - iLIDS-VID: Rank-1<BR>  Person re-identification - iLIDS-VID: Rank-20<BR>  Person re-identification - iLIDS-VID: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-09-01<BR>ratio: 0.9565<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-10<BR>  Person re-identification - MARS: Rank-20<BR>  Person re-identification - PRID2011: Rank-1<BR>  Person re-identification - iLIDS-VID: Rank-1<BR>  Person re-identification - iLIDS-VID: Rank-20<BR>","<BR>task: Person re-identification<BR>date: 2019-10-01<BR>ratio: 0.8957<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-10<BR>  Person re-identification - DukeMTMC-reID: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2019-11-01<BR>ratio: 0.5149<BR>benchmarks:<BR>  Person re-identification - MARS: mAP<BR>  Person re-identification - PRID2011: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2019-12-01<BR>ratio: 0.1066<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2020-01-01<BR>ratio: 0.0942<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - DukeMTMC-reID: Rank-10<BR>","<BR>task: Person re-identification<BR>date: 2020-03-01<BR>ratio: 0.1795<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>  Self-Supervised Person Re-Identification - SYSU-30k: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2020-06-01<BR>ratio: 0.2821<BR>benchmarks:<BR>  Person re-identification - MSMT17-C: Rank-1<BR>  Person re-identification - MSMT17-C: mAP<BR>  Self-Supervised Person Re-Identification - SYSU-30k: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2020-07-01<BR>ratio: 0.5699<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-5<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>","<BR>task: Person re-identification<BR>date: 2020-09-01<BR>ratio: 0.4422<BR>benchmarks:<BR>  Person re-identification - CUHK03-C: Rank-1<BR>  Person re-identification - CUHK03-C: mAP<BR>  Person re-identification - CUHK03-C: mINP<BR>","<BR>task: Person re-identification<BR>date: 2020-10-01<BR>ratio: 0.1329<BR>benchmarks:<BR>  Person re-identification - CUHK03 detected: MAP<BR>  Person re-identification - CUHK03 detected: Rank-1<BR>  Person re-identification - CUHK03 labeled: MAP<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2020-12-01<BR>ratio: 0.0955<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: Rank-10<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - Market-1501: mAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2021-01-01<BR>ratio: 0.1383<BR>benchmarks:<BR>  Person re-identification - CUHK03 labeled: Rank-1<BR>  Person re-identification - iLIDS-VID: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2021-02-01<BR>ratio: 0.8344<BR>benchmarks:<BR>  Person re-identification - Market-1501-C: Rank-1<BR>  Person re-identification - Market-1501-C: mAP<BR>  Person re-identification - Market-1501-C: mINP<BR>","<BR>task: Person re-identification<BR>date: 2021-03-01<BR>ratio: 0.5723<BR>benchmarks:<BR>  Person re-identification - DukeMTMC-reID: mAP<BR>  Person re-identification - SYSU-30k: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2021-04-01<BR>ratio: 0.98<BR>benchmarks:<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: mAP<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - Market-1501->MSMT17: Rank-1<BR>  Generalizable person re-identification - Market-1501->MSMT17: mAP<BR>  Person re-identification - DukeMTMC-reID: Rank-10<BR>  Person re-identification - DukeMTMC-reID: Rank-1<BR>  Person re-identification - MARS: Rank-20<BR>  Person re-identification - Market-1501: mAP<BR>  Self-Supervised Person Re-Identification - SYSU-30k: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2021-05-01<BR>ratio: 0.6515<BR>benchmarks:<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: Rank-1<BR>  Generalizable person re-identification - MSMT17(all)->Market-1501: mAP<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): Rank-1<BR>  Generalizable person re-identification - Market-1501->CUHK03-NP (detected): mAP<BR>  Generalizable person re-identification - Market-1501->MSMT17: Rank-1<BR>  Generalizable person re-identification - Market-1501->MSMT17: mAP<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>","<BR>task: Person re-identification<BR>date: 2021-06-01<BR>ratio: 0.0879<BR>benchmarks:<BR>  Person re-identification - CUHK03: MAP<BR>  Person re-identification - CUHK03: Rank-1<BR>  Person re-identification - Market-1501: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2021-08-01<BR>ratio: 0.0538<BR>benchmarks:<BR>  Person re-identification - MARS: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2021-09-01<BR>ratio: 0.0598<BR>benchmarks:<BR>  Unsupervised person re-identification - DukeMTMC-reID: MAP<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-10<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-1<BR>  Unsupervised person re-identification - DukeMTMC-reID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-10<BR>  Unsupervised person re-identification - Market-1501: Rank-5<BR>","<BR>task: Person re-identification<BR>date: 2021-11-01<BR>ratio: 0.96<BR>benchmarks:<BR>  Person re-identification - CUHK03-C: Rank-1<BR>  Person re-identification - CUHK03-C: mAP<BR>  Person re-identification - CUHK03-C: mINP<BR>  Person re-identification - MSMT17-C: Rank-1<BR>  Person re-identification - MSMT17-C: mAP<BR>  Person re-identification - MSMT17-C: mINP<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - Market-1501-C: Rank-1<BR>  Person re-identification - Market-1501-C: mAP<BR>  Person re-identification - iLIDS-VID: Rank-5<BR>  Unsupervised person re-identification - Market-1501: MAP<BR>  Unsupervised person re-identification - Market-1501: Rank-1<BR>","<BR>task: Person re-identification<BR>date: 2021-12-01<BR>ratio: 0.0587<BR>benchmarks:<BR>  Person re-identification - MSMT17: Rank-1<BR>  Person re-identification - MSMT17: mAP<BR>","<BR>task: Person search<BR>date: 2021-09-01<BR>ratio: 0.6<BR>benchmarks:<BR>  Person search - CUHK-SYSU: MAP<BR>  Person search - CUHK-SYSU: Top-1<BR>","<BR>task: Person search<BR>date: 2021-12-01<BR>ratio: 0.4<BR>benchmarks:<BR>  Person search - CUHK-SYSU: MAP<BR>  Person search - CUHK-SYSU: Top-1<BR>","<BR>task: Point Cloud Segmentation<BR>date: 2017-06-01<BR>ratio: 0.2588<BR>benchmarks:<BR>  Point Cloud Segmentation - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point Cloud Segmentation<BR>date: 2020-10-01<BR>ratio: 0.5294<BR>benchmarks:<BR>  Point Cloud Segmentation - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point Cloud Segmentation<BR>date: 2020-12-01<BR>ratio: 0.2118<BR>benchmarks:<BR>  Point Cloud Segmentation - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud classification<BR>date: 2016-08-01<BR>ratio: 0.9416<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Mean Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2017-04-01<BR>ratio: 0.3208<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2017-06-01<BR>ratio: 0.869<BR>benchmarks:<BR>  3D point cloud classification - IntrA: F1 score (5-fold)<BR>  3D point cloud classification - ModelNet40-C: Error Rate<BR>  3D point cloud classification - ScanObjectNN: Mean Accuracy<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2018-01-01<BR>ratio: 0.6233<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>  Point cloud classification - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud classification<BR>date: 2019-08-01<BR>ratio: 0.0377<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2019-10-01<BR>ratio: 0.0943<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2019-11-01<BR>ratio: 0.1529<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>  3D point cloud classification - ScanObjectNN: Mean Accuracy<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2020-05-01<BR>ratio: 0.0127<BR>benchmarks:<BR>  3D point cloud classification - ScanObjectNN: Mean Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2020-08-01<BR>ratio: 0.3909<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40-C: Error Rate<BR>","<BR>task: Point cloud classification<BR>date: 2020-11-01<BR>ratio: 0.1394<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Mean Accuracy<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2020-12-01<BR>ratio: 0.0437<BR>benchmarks:<BR>  3D point cloud classification - IntrA: F1 score (5-fold)<BR>","<BR>task: Point cloud classification<BR>date: 2021-01-01<BR>ratio: 0.1818<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40-C: Error Rate<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2021-02-01<BR>ratio: 0.2171<BR>benchmarks:<BR>  Point cloud classification - PointCloud-C: mean Corruption Error (mCE)<BR>","<BR>task: Point cloud classification<BR>date: 2021-05-01<BR>ratio: 0.0566<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2021-09-01<BR>ratio: 0.2264<BR>benchmarks:<BR>  3D point cloud classification - ModelNet40: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2021-11-01<BR>ratio: 0.1152<BR>benchmarks:<BR>  3D point cloud classification - ScanObjectNN: Overall Accuracy<BR>","<BR>task: Point cloud classification<BR>date: 2021-12-01<BR>ratio: 0.0873<BR>benchmarks:<BR>  3D point cloud classification - IntrA: F1 score (5-fold)<BR>  3D point cloud classification - ScanObjectNN: Mean Accuracy<BR>","<BR>task: Point cloud generation<BR>date: 2018-08-01<BR>ratio: 0.9863<BR>benchmarks:<BR>  Point cloud completion - ShapeNet: Chamfer Distance<BR>","<BR>task: Point cloud generation<BR>date: 2019-06-01<BR>ratio: 0.4937<BR>benchmarks:<BR>  Point cloud completion - Completion3D: Chamfer Distance<BR>","<BR>task: Point cloud generation<BR>date: 2019-11-01<BR>ratio: 0.1887<BR>benchmarks:<BR>  Point cloud completion - ShapeNet: F-Score@1%<BR>","<BR>task: Point cloud generation<BR>date: 2020-06-01<BR>ratio: 0.5063<BR>benchmarks:<BR>  Point cloud completion - Completion3D: Chamfer Distance<BR>  Point cloud completion - ShapeNet: Chamfer Distance<BR>  Point cloud completion - ShapeNet: F-Score@1%<BR>","<BR>task: Point cloud generation<BR>date: 2021-08-01<BR>ratio: 0.7547<BR>benchmarks:<BR>  Point cloud completion - ShapeNet: Chamfer Distance<BR>  Point cloud completion - ShapeNet: F-Score@1%<BR>","<BR>task: Point cloud registration<BR>date: 2018-08-01<BR>ratio: 0.1582<BR>benchmarks:<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>","<BR>task: Point cloud registration<BR>date: 2018-11-01<BR>ratio: 0.7247<BR>benchmarks:<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>","<BR>task: Point cloud registration<BR>date: 2019-10-01<BR>ratio: 0.5537<BR>benchmarks:<BR>  Point cloud registration - 3DLoMatch (10-30% overlap): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - 3DMatch (at least 30% overlapped - sample 5k interest points): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - KITTI: Success Rate<BR>","<BR>task: Point cloud registration<BR>date: 2020-03-01<BR>ratio: 0.8416<BR>benchmarks:<BR>  Point cloud registration - 3DMatch (trained on KITTI): Recall<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>  Point cloud registration - KITTI (trained on 3DMatch): Success Rate<BR>  Point cloud registration - KITTI: Success Rate<BR>","<BR>task: Point cloud registration<BR>date: 2020-04-01<BR>ratio: 0.2915<BR>benchmarks:<BR>  Point cloud registration - 3DLoMatch (10-30% overlap): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - 3DMatch (at least 30% overlapped - sample 5k interest points): Recall ( correspondence RMSE below 0.2)<BR>","<BR>task: Point cloud registration<BR>date: 2020-09-01<BR>ratio: 0.7594<BR>benchmarks:<BR>  Point cloud registration - ETH (trained on 3DMatch): Recall<BR>  Point cloud registration - KITTI (trained on 3DMatch): Success Rate<BR>","<BR>task: Point cloud registration<BR>date: 2020-11-01<BR>ratio: 0.4678<BR>benchmarks:<BR>  Point cloud registration - 3DLoMatch (10-30% overlap): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - 3DMatch (at least 30% overlapped - sample 5k interest points): Recall ( correspondence RMSE below 0.2)<BR>  Point cloud registration - 3DMatch (trained on KITTI): Recall<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>","<BR>task: Point cloud registration<BR>date: 2021-03-01<BR>ratio: 0.0253<BR>benchmarks:<BR>  Point cloud registration - 3DMatch Benchmark: Feature Matching Recall<BR>","<BR>task: Point cloud registration<BR>date: 2021-05-01<BR>ratio: 0.2812<BR>benchmarks:<BR>  Point cloud registration - 3DMatch (trained on KITTI): Recall<BR>  Point cloud registration - ETH (trained on 3DMatch): Recall<BR>  Point cloud registration - KITTI (trained on 3DMatch): Success Rate<BR>  Point cloud registration - KITTI: Success Rate<BR>","<BR>task: Point cloud super resolution<BR>date: 2019-08-01<BR>ratio: 0.9518<BR>benchmarks:<BR>  Point cloud super resolution - SHREC15: F-measure (%)<BR>","<BR>task: Point cloud super resolution<BR>date: 2021-02-01<BR>ratio: 0.0482<BR>benchmarks:<BR>  Point cloud super resolution - SHREC15: F-measure (%)<BR>","<BR>task: Pose tracking<BR>date: 2017-12-01<BR>ratio: 0.2195<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>  Pose tracking - PoseTrack2017: mAP<BR>","<BR>task: Pose tracking<BR>date: 2018-02-01<BR>ratio: 0.2155<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: mAP<BR>","<BR>task: Pose tracking<BR>date: 2018-04-01<BR>ratio: 0.7387<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>  Pose tracking - PoseTrack2017: mAP<BR>","<BR>task: Pose tracking<BR>date: 2019-02-01<BR>ratio: 0.0242<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>  Pose tracking - PoseTrack2017: mAP<BR>","<BR>task: Pose tracking<BR>date: 2019-05-01<BR>ratio: 0.0051<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>","<BR>task: Pose tracking<BR>date: 2019-12-01<BR>ratio: 0.1997<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>","<BR>task: Pose tracking<BR>date: 2020-03-01<BR>ratio: 0.187<BR>benchmarks:<BR>  Pose tracking - PoseTrack2017: MOTA<BR>","<BR>task: Quantization<BR>date: 2021-03-01<BR>ratio: 0.7836<BR>benchmarks:<BR>  Data Free Quantization - CIFAR-100: CIFAR-100 W5A5 Top-1 Accuracy<BR>","<BR>task: Quantization<BR>date: 2021-11-01<BR>ratio: 0.2164<BR>benchmarks:<BR>  Data Free Quantization - CIFAR-100: CIFAR-100 W5A5 Top-1 Accuracy<BR>","<BR>task: Rain removal<BR>date: 2018-02-01<BR>ratio: 0.7253<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test1200: SSIM<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2018-07-01<BR>ratio: 0.6218<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Rain100H: SSIM<BR>  Single image deraining - Rain100L: PSNR<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2019-01-01<BR>ratio: 0.9135<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Rain100H: SSIM<BR>  Single image deraining - Rain100L: PSNR<BR>  Single image deraining - Rain100L: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2019-06-01<BR>ratio: 0.0989<BR>benchmarks:<BR>  Single image deraining - Test1200: SSIM<BR>","<BR>task: Rain removal<BR>date: 2020-03-01<BR>ratio: 0.3628<BR>benchmarks:<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test1200: SSIM<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2020-12-01<BR>ratio: 0.2838<BR>benchmarks:<BR>  Single image deraining - Rain100L: PSNR<BR>  Single image deraining - Rain100L: SSIM<BR>","<BR>task: Rain removal<BR>date: 2021-02-01<BR>ratio: 0.3001<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2021-05-01<BR>ratio: 0.0796<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test1200: SSIM<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Rain removal<BR>date: 2021-11-01<BR>ratio: 0.1853<BR>benchmarks:<BR>  Single image deraining - Rain100H: PSNR<BR>  Single image deraining - Rain100H: SSIM<BR>  Single image deraining - Test100: PSNR<BR>  Single image deraining - Test100: SSIM<BR>  Single image deraining - Test1200: PSNR<BR>  Single image deraining - Test1200: SSIM<BR>  Single image deraining - Test2800: PSNR<BR>  Single image deraining - Test2800: SSIM<BR>","<BR>task: Reconstruction<BR>date: 2020-06-01<BR>ratio: 0.6957<BR>benchmarks:<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>","<BR>task: Reconstruction<BR>date: 2020-08-01<BR>ratio: 0.8916<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>  3D human reconstruction - AGORA: F-MPJPE<BR>  3D human reconstruction - AGORA: F-MVE<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: Reconstruction<BR>date: 2020-12-01<BR>ratio: 0.3876<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>","<BR>task: Reconstruction<BR>date: 2021-08-01<BR>ratio: 0.1084<BR>benchmarks:<BR>  3D human reconstruction - Expressive hands and faces dataset (EHF): TR V2V (mm), left hand<BR>","<BR>task: Reconstruction<BR>date: 2021-12-01<BR>ratio: 0.4351<BR>benchmarks:<BR>  3D Semantic Scene Completion from a single RGB image - SemanticKITTI: mIoU<BR>","<BR>task: Remote sensing<BR>date: 2020-03-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Change detection for remote sensing images - CDD Dataset (season-varying): F1-Score<BR>","<BR>task: Remote sensing<BR>date: 2021-02-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Change detection for remote sensing images - CDD Dataset (season-varying): F1-Score<BR>","<BR>task: Robot navigation<BR>date: 2020-07-01<BR>ratio: 0.7915<BR>benchmarks:<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SPL<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SUCCESS<BR>","<BR>task: Robot navigation<BR>date: 2021-04-01<BR>ratio: 0.2164<BR>benchmarks:<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SPL<BR>  Robot navigation - Habitat 2020 Object Nav test-std: SUCCESS<BR>","<BR>task: Saliency detection<BR>date: 2019-10-01<BR>ratio: 0.4894<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Saliency detection<BR>date: 2020-04-01<BR>ratio: 0.8841<BR>benchmarks:<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Saliency detection<BR>date: 2020-07-01<BR>ratio: 0.9524<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: max E-measure<BR>","<BR>task: Saliency detection<BR>date: 2020-10-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Video saliency detection - DHF1K: NSS<BR>","<BR>task: Saliency detection<BR>date: 2020-12-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Video saliency detection - DHF1K: NSS<BR>","<BR>task: Saliency detection<BR>date: 2021-03-01<BR>ratio: 0.9556<BR>benchmarks:<BR>  Co-salient object detection - CoCA: MAE<BR>  Co-salient object detection - CoCA: Mean F-measure<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max E-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoCA: mean E-measure<BR>  Co-salient object detection - CoSOD3k: MAE<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max E-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Saliency detection<BR>date: 2021-10-01<BR>ratio: 0.2812<BR>benchmarks:<BR>  Co-salient object detection - CoCA: S-measure<BR>  Co-salient object detection - CoCA: max F-measure<BR>  Co-salient object detection - CoSOD3k: S-measure<BR>  Co-salient object detection - CoSOD3k: max F-measure<BR>  Co-salient object detection - CoSal2015: MAE<BR>  Co-salient object detection - CoSal2015: S-measure<BR>  Co-salient object detection - CoSal2015: max E-measure<BR>  Co-salient object detection - CoSal2015: max F-measure<BR>","<BR>task: Scene parsing<BR>date: 2015-07-01<BR>ratio: 0.4604<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2016-03-01<BR>ratio: 0.1038<BR>benchmarks:<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2016-09-01<BR>ratio: 0.0612<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2017-11-01<BR>ratio: 0.8537<BR>benchmarks:<BR>  Panoptic Scene Graph Generation - PSG Dataset: R@20<BR>  Panoptic Scene Graph Generation - PSG Dataset: mR@20<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2018-06-01<BR>ratio: 0.2168<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2018-08-01<BR>ratio: 0.0321<BR>benchmarks:<BR>  Scene graph generation - Visual Genome: Recall-at-50<BR>","<BR>task: Scene parsing<BR>date: 2018-11-01<BR>ratio: 0.8757<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: Scene parsing<BR>date: 2018-12-01<BR>ratio: 0.7779<BR>benchmarks:<BR>  Panoptic Scene Graph Generation - PSG Dataset: R@20<BR>  Panoptic Scene Graph Generation - PSG Dataset: mR@20<BR>  Scene graph generation - Visual Genome: Recall-at-50<BR>","<BR>task: Scene parsing<BR>date: 2019-01-01<BR>ratio: 0.6177<BR>benchmarks:<BR>  3D room layouts from a single rgb panorama - PanoContext: 3DIoU<BR>  3D room layouts from a single rgb panorama - Stanford 2D-3D: 3DIoU<BR>","<BR>task: Scene parsing<BR>date: 2019-04-01<BR>ratio: 0.3766<BR>benchmarks:<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2019-10-01<BR>ratio: 0.2987<BR>benchmarks:<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2019-12-01<BR>ratio: 0.028<BR>benchmarks:<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2020-02-01<BR>ratio: 0.19<BR>benchmarks:<BR>  Scene graph generation - Visual Genome: Recall-at-50<BR>","<BR>task: Scene parsing<BR>date: 2020-03-01<BR>ratio: 0.0765<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2020-04-01<BR>ratio: 0.2547<BR>benchmarks:<BR>  Face parsing - CelebAMask-HQ: Mean F1<BR>  Face parsing - LaPa: Mean F1<BR>","<BR>task: Scene parsing<BR>date: 2020-05-01<BR>ratio: 0.042<BR>benchmarks:<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2020-07-01<BR>ratio: 0.2902<BR>benchmarks:<BR>  Face parsing - CelebAMask-HQ: Mean F1<BR>  Face parsing - LaPa: Mean F1<BR>","<BR>task: Scene parsing<BR>date: 2020-09-01<BR>ratio: 0.9655<BR>benchmarks:<BR>  Unbiased Scene Graph Generation - Visual Genome: mR@20<BR>  Unbiased Scene Graph Generation - Visual Genome: ng-mR@20<BR>","<BR>task: Scene parsing<BR>date: 2021-01-01<BR>ratio: 0.2679<BR>benchmarks:<BR>  Face parsing - CelebAMask-HQ: Mean F1<BR>  Face parsing - LaPa: Mean F1<BR>","<BR>task: Scene parsing<BR>date: 2021-02-01<BR>ratio: 0.1119<BR>benchmarks:<BR>  Face parsing - LaPa: Mean F1<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2021-06-01<BR>ratio: 0.042<BR>benchmarks:<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2021-07-01<BR>ratio: 0.4835<BR>benchmarks:<BR>  Scene text recognition - ICDAR 2003: Accuracy<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>  Unbiased Scene Graph Generation - Visual Genome: mR@20<BR>  Unbiased Scene Graph Generation - Visual Genome: ng-mR@20<BR>","<BR>task: Scene parsing<BR>date: 2021-08-01<BR>ratio: 0.2308<BR>benchmarks:<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2021-11-01<BR>ratio: 0.4812<BR>benchmarks:<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>  Scene text recognition - IIIT5k: Accuracy<BR>  Scene text recognition - SVTP: Accuracy<BR>","<BR>task: Scene parsing<BR>date: 2021-12-01<BR>ratio: 0.7154<BR>benchmarks:<BR>  Face parsing - CelebAMask-HQ: Mean F1<BR>  Face parsing - LaPa: Mean F1<BR>  Scene text recognition - ICDAR2013: Accuracy<BR>  Scene text recognition - ICDAR2015: Accuracy<BR>  Scene text recognition - IIIT5k: Accuracy<BR>  Scene text recognition - SVT: Accuracy<BR>  Scene text recognition - SVTP: Accuracy<BR>","<BR>task: Scene text detection<BR>date: 2015-04-01<BR>ratio: 0.1818<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Recall<BR>","<BR>task: Scene text detection<BR>date: 2016-04-01<BR>ratio: 0.3933<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Precision<BR>  Scene text detection - ICDAR 2013: Recall<BR>","<BR>task: Scene text detection<BR>date: 2017-03-01<BR>ratio: 0.2972<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Recall<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - ICDAR 2015: Recall<BR>","<BR>task: Scene text detection<BR>date: 2017-04-01<BR>ratio: 0.4059<BR>benchmarks:<BR>  Scene text detection - COCO-Text: F-Measure<BR>  Scene text detection - COCO-Text: Precision<BR>  Scene text detection - COCO-Text: Recall<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - ICDAR 2015: Recall<BR>  Scene text detection - MSRA-TD500: Precision<BR>","<BR>task: Scene text detection<BR>date: 2017-07-01<BR>ratio: 0.2807<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>","<BR>task: Scene text detection<BR>date: 2017-08-01<BR>ratio: 0.1791<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Precision<BR>  Scene text detection - ICDAR 2013: Recall<BR>","<BR>task: Scene text detection<BR>date: 2017-09-01<BR>ratio: 0.8637<BR>benchmarks:<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - MSRA-TD500: Precision<BR>  Scene text detection - MSRA-TD500: Recall<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>  Scene text detection - Total-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-01-01<BR>ratio: 0.7472<BR>benchmarks:<BR>  Scene text detection - COCO-Text: F-Measure<BR>  Scene text detection - COCO-Text: Precision<BR>  Scene text detection - COCO-Text: Recall<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - ICDAR 2015: Recall<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>","<BR>task: Scene text detection<BR>date: 2018-02-01<BR>ratio: 0.7578<BR>benchmarks:<BR>  Scene text detection - ICDAR 2017 MLT: F-Measure<BR>  Scene text detection - ICDAR 2017 MLT: Precision<BR>  Scene text detection - ICDAR 2017 MLT: Recall<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>","<BR>task: Scene text detection<BR>date: 2018-04-01<BR>ratio: 0.1823<BR>benchmarks:<BR>  Scene text detection - COCO-Text: F-Measure<BR>  Scene text detection - COCO-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-06-01<BR>ratio: 0.6441<BR>benchmarks:<BR>  Scene text detection - ICDAR 2017 MLT: F-Measure<BR>  Scene text detection - SCUT-CTW1500: Precision<BR>  Scene text detection - SCUT-CTW1500: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-07-01<BR>ratio: 0.3559<BR>benchmarks:<BR>  Curved text detection - SCUT-CTW1500: F-Measure<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Precision<BR>  Scene text detection - ICDAR 2013: Recall<BR>  Scene text detection - SCUT-CTW1500: Recall<BR>","<BR>task: Scene text detection<BR>date: 2018-11-01<BR>ratio: 0.6148<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2013: Recall<BR>  Scene text detection - ICDAR 2017 MLT: F-Measure<BR>  Scene text detection - SCUT-CTW1500: F-Measure<BR>  Scene text detection - SCUT-CTW1500: Precision<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2019-03-01<BR>ratio: 0.6215<BR>benchmarks:<BR>  Scene text detection - ICDAR 2017 MLT: F-Measure<BR>  Scene text detection - ICDAR 2017 MLT: Precision<BR>  Scene text detection - ICDAR 2017 MLT: Recall<BR>","<BR>task: Scene text detection<BR>date: 2019-04-01<BR>ratio: 0.8295<BR>benchmarks:<BR>  Curved text detection - SCUT-CTW1500: F-Measure<BR>  Scene text detection - ICDAR 2013: Precision<BR>  Scene text detection - ICDAR 2013: Recall<BR>  Scene text detection - MSRA-TD500: Precision<BR>  Scene text detection - MSRA-TD500: Recall<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>","<BR>task: Scene text detection<BR>date: 2019-08-01<BR>ratio: 0.4058<BR>benchmarks:<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>  Scene text detection - MSRA-TD500: Recall<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>","<BR>task: Scene text detection<BR>date: 2019-10-01<BR>ratio: 0.2675<BR>benchmarks:<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - ICDAR 2015: Recall<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>  Scene text detection - Total-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2019-11-01<BR>ratio: 0.6<BR>benchmarks:<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>  Scene text detection - MSRA-TD500: Precision<BR>","<BR>task: Scene text detection<BR>date: 2019-12-01<BR>ratio: 0.0134<BR>benchmarks:<BR>  Scene text detection - ICDAR 2017 MLT: Recall<BR>","<BR>task: Scene text detection<BR>date: 2020-05-01<BR>ratio: 0.3852<BR>benchmarks:<BR>  Scene text detection - ICDAR 2013: F-Measure<BR>  Scene text detection - ICDAR 2015: F-Measure<BR>  Scene text detection - ICDAR 2015: Precision<BR>  Scene text detection - SCUT-CTW1500: F-Measure<BR>  Scene text detection - SCUT-CTW1500: Precision<BR>  Scene text detection - Total-Text: F-Measure<BR>  Scene text detection - Total-Text: Recall<BR>","<BR>task: Scene text detection<BR>date: 2021-11-01<BR>ratio: 0.1856<BR>benchmarks:<BR>  Scene text detection - MSRA-TD500: F-Measure<BR>  Scene text detection - Total-Text: Precision<BR>","<BR>task: Semantic segmentation<BR>date: 2014-11-01<BR>ratio: 0.2725<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2014-12-01<BR>ratio: 0.2416<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-02-01<BR>ratio: 0.0797<BR>benchmarks:<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-03-01<BR>ratio: 0.0458<BR>benchmarks:<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-04-01<BR>ratio: 0.3972<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-09-01<BR>ratio: 0.3102<BR>benchmarks:<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2015-11-01<BR>ratio: 0.1841<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - CamVid: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-03-01<BR>ratio: 0.1414<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-05-01<BR>ratio: 0.0458<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-06-01<BR>ratio: 0.0458<BR>benchmarks:<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-11-01<BR>ratio: 0.4776<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - CamVid: Mean IoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-12-01<BR>ratio: 0.4583<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Semantic segmentation - ScanNetV2: Mean IoU<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-03-01<BR>ratio: 0.2689<BR>benchmarks:<BR>  Semantic segmentation - PASCAL VOC 2012 val: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-04-01<BR>ratio: 0.994<BR>benchmarks:<BR>  Semantic segmentation - Semantic3D: mIoU<BR>  Semantic segmentation - Trans10K: GFLOPs<BR>","<BR>task: Semantic segmentation<BR>date: 2017-06-01<BR>ratio: 0.1381<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Semantic segmentation - PASCAL VOC 2012 val: mIoU<BR>  Semantic segmentation - ScanNet: 3DIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-10-01<BR>ratio: 0.4727<BR>benchmarks:<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>  Semantic segmentation - Semantic3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-11-01<BR>ratio: 0.8126<BR>benchmarks:<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - S3DIS Area5: mIoU<BR>  Semantic segmentation - S3DIS: mAcc<BR>  Semantic segmentation - ScanNet: 3DIoU<BR>  Semantic segmentation - Semantic3D: mIoU<BR>  Semantic segmentation - ShapeNet: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-12-01<BR>ratio: 0.0779<BR>benchmarks:<BR>  Semantic segmentation - ScanNet: 3DIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-02-01<BR>ratio: 0.727<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - DADA-seg: mIoU<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Semantic segmentation - SkyScapes-Dense: Mean IoU<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-03-01<BR>ratio: 0.1374<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - ScanNetV2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-04-01<BR>ratio: 0.4539<BR>benchmarks:<BR>  Semantic segmentation - LIP val: mIoU<BR>  Semantic segmentation - PASCAL VOC 2012 val: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-06-01<BR>ratio: 0.7333<BR>benchmarks:<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - CamVid: Mean IoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - RSMSS: mIoU<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-07-01<BR>ratio: 0.2139<BR>benchmarks:<BR>  Semantic segmentation - CamVid: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-08-01<BR>ratio: 0.3657<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - ScanNetV2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-09-01<BR>ratio: 0.146<BR>benchmarks:<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - DADA-seg: mIoU<BR>  Semantic segmentation - LIP val: mIoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-12-01<BR>ratio: 0.3234<BR>benchmarks:<BR>  Semantic segmentation - CamVid: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-01-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - Nighttime Driving: mIoU<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-02-01<BR>ratio: 0.0683<BR>benchmarks:<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-03-01<BR>ratio: 0.0229<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-04-01<BR>ratio: 0.7456<BR>benchmarks:<BR>  Semantic segmentation - DADA-seg: mIoU<BR>  Semantic segmentation - LIP val: mIoU<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - S3DIS Area5: mIoU<BR>  Semantic segmentation - S3DIS Area5: oAcc<BR>  Semantic segmentation - S3DIS: Mean IoU<BR>  Semantic segmentation - S3DIS: mAcc<BR>  Semantic segmentation - S3DIS: oAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2019-05-01<BR>ratio: 0.0652<BR>benchmarks:<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-06-01<BR>ratio: 0.1791<BR>benchmarks:<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - ScanNetV2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-07-01<BR>ratio: 0.2422<BR>benchmarks:<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-08-01<BR>ratio: 0.5844<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - SynPASS: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-09-01<BR>ratio: 0.2226<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - Cityscapes test: Mean IoU (class)<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - LIP val: mIoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-10-01<BR>ratio: 0.273<BR>benchmarks:<BR>  Semantic segmentation - LIP val: mIoU<BR>  Semantic segmentation - SkyScapes-Dense: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-11-01<BR>ratio: 0.95<BR>benchmarks:<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - S3DIS: mAcc<BR>  Semantic segmentation - Semantic3D: mIoU<BR>  Semantic segmentation - Semantic3D: oAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2019-12-01<BR>ratio: 0.6351<BR>benchmarks:<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>  Semantic segmentation - Stanford2D3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-02-01<BR>ratio: 0.0364<BR>benchmarks:<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-03-01<BR>ratio: 0.4148<BR>benchmarks:<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-04-01<BR>ratio: 0.2667<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - RSMSS: mIoU<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-05-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - Nighttime Driving: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-06-01<BR>ratio: 0.3636<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: Pixel Accuracy<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Semantic segmentation - PASCAL VOC 2012 val: mIoU<BR>  Semantic segmentation - SUN-RGBD: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-07-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Semantic segmentation - EventScape: mIoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-08-01<BR>ratio: 0.0199<BR>benchmarks:<BR>  Semantic segmentation - DADA-seg: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-11-01<BR>ratio: 0.5806<BR>benchmarks:<BR>  Semantic segmentation - S3DIS Area5: oAcc<BR>  Semantic segmentation - Stanford2D3D Panoramic: mIoU<BR>  Semantic segmentation - ZJU-RGB-P: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-12-01<BR>ratio: 0.6109<BR>benchmarks:<BR>  Semantic segmentation - DADA-seg: mIoU<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - S3DIS Area5: mIoU<BR>  Semantic segmentation - S3DIS Area5: oAcc<BR>  Semantic segmentation - S3DIS: Mean IoU<BR>  Semantic segmentation - S3DIS: mAcc<BR>  Semantic segmentation - S3DIS: oAcc<BR>  Semantic segmentation - Semantic3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-01-01<BR>ratio: 0.2542<BR>benchmarks:<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-03-01<BR>ratio: 0.8464<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: Pixel Accuracy<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Test Score<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - S3DIS: mAcc<BR>  Semantic segmentation - Semantic3D: oAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2021-04-01<BR>ratio: 0.0522<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-05-01<BR>ratio: 0.9533<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: Pixel Accuracy<BR>  Semantic segmentation - EventScape: mIoU<BR>  Semantic segmentation - SynPASS: mIoU<BR>  Semantic segmentation - ZJU-RGB-P: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-06-01<BR>ratio: 0.202<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - UAVid: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-07-01<BR>ratio: 0.2763<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: Pixel Accuracy<BR>  Semantic segmentation - Cityscapes val: mIoU<BR>  Semantic segmentation - Trans10K: GFLOPs<BR>  Semantic segmentation - Trans10K: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-08-01<BR>ratio: 0.4744<BR>benchmarks:<BR>  Semantic segmentation - DADA-seg: mIoU<BR>  Semantic segmentation - Stanford2D3D: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-09-01<BR>ratio: 0.798<BR>benchmarks:<BR>  Semantic segmentation - UAVid: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-10-01<BR>ratio: 0.3684<BR>benchmarks:<BR>  Semantic segmentation - ScanNet: 3DIoU<BR>  Semantic segmentation - ShapeNet: Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-11-01<BR>ratio: 0.1715<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>  Semantic segmentation - ADE20K: Validation mIoU<BR>  Semantic segmentation - COCO-Stuff test: mIoU<BR>  Semantic segmentation - DensePASS: mIoU<BR>  Semantic segmentation - NYU Depth v2: Mean IoU<BR>  Semantic segmentation - PASCAL Context: mIoU<BR>  Semantic segmentation - S3DIS Area5: mAcc<BR>  Semantic segmentation - S3DIS Area5: mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2021-12-01<BR>ratio: 0.0629<BR>benchmarks:<BR>  Semantic segmentation - ADE20K val: mIoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2017-11-01<BR>ratio: 0.1772<BR>benchmarks:<BR>  Text style transfer - Yelp Review Dataset (Small): G-Score (BLEU, Accuracy)<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2018-04-01<BR>ratio: 0.268<BR>benchmarks:<BR>  Text style transfer - Yelp Review Dataset (Small): G-Score (BLEU, Accuracy)<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2019-07-01<BR>ratio: 0.2679<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-5 - PASCAL VOC 2012: Mean IoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2019-08-01<BR>ratio: 0.5549<BR>benchmarks:<BR>  Text style transfer - Yelp Review Dataset (Small): G-Score (BLEU, Accuracy)<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2020-02-01<BR>ratio: 0.5057<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-5 - PASCAL VOC 2012: Mean IoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2020-04-01<BR>ratio: 0.8799<BR>benchmarks:<BR>  2D Semantic Segmentation - xBD: Weighted Average F1-score<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2020-10-01<BR>ratio: 0.1165<BR>benchmarks:<BR>  2D Semantic Segmentation - xBD: Weighted Average F1-score<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2020-11-01<BR>ratio: 0.1043<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2021-03-01<BR>ratio: 0.1199<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-5 - PASCAL VOC 2012: Mean IoU<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2021-05-01<BR>ratio: 0.0037<BR>benchmarks:<BR>  2D Semantic Segmentation - xBD: Weighted Average F1-score<BR>","<BR>task: Semantic segmentation // 2D Semantic Segmentation<BR>date: 2021-06-01<BR>ratio: 0.7967<BR>benchmarks:<BR>  Disjoint 10-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-1 - PASCAL VOC 2012: mIoU<BR>  Disjoint 15-5 - PASCAL VOC 2012: Mean IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2016-12-01<BR>ratio: 0.0169<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2017-06-01<BR>ratio: 0.9341<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (A)<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (A)<BR>  3D part segmentation - IntrA: IoU (V)<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2018-01-01<BR>ratio: 0.4561<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (V)<BR>  3D part segmentation - ShapeNet-Part: Class Average IoU<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2018-03-01<BR>ratio: 0.1125<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (A)<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (A)<BR>  3D part segmentation - IntrA: IoU (V)<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2018-06-01<BR>ratio: 0.0441<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2018-11-01<BR>ratio: 0.0097<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (V)<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2019-02-01<BR>ratio: 0.0678<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2019-04-01<BR>ratio: 0.0877<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Class Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2020-03-01<BR>ratio: 0.339<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2020-12-01<BR>ratio: 0.4561<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Class Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2021-01-01<BR>ratio: 0.2881<BR>benchmarks:<BR>  3D part segmentation - ShapeNet-Part: Instance Average IoU<BR>","<BR>task: Semantic segmentation // 3D part segmentation<BR>date: 2021-12-01<BR>ratio: 0.0237<BR>benchmarks:<BR>  3D part segmentation - IntrA: DSC (A)<BR>  3D part segmentation - IntrA: DSC (V)<BR>  3D part segmentation - IntrA: IoU (A)<BR>  3D part segmentation - IntrA: IoU (V)<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2017-06-01<BR>ratio: 0.4678<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2017-10-01<BR>ratio: 0.1673<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2018-07-01<BR>ratio: 0.1139<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2018-09-01<BR>ratio: 0.0676<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2019-04-01<BR>ratio: 0.5322<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2019-10-01<BR>ratio: 0.108<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2020-03-01<BR>ratio: 0.0125<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2020-07-01<BR>ratio: 0.4943<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2020-08-01<BR>ratio: 0.3977<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2020-11-01<BR>ratio: 0.0445<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // 3D semantic segmentation<BR>date: 2021-02-01<BR>ratio: 0.0338<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-03-01<BR>ratio: 0.4292<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-09-01<BR>ratio: 0.1143<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-10-01<BR>ratio: 0.0377<BR>benchmarks:<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2019-12-01<BR>ratio: 0.4169<BR>benchmarks:<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2020-03-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2020-07-01<BR>ratio: 0.6116<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2020-08-01<BR>ratio: 0.1792<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2020-12-01<BR>ratio: 0.7059<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2021-04-01<BR>ratio: 0.5111<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (10-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - FSS-1000: Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (10-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Few-shot semantic segmentation<BR>date: 2021-12-01<BR>ratio: 0.0613<BR>benchmarks:<BR>  Few-shot semantic segmentation - COCO-20i (1-shot): Mean IoU<BR>  Few-shot semantic segmentation - COCO-20i (5-shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (1-Shot): Mean IoU<BR>  Few-shot semantic segmentation - PASCAL-5i (5-Shot): Mean IoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2017-08-01<BR>ratio: 0.4483<BR>benchmarks:<BR>  Human part segmentation - PASCAL-Part: mIoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2018-05-01<BR>ratio: 0.2101<BR>benchmarks:<BR>  Human part segmentation - PASCAL-Part: mIoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2018-09-01<BR>ratio: 0.2448<BR>benchmarks:<BR>  Human part segmentation - PASCAL-Part: mIoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2018-11-01<BR>ratio: 0.4542<BR>benchmarks:<BR>  Human part segmentation - CIHP: Mean IoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2019-07-01<BR>ratio: 0.0969<BR>benchmarks:<BR>  Human part segmentation - PASCAL-Part: mIoU<BR>","<BR>task: Semantic segmentation // Human part segmentation<BR>date: 2019-10-01<BR>ratio: 0.5458<BR>benchmarks:<BR>  Human part segmentation - CIHP: Mean IoU<BR>","<BR>task: Semantic segmentation // LIDAR semantic segmentation<BR>date: 2019-04-01<BR>ratio: 0.8685<BR>benchmarks:<BR>  LIDAR semantic segmentation - Paris-Lille-3D: mIOU<BR>","<BR>task: Semantic segmentation // LIDAR semantic segmentation<BR>date: 2020-04-01<BR>ratio: 0.1315<BR>benchmarks:<BR>  LIDAR semantic segmentation - Paris-Lille-3D: mIOU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2018-12-01<BR>ratio: 0.8012<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>  Panoptic segmentation - Cityscapes val: AP<BR>  Panoptic segmentation - Cityscapes val: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-01-01<BR>ratio: 0.178<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Cityscapes val: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-05-01<BR>ratio: 0.6053<BR>benchmarks:<BR>  Panoptic segmentation - Indian Driving Dataset: PQ<BR>  Panoptic segmentation - KITTI Panoptic Segmentation: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-09-01<BR>ratio: 0.4944<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Cityscapes val: PQth<BR>  Panoptic segmentation - Mapillary val: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-10-01<BR>ratio: 0.0655<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2019-11-01<BR>ratio: 0.771<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes test: PQ<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Cityscapes val: mIoU<BR>  Panoptic segmentation - Mapillary val: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-03-01<BR>ratio: 0.5238<BR>benchmarks:<BR>  Panoptic segmentation - Cityscapes test: PQ<BR>  Panoptic segmentation - Cityscapes val: AP<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Cityscapes val: mIoU<BR>  Panoptic segmentation - Mapillary val: PQ<BR>  Panoptic segmentation - Mapillary val: mIoU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-04-01<BR>ratio: 0.9286<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: PQth<BR>  Panoptic segmentation - Cityscapes test: PQ<BR>  Panoptic segmentation - Cityscapes val: PQst<BR>  Panoptic segmentation - Cityscapes val: PQth<BR>  Panoptic segmentation - Cityscapes val: mIoU<BR>  Panoptic segmentation - Indian Driving Dataset: PQ<BR>  Panoptic segmentation - KITTI Panoptic Segmentation: PQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-06-01<BR>ratio: 0.0826<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-11-01<BR>ratio: 0.7973<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - Cityscapes test: PQ<BR>  Panoptic segmentation - Cityscapes val: AP<BR>  Panoptic segmentation - Cityscapes val: PQ<BR>  Panoptic segmentation - Mapillary val: PQ<BR>  Panoptic segmentation - Mapillary val: PQth<BR>  Panoptic segmentation - Mapillary val: mIoU<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2020-12-01<BR>ratio: 0.7625<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: PQth<BR>  Panoptic segmentation - COCO minival: RQ<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>  Panoptic segmentation - Cityscapes val: PQst<BR>  Panoptic segmentation - Mapillary val: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2021-06-01<BR>ratio: 0.161<BR>benchmarks:<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2021-07-01<BR>ratio: 0.2375<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: RQ<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2021-09-01<BR>ratio: 0.2566<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: PQth<BR>  Panoptic segmentation - COCO test-dev: PQ<BR>  Panoptic segmentation - COCO test-dev: PQst<BR>  Panoptic segmentation - COCO test-dev: PQth<BR>","<BR>task: Semantic segmentation // Panoptic segmentation<BR>date: 2021-12-01<BR>ratio: 0.1603<BR>benchmarks:<BR>  Panoptic segmentation - COCO minival: PQ<BR>  Panoptic segmentation - COCO minival: PQst<BR>  Panoptic segmentation - COCO minival: PQth<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2015-02-01<BR>ratio: 0.8263<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2015-11-01<BR>ratio: 0.1917<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2016-06-01<BR>ratio: 0.4008<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2016-11-01<BR>ratio: 0.5029<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2016-12-01<BR>ratio: 0.0906<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Frame (fps)<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2017-04-01<BR>ratio: 0.7499<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Frame (fps)<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2018-08-01<BR>ratio: 0.1676<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2018-09-01<BR>ratio: 0.0347<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2018-11-01<BR>ratio: 0.0058<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2019-03-01<BR>ratio: 0.0405<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2019-09-01<BR>ratio: 0.0231<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2020-02-01<BR>ratio: 0.2601<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2020-04-01<BR>ratio: 0.8085<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Frame (fps)<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>  Real-time semantic segmentation - Cityscapes test: Time (ms)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2020-12-01<BR>ratio: 0.0311<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2021-01-01<BR>ratio: 0.0777<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: Time (ms)<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2021-04-01<BR>ratio: 0.1731<BR>benchmarks:<BR>  Real-time semantic segmentation - Cityscapes test: Frame (fps)<BR>","<BR>task: Semantic segmentation // Real-time semantic segmentation<BR>date: 2021-11-01<BR>ratio: 0.0155<BR>benchmarks:<BR>  Real-time semantic segmentation - CamVid: mIoU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2016-12-01<BR>ratio: 0.098<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2019-05-01<BR>ratio: 0.0196<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2019-06-01<BR>ratio: 0.2647<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2019-08-01<BR>ratio: 0.2647<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2021-05-01<BR>ratio: 0.3039<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Scene segmentation<BR>date: 2021-10-01<BR>ratio: 0.049<BR>benchmarks:<BR>  Thermal Image Segmentation - MFN Dataset: mIOU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2019-06-01<BR>ratio: 0.7781<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2020-04-01<BR>ratio: 0.8627<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 100 samples labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 1% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2020-07-01<BR>ratio: 0.7231<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 50% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2020-12-01<BR>ratio: 0.5321<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 100 samples labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 25% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-03-01<BR>ratio: 0.4521<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 2% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-04-01<BR>ratio: 0.8553<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 100 samples labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 50% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 1% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 2% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-06-01<BR>ratio: 0.8206<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - Cityscapes 12.5% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Cityscapes 50% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 12.5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-11-01<BR>ratio: 0.7573<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 50%: Validation mIoU<BR>  Semi-supervised semantic segmentation - Pascal VOC 2012 12.5% labeled: Validation mIoU<BR>","<BR>task: Semantic segmentation // Semi-supervised semantic segmentation<BR>date: 2021-12-01<BR>ratio: 0.2427<BR>benchmarks:<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 25% labeled: Validation mIoU<BR>  Semi-supervised semantic segmentation - PASCAL VOC 2012 50%: Validation mIoU<BR>","<BR>task: Semantic segmentation // Unsupervised semantic segmentation<BR>date: 2020-07-01<BR>ratio: 0.2793<BR>benchmarks:<BR>  Unsupervised semantic segmentation - COCO-Stuff: Pixel Accuracy<BR>","<BR>task: Semantic segmentation // Unsupervised semantic segmentation<BR>date: 2021-03-01<BR>ratio: 0.0613<BR>benchmarks:<BR>  Unsupervised semantic segmentation - COCO-Stuff: Pixel Accuracy<BR>","<BR>task: Semantic segmentation // Unsupervised semantic segmentation<BR>date: 2021-10-01<BR>ratio: 0.6595<BR>benchmarks:<BR>  Unsupervised semantic segmentation - COCO-Stuff: Pixel Accuracy<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2019-10-01<BR>ratio: 0.4286<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - COCO 2014 val: mIoU<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 val: Mean IoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2020-09-01<BR>ratio: 0.2738<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 val: Mean IoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2020-12-01<BR>ratio: 0.1333<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2021-01-01<BR>ratio: 0.4933<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 val: Mean IoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2021-05-01<BR>ratio: 0.118<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - COCO 2014 val: mIoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2021-08-01<BR>ratio: 0.2528<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - COCO 2014 val: mIoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2021-10-01<BR>ratio: 0.2022<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - COCO 2014 val: mIoU<BR>","<BR>task: Semantic segmentation // Weakly-supervised semantic segmentation<BR>date: 2021-12-01<BR>ratio: 0.0133<BR>benchmarks:<BR>  Weakly-supervised semantic segmentation - PASCAL VOC 2012 test: Mean IoU<BR>","<BR>task: Semi-supervised object detection<BR>date: 2021-02-01<BR>ratio: 0.3684<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 100% labeled data: mAP<BR>","<BR>task: Semi-supervised object detection<BR>date: 2021-03-01<BR>ratio: 0.7809<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 1% labeled data: mAP<BR>  Semi-supervised object detection - COCO 10% labeled data: mAP<BR>  Semi-supervised object detection - COCO 2% labeled data: mAP<BR>  Semi-supervised object detection - COCO 5% labeled data: mAP<BR>","<BR>task: Semi-supervised object detection<BR>date: 2021-06-01<BR>ratio: 0.6316<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 1% labeled data: mAP<BR>  Semi-supervised object detection - COCO 10% labeled data: mAP<BR>  Semi-supervised object detection - COCO 100% labeled data: mAP<BR>  Semi-supervised object detection - COCO 5% labeled data: mAP<BR>","<BR>task: Semi-supervised object detection<BR>date: 2021-11-01<BR>ratio: 0.2191<BR>benchmarks:<BR>  Semi-supervised object detection - COCO 1% labeled data: mAP<BR>  Semi-supervised object detection - COCO 2% labeled data: mAP<BR>","<BR>task: Sign language recognition<BR>date: 2020-05-01<BR>ratio: 0.8267<BR>benchmarks:<BR>  Sign language recognition - RWTH-PHOENIX-Weather 2014: Word Error Rate (WER)<BR>","<BR>task: Sign language recognition<BR>date: 2020-07-01<BR>ratio: 0.5463<BR>benchmarks:<BR>  Sign language recognition - WLASL-2000: Top-1 Accuracy<BR>","<BR>task: Sign language recognition<BR>date: 2021-01-01<BR>ratio: 0.1733<BR>benchmarks:<BR>  Sign language recognition - RWTH-PHOENIX-Weather 2014: Word Error Rate (WER)<BR>","<BR>task: Sign language recognition<BR>date: 2021-03-01<BR>ratio: 0.4537<BR>benchmarks:<BR>  Sign language recognition - WLASL-2000: Top-1 Accuracy<BR>","<BR>task: Sign language translation<BR>date: 2020-04-01<BR>ratio: 0.97<BR>benchmarks:<BR>  Sign language translation - RWTH-PHOENIX-Weather 2014 T: BLEU-4<BR>","<BR>task: Sign language translation<BR>date: 2021-09-01<BR>ratio: 0.03<BR>benchmarks:<BR>  Sign language translation - RWTH-PHOENIX-Weather 2014 T: BLEU-4<BR>","<BR>task: Single-object discovery<BR>date: 2020-07-01<BR>ratio: 0.7668<BR>benchmarks:<BR>  Single-object discovery - COCO_20k: CorLoc<BR>","<BR>task: Single-object discovery<BR>date: 2021-09-01<BR>ratio: 0.2332<BR>benchmarks:<BR>  Single-object discovery - COCO_20k: CorLoc<BR>","<BR>task: Super-resolution<BR>date: 2015-08-01<BR>ratio: 0.1475<BR>benchmarks:<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2015-11-01<BR>ratio: 0.5128<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: PSNR 4x T2w<BR>  Super-resolution - IXI: SSIM 4x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>","<BR>task: Super-resolution<BR>date: 2016-03-01<BR>ratio: 0.0844<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2016-06-01<BR>ratio: 0.2222<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2016-08-01<BR>ratio: 0.5294<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2016-09-01<BR>ratio: 0.2727<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2016-11-01<BR>ratio: 0.2563<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2016-12-01<BR>ratio: 0.3015<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2017-04-01<BR>ratio: 0.1761<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2017-07-01<BR>ratio: 0.8641<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2017-12-01<BR>ratio: 0.612<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Super-resolution<BR>date: 2018-01-01<BR>ratio: 0.4027<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2018-02-01<BR>ratio: 0.2253<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: PSNR 4x T2w<BR>  Super-resolution - IXI: SSIM 4x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>","<BR>task: Super-resolution<BR>date: 2018-03-01<BR>ratio: 0.1868<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>","<BR>task: Super-resolution<BR>date: 2018-04-01<BR>ratio: 0.764<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>  Image super-resolution - WebFace - 8x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2018-05-01<BR>ratio: 0.4188<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2018-06-01<BR>ratio: 0.3649<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2018-07-01<BR>ratio: 0.0656<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2018-09-01<BR>ratio: 0.9448<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - PIRM-test: NIQE<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Super-resolution<BR>date: 2018-10-01<BR>ratio: 0.2179<BR>benchmarks:<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: PSNR 4x T2w<BR>  Super-resolution - IXI: SSIM 4x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>","<BR>task: Super-resolution<BR>date: 2018-11-01<BR>ratio: 0.0041<BR>benchmarks:<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2018-12-01<BR>ratio: 0.5506<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Super-resolution<BR>date: 2019-02-01<BR>ratio: 0.039<BR>benchmarks:<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2019-03-01<BR>ratio: 0.8519<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Super-resolution<BR>date: 2019-04-01<BR>ratio: 0.5217<BR>benchmarks:<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-05-01<BR>ratio: 0.0133<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-06-01<BR>ratio: 0.961<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 2x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 3x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 3x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>  Image super-resolution - VggFace2 - 8x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-07-01<BR>ratio: 0.7658<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 4x upscaling: PSNR<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 2x upscaling: PSNR<BR>  Image super-resolution - Set5 - 2x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 2x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 2x upscaling: SSIM<BR>  Image super-resolution - Urban100 - 4x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2019-08-01<BR>ratio: 0.2<BR>benchmarks:<BR>  Image super-resolution - PIRM-test: NIQE<BR>","<BR>task: Super-resolution<BR>date: 2019-09-01<BR>ratio: 0.7778<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Image super-resolution - BSD100 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: SSIM<BR>  Image super-resolution - Set5 - 4x upscaling: SSIM<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-10-01<BR>ratio: 0.8359<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: SSIM<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: SSIM<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-11-01<BR>ratio: 0.8468<BR>benchmarks:<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2019-12-01<BR>ratio: 0.7143<BR>benchmarks:<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>","<BR>task: Super-resolution<BR>date: 2020-01-01<BR>ratio: 0.6304<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>","<BR>task: Super-resolution<BR>date: 2020-03-01<BR>ratio: 0.3407<BR>benchmarks:<BR>  Image super-resolution - Urban100 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2020-05-01<BR>ratio: 0.8441<BR>benchmarks:<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 1024 x 1024 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 256 x 256 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FED<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: FID<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: LPIPS<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: Ms-ssim<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: NIQE<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: PSNR<BR>  Image super-resolution - FFHQ 512 x 512 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2020-06-01<BR>ratio: 0.9497<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Super-resolution<BR>date: 2020-07-01<BR>ratio: 0.9937<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>","<BR>task: Super-resolution<BR>date: 2020-08-01<BR>ratio: 0.0465<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 3x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2020-11-01<BR>ratio: 0.2529<BR>benchmarks:<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2020-12-01<BR>ratio: 0.5413<BR>benchmarks:<BR>  Image super-resolution - BSD100 - 2x upscaling: PSNR<BR>  Image super-resolution - Set14 - 3x upscaling: PSNR<BR>  Image super-resolution - Urban100 - 3x upscaling: PSNR<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2021-05-01<BR>ratio: 0.2352<BR>benchmarks:<BR>  Multi-frame super-resolution - PROBA-V: Normalized cPSNR<BR>","<BR>task: Super-resolution<BR>date: 2021-06-01<BR>ratio: 0.8333<BR>benchmarks:<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Flickr1024 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2012 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 2x upscaling: PSNR<BR>  Stereo Image Super-Resolution - KITTI2015 - 4x upscaling: PSNR<BR>  Stereo Image Super-Resolution - Middlebury - 4x upscaling: PSNR<BR>  Super-resolution - IXI: PSNR 2x T2w<BR>  Super-resolution - IXI: PSNR 4x T2w<BR>  Super-resolution - IXI: SSIM 4x T2w<BR>  Super-resolution - IXI: SSIM for 2x T2w<BR>","<BR>task: Super-resolution<BR>date: 2021-08-01<BR>ratio: 0.2632<BR>benchmarks:<BR>  Audio super-resolution - Piano: Log-Spectral Distance<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Audio super-resolution - Voice Bank corpus (VCTK): Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: LPIPS<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: PSNR<BR>  Image super-resolution - Manga109 - 4x upscaling: SSIM<BR>  Image super-resolution - Set14 - 4x upscaling: PSNR<BR>  Image super-resolution - Set5 - 4x upscaling: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2021-10-01<BR>ratio: 0.2308<BR>benchmarks:<BR>  Audio super-resolution - VCTK Multi-Speaker: Log-Spectral Distance<BR>  Burst Image Super-Resolution - BurstSR: PSNR<BR>","<BR>task: Super-resolution<BR>date: 2021-11-01<BR>ratio: 0.028<BR>benchmarks:<BR>  Image super-resolution - Set5 - 3x upscaling: PSNR<BR>","<BR>task: Surface normals estimation<BR>date: 2019-04-01<BR>ratio: 0.9344<BR>benchmarks:<BR>  Surface normals estimation - PCPNet: RMSE<BR>","<BR>task: Surface normals estimation<BR>date: 2020-03-01<BR>ratio: 0.0656<BR>benchmarks:<BR>  Surface normals estimation - PCPNet: RMSE<BR>","<BR>task: Text based person retrieval<BR>date: 2018-09-01<BR>ratio: 0.2824<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>","<BR>task: Text based person retrieval<BR>date: 2019-06-01<BR>ratio: 0.5079<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Text based person retrieval<BR>date: 2020-03-01<BR>ratio: 0.1746<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Text based person retrieval<BR>date: 2020-10-01<BR>ratio: 0.1454<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Text based person retrieval<BR>date: 2021-01-01<BR>ratio: 0.1415<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Text based person retrieval<BR>date: 2021-05-01<BR>ratio: 0.1863<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-10<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>  Text based person retrieval - CUHK-PEDES: R-at-5<BR>","<BR>task: Text based person retrieval<BR>date: 2021-10-01<BR>ratio: 0.022<BR>benchmarks:<BR>  Text based person retrieval - CUHK-PEDES: R-at-1<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2017-01-01<BR>ratio: 0.0344<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2018-01-01<BR>ratio: 0.9286<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2018-02-01<BR>ratio: 0.9509<BR>benchmarks:<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2020-07-01<BR>ratio: 0.4883<BR>benchmarks:<BR>  Abnormal event detection in video - UBI-Fights: AUC<BR>  Abnormal event detection in video - UBI-Fights: Decidability<BR>  Abnormal event detection in video - UBI-Fights: EER<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2020-08-01<BR>ratio: 0.0491<BR>benchmarks:<BR>  Anomaly detection in surveillance videos - UCF-Crime: ROC AUC<BR>","<BR>task: Video process // Abnormal event detection in video<BR>date: 2021-01-01<BR>ratio: 0.3527<BR>benchmarks:<BR>  Semi-supervised anomaly detection - UBI-Fights: AUC<BR>  Semi-supervised anomaly detection - UBI-Fights: Decidability<BR>  Semi-supervised anomaly detection - UBI-Fights: EER<BR>","<BR>task: Video process // Action classification<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-06-01<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Action classification<BR>date: 2016-12-01<BR>ratio: 0.0797<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>","<BR>task: Video process // Action classification<BR>date: 2017-03-01<BR>ratio: 0.1754<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2017-04-01<BR>ratio: 0.2083<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-05-01<BR>ratio: 0.6114<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Action classification - Toyota Smarthome dataset: CS<BR>  Action classification - Toyota Smarthome dataset: CV1<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-06-01<BR>ratio: 0.0467<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-10-01<BR>ratio: 0.0044<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2017-11-01<BR>ratio: 0.3438<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>  Action classification - Toyota Smarthome dataset: CS<BR>","<BR>task: Video process // Action classification<BR>date: 2017-12-01<BR>ratio: 0.0156<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@5<BR>","<BR>task: Video process // Action classification<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Video process // Action classification<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Video process // Action classification<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-07-01<BR>ratio: 0.7234<BR>benchmarks:<BR>  Action classification - THUMOS\u201914: mAP<BR>","<BR>task: Video process // Action classification<BR>date: 2018-11-01<BR>ratio: 0.138<BR>benchmarks:<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2018-12-01<BR>ratio: 0.4719<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Action classification<BR>date: 2019-05-01<BR>ratio: 0.3438<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Moments in Time: Top 5 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2019-06-01<BR>ratio: 0.3793<BR>benchmarks:<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Video process // Action classification<BR>date: 2019-08-01<BR>ratio: 0.2766<BR>benchmarks:<BR>  Action classification - THUMOS\u201914: mAP<BR>","<BR>task: Video process // Action classification<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Video process // Action classification<BR>date: 2019-10-01<BR>ratio: 0.1259<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CS<BR>  Action classification - Toyota Smarthome dataset: CV1<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>","<BR>task: Video process // Action classification<BR>date: 2019-11-01<BR>ratio: 0.016<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2020-01-01<BR>ratio: 0.0774<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>","<BR>task: Video process // Action classification<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2020-07-01<BR>ratio: 0.3755<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CS<BR>  Action classification - Toyota Smarthome dataset: CV1<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2020-08-01<BR>ratio: 0.0962<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CS<BR>","<BR>task: Video process // Action classification<BR>date: 2020-10-01<BR>ratio: 0.0013<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2021-02-01<BR>ratio: 0.1887<BR>benchmarks:<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Moments in Time: Top 5 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2021-03-01<BR>ratio: 0.6617<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Action classification - Kinetics-400: Acc@5<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>  Action classification - Kinetics-700: Top-1 Accuracy<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>","<BR>task: Video process // Action classification<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>  Action classification - Kinetics-700: Top-5 Accuracy<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Moments in Time: Top 5 Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Video process // Action classification<BR>date: 2021-05-01<BR>ratio: 0.2543<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CS<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2021-06-01<BR>ratio: 0.2414<BR>benchmarks:<BR>  Action classification - Charades: MAP<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Action classification<BR>date: 2021-07-01<BR>ratio: 0.282<BR>benchmarks:<BR>  Action classification - Toyota Smarthome dataset: CV2<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2021-09-01<BR>ratio: 0.0625<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@5<BR>","<BR>task: Video process // Action classification<BR>date: 2021-11-01<BR>ratio: 0.2069<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-400: Acc@5<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>","<BR>task: Video process // Action classification<BR>date: 2021-12-01<BR>ratio: 0.3981<BR>benchmarks:<BR>  Action classification - Kinetics-400: Acc@1<BR>  Action classification - Kinetics-600: Top-1 Accuracy<BR>  Action classification - Kinetics-600: Top-5 Accuracy<BR>  Action classification - Kinetics-700: Top-1 Accuracy<BR>  Action classification - Kinetics-700: Top-5 Accuracy<BR>  Action classification - Moments in Time: Top 1 Accuracy<BR>  Action classification - Moments in Time: Top 5 Accuracy<BR>","<BR>task: Video process // Action spotting<BR>date: 2019-12-01<BR>ratio: 0.5039<BR>benchmarks:<BR>  Action spotting - SoccerNet: Average-mAP<BR>","<BR>task: Video process // Action spotting<BR>date: 2021-02-01<BR>ratio: 0.4961<BR>benchmarks:<BR>  Action spotting - SoccerNet: Average-mAP<BR>","<BR>task: Video process // Activity recognition in videos<BR>date: 2014-12-01<BR>ratio: 0.6093<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Video process // Activity recognition in videos<BR>date: 2015-05-01<BR>ratio: 0.1674<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Video process // Activity recognition in videos<BR>date: 2016-05-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  Activity recognition in videos - DogCentric: Accuracy<BR>","<BR>task: Video process // Object tracking<BR>date: 2015-04-01<BR>ratio: 0.3149<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2016-08-01<BR>ratio: 0.0433<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2016-11-01<BR>ratio: 0.0835<BR>benchmarks:<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-04-01<BR>ratio: 0.1489<BR>benchmarks:<BR>  Visual object tracking - OTB-2013: AUC<BR>  Visual object tracking - OTB-50: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-05-01<BR>ratio: 0.0459<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-06-01<BR>ratio: 0.259<BR>benchmarks:<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2017-10-01<BR>ratio: 0.6164<BR>benchmarks:<BR>  Visual object tracking - OTB-2013: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-02-01<BR>ratio: 0.8511<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>  Visual object tracking - OTB-2013: AUC<BR>  Visual object tracking - OTB-2015: AUC<BR>  Visual object tracking - OTB-50: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-03-01<BR>ratio: 0.1325<BR>benchmarks:<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-04-01<BR>ratio: 0.0131<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-06-01<BR>ratio: 0.2289<BR>benchmarks:<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-11-01<BR>ratio: 0.5471<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>  Multi-object tracking - MOT17: MOTA<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2018-12-01<BR>ratio: 0.1867<BR>benchmarks:<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-01-01<BR>ratio: 0.2761<BR>benchmarks:<BR>  Visual object tracking - VOT2017: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-02-01<BR>ratio: 0.0179<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-03-01<BR>ratio: 0.3843<BR>benchmarks:<BR>  Online multi-object tracking - MOT16: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-04-01<BR>ratio: 0.5646<BR>benchmarks:<BR>  Visual object tracking - GOT-10k: Average Overlap<BR>  Visual object tracking - GOT-10k: Success Rate 0.5<BR>  Visual object tracking - LaSOT: AUC<BR>  Visual object tracking - LaSOT: Normalized Precision<BR>  Visual object tracking - LaSOT: Precision<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-06-01<BR>ratio: 0.2318<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>  Multi-object tracking - MOT17: MOTA<BR>  Visual object tracking - OTB-2015: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-07-01<BR>ratio: 0.7239<BR>benchmarks:<BR>  Visual object tracking - VOT2017/18: Expected Average Overlap (EAO)<BR>  Visual object tracking - VOT2017: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-09-01<BR>ratio: 0.3148<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2019-11-01<BR>ratio: 0.4255<BR>benchmarks:<BR>  Visual object tracking - GOT-10k: Average Overlap<BR>  Visual object tracking - GOT-10k: Success Rate 0.5<BR>  Visual object tracking - LaSOT: AUC<BR>  Visual object tracking - LaSOT: Normalized Precision<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-02-01<BR>ratio: 0.0502<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-04-01<BR>ratio: 0.6897<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>  Multi-object tracking - MOT17: MOTA<BR>  Multi-object tracking - MOT20: IDF1<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-06-01<BR>ratio: 0.6067<BR>benchmarks:<BR>  Multi-object tracking - DanceTrack: AssA<BR>  Multi-object tracking - DanceTrack: HOTA<BR>  Multi-object tracking - DanceTrack: IDF1<BR>  Visual object tracking - VOT2019: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-07-01<BR>ratio: 0.28<BR>benchmarks:<BR>  Multi-object tracking - MOT16: MOTA<BR>  Visual object tracking - OTB-2013: AUC<BR>  Visual object tracking - OTB-2015: AUC<BR>  Visual object tracking - UAV123: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-11-01<BR>ratio: 0.034<BR>benchmarks:<BR>  Multiple object tracking - KITTI Tracking test: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2020-12-01<BR>ratio: 0.0806<BR>benchmarks:<BR>  Visual object tracking - LaSOT: Precision<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-01-01<BR>ratio: 0.0138<BR>benchmarks:<BR>  Multi-object tracking - MOT17: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-03-01<BR>ratio: 0.72<BR>benchmarks:<BR>  Multi-object tracking - MOTS20: IDF1<BR>  Online multi-object tracking - MOT16: MOTA<BR>  Visual object tracking - GOT-10k: Average Overlap<BR>  Visual object tracking - GOT-10k: Success Rate 0.5<BR>  Visual object tracking - LaSOT: AUC<BR>  Visual object tracking - LaSOT: Normalized Precision<BR>  Visual object tracking - LaSOT: Precision<BR>  Visual object tracking - OTB-2015: AUC<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - UAV123: AUC<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-04-01<BR>ratio: 0.9674<BR>benchmarks:<BR>  Multi-object tracking - MOT17: IDF1<BR>  Multi-object tracking - MOT17: MOTA<BR>  Multi-object tracking - MOT20: IDF1<BR>  Multi-object tracking - MOT20: MOTA<BR>  Visual object tracking - OTB-2015: AUC<BR>  Visual object tracking - VOT2019: Expected Average Overlap (EAO)<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-05-01<BR>ratio: 0.4091<BR>benchmarks:<BR>  Multi-object tracking - DanceTrack: AssA<BR>  Multi-object tracking - DanceTrack: HOTA<BR>  Multi-object tracking - DanceTrack: IDF1<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-07-01<BR>ratio: 0.3427<BR>benchmarks:<BR>  Multi-object tracking - MOTS20: IDF1<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-10-01<BR>ratio: 0.5225<BR>benchmarks:<BR>  Multi-object tracking - DanceTrack: IDF1<BR>  Multi-object tracking - MOT17: IDF1<BR>  Multi-object tracking - MOT17: MOTA<BR>  Multi-object tracking - MOT20: MOTA<BR>","<BR>task: Video process // Object tracking<BR>date: 2021-12-01<BR>ratio: 0.2056<BR>benchmarks:<BR>  Visual object tracking - GOT-10k: Average Overlap<BR>  Visual object tracking - LaSOT: AUC<BR>  Visual object tracking - LaSOT: Normalized Precision<BR>  Visual object tracking - LaSOT: Precision<BR>  Visual object tracking - TrackingNet: Accuracy<BR>  Visual object tracking - TrackingNet: Normalized Precision<BR>  Visual object tracking - TrackingNet: Precision<BR>","<BR>task: Video process // Video Enhancement<BR>date: 2019-02-01<BR>ratio: 0.1563<BR>benchmarks:<BR>  Video Enhancement - MFQE v2: Incremental PSNR<BR>","<BR>task: Video process // Video Enhancement<BR>date: 2019-05-01<BR>ratio: 0.2969<BR>benchmarks:<BR>  Video Enhancement - MFQE v2: Incremental PSNR<BR>","<BR>task: Video process // Video Enhancement<BR>date: 2020-04-01<BR>ratio: 0.125<BR>benchmarks:<BR>  Video Enhancement - MFQE v2: Incremental PSNR<BR>","<BR>task: Video process // Video Enhancement<BR>date: 2021-04-01<BR>ratio: 0.4219<BR>benchmarks:<BR>  Video Enhancement - MFQE v2: Incremental PSNR<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2019-08-01<BR>ratio: 0.7342<BR>benchmarks:<BR>  Video Quality Assessment - KoNViD-1k: PLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: KLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: PLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: SRCC<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2020-05-01<BR>ratio: 0.1006<BR>benchmarks:<BR>  Video Quality Assessment - KoNViD-1k: PLCC<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2020-08-01<BR>ratio: 0.1194<BR>benchmarks:<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: KLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: PLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: SRCC<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2020-11-01<BR>ratio: 0.3725<BR>benchmarks:<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: KLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: PLCC<BR>  Video Quality Assessment - MSU Video Quality Metrics Benchmark: SRCC<BR>","<BR>task: Video process // Video Quality Assessment<BR>date: 2021-01-01<BR>ratio: 0.7639<BR>benchmarks:<BR>  Video Quality Assessment - KoNViD-1k: PLCC<BR>","<BR>task: Video process // Video captioning<BR>date: 2019-04-01<BR>ratio: 0.1189<BR>benchmarks:<BR>  Video captioning - YouCook2: BLEU-3<BR>  Video captioning - YouCook2: CIDEr<BR>  Video captioning - YouCook2: METEOR<BR>  Video captioning - YouCook2: ROUGE-L<BR>","<BR>task: Video process // Video captioning<BR>date: 2020-02-01<BR>ratio: 0.9963<BR>benchmarks:<BR>  Video captioning - YouCook2: BLEU-3<BR>  Video captioning - YouCook2: CIDEr<BR>  Video captioning - YouCook2: METEOR<BR>  Video captioning - YouCook2: ROUGE-L<BR>","<BR>task: Video process // Video captioning<BR>date: 2020-05-01<BR>ratio: 0.7949<BR>benchmarks:<BR>  Dense video captioning - ActivityNet Captions: BLEU-3<BR>  Dense video captioning - ActivityNet Captions: BLEU-4<BR>  Dense video captioning - ActivityNet Captions: METEOR<BR>","<BR>task: Video process // Video captioning<BR>date: 2020-06-01<BR>ratio: 0.3138<BR>benchmarks:<BR>  Dense video captioning - ActivityNet Captions: METEOR<BR>","<BR>task: Video process // Video captioning<BR>date: 2020-11-01<BR>ratio: 0.2051<BR>benchmarks:<BR>  Dense video captioning - ActivityNet Captions: BLEU-3<BR>  Dense video captioning - ActivityNet Captions: BLEU-4<BR>","<BR>task: Video process // Video captioning<BR>date: 2021-07-01<BR>ratio: 0.8866<BR>benchmarks:<BR>  Dense video captioning - ActivityNet Captions: BLEU-4<BR>  Dense video captioning - ActivityNet Captions: METEOR<BR>","<BR>task: Video process // Video classification<BR>date: 2014-12-01<BR>ratio: 0.4206<BR>benchmarks:<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>","<BR>task: Video process // Video classification<BR>date: 2015-03-01<BR>ratio: 0.726<BR>benchmarks:<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2015-05-01<BR>ratio: 0.2726<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2015-06-01<BR>ratio: 0.2462<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2016-01-01<BR>ratio: 0.1095<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>","<BR>task: Video process // Video classification<BR>date: 2016-04-01<BR>ratio: 0.1378<BR>benchmarks:<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2016-06-01<BR>ratio: 0.3057<BR>benchmarks:<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2016-07-01<BR>ratio: 0.4914<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2016-08-01<BR>ratio: 0.1598<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2016-09-01<BR>ratio: 0.3082<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Video classification<BR>date: 2016-11-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-03-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2017-04-01<BR>ratio: 0.2983<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-05-01<BR>ratio: 0.2867<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-06-01<BR>ratio: 0.4678<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-07-01<BR>ratio: 0.0442<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-08-01<BR>ratio: 0.7692<BR>benchmarks:<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-10-01<BR>ratio: 0.1673<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-11-01<BR>ratio: 0.7219<BR>benchmarks:<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - Sports-1M: Clip Hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2017-12-01<BR>ratio: 0.2303<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-01-01<BR>ratio: 0.5<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>","<BR>task: Video process // Video classification<BR>date: 2018-02-01<BR>ratio: 0.939<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - Florence 3D: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-04-01<BR>ratio: 0.88<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-05-01<BR>ratio: 0.967<BR>benchmarks:<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>","<BR>task: Video process // Video classification<BR>date: 2018-06-01<BR>ratio: 0.8551<BR>benchmarks:<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (RGB+pose)<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-07-01<BR>ratio: 0.6555<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - Jester: Val<BR>","<BR>task: Video process // Video classification<BR>date: 2018-09-01<BR>ratio: 0.0676<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>","<BR>task: Video process // Video classification<BR>date: 2018-10-01<BR>ratio: 0.0071<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>","<BR>task: Video process // Video classification<BR>date: 2018-11-01<BR>ratio: 0.5667<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2018-12-01<BR>ratio: 0.6875<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2019-02-01<BR>ratio: 0.9489<BR>benchmarks:<BR>  Video classification - YouTube-8M: Hit-at-1<BR>","<BR>task: Video process // Video classification<BR>date: 2019-03-01<BR>ratio: 0.3298<BR>benchmarks:<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>","<BR>task: Video process // Video classification<BR>date: 2019-04-01<BR>ratio: 0.5592<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  3D semantic segmentation - SensatUrban: mIoU<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Sports-1M: Video hit-at-1<BR>  Action recognition - Sports-1M: Video hit-at-5<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.2<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.3<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.4<BR>  Skeleton based action recognition - JHMDB Pose Tracking: PCK-at-0.5<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - SYSU 3D: Accuracy<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (AV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CS)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV I)<BR>  Skeleton based action recognition - Varying-view RGB-D Action-Skeleton: Accuracy (CV II)<BR>","<BR>task: Video process // Video classification<BR>date: 2019-05-01<BR>ratio: 0.0096<BR>benchmarks:<BR>  Action recognition - Jester: Val<BR>","<BR>task: Video process // Video classification<BR>date: 2019-06-01<BR>ratio: 0.0511<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Video classification - YouTube-8M: Hit-at-1<BR>","<BR>task: Video process // Video classification<BR>date: 2019-07-01<BR>ratio: 0.4559<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - THUMOS\u201914: mAP-at-0.3<BR>  Action recognition - THUMOS\u201914: mAP-at-0.4<BR>  Action recognition - THUMOS\u201914: mAP-at-0.5<BR>  Skeleton based action recognition - J-HMDB: Accuracy (pose)<BR>  Skeleton based action recognition - JHMDB (2D poses only): Average accuracy of 3 splits<BR>","<BR>task: Video process // Video classification<BR>date: 2019-08-01<BR>ratio: 0.8605<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2019-09-01<BR>ratio: 0.2857<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CS)<BR>  Skeleton based action recognition - PKU-MMD: mAP-at-0.50 (CV)<BR>","<BR>task: Video process // Video classification<BR>date: 2019-10-01<BR>ratio: 0.108<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>","<BR>task: Video process // Video classification<BR>date: 2019-11-01<BR>ratio: 0.9701<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2019-12-01<BR>ratio: 0.9111<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-01-01<BR>ratio: 0.1804<BR>benchmarks:<BR>  Action recognition - HMDB-51: Average accuracy of 3 splits<BR>","<BR>task: Video process // Video classification<BR>date: 2020-03-01<BR>ratio: 0.064<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2020-04-01<BR>ratio: 0.348<BR>benchmarks:<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-06-01<BR>ratio: 0.4167<BR>benchmarks:<BR>  Action recognition - AVA v2.1: mAP (Val)<BR>  Action recognition - AVA v2.2: mAP<BR>  Deblurring - GoPro: PSNR<BR>  Deblurring - GoPro: SSIM<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - UAV-Human: CSv1(%)<BR>  Skeleton based action recognition - UAV-Human: CSv2(%)<BR>  Skeleton based action recognition - UT-Kinect: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-07-01<BR>ratio: 0.4943<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2020-08-01<BR>ratio: 0.3977<BR>benchmarks:<BR>  3D semantic segmentation - PartNet: mIOU<BR>  Action recognition - Jester: Val<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-10-01<BR>ratio: 0.9051<BR>benchmarks:<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2020-11-01<BR>ratio: 0.1022<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2020-12-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - UCF101: 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-01-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-02-01<BR>ratio: 0.7765<BR>benchmarks:<BR>  3D semantic segmentation - SemanticKITTI: mIoU<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R: SSIM (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-03-01<BR>ratio: 0.4753<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Self-supervised action recognition - HMDB51 (finetuned): Top-1 Accuracy<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101 (finetuned): 3-fold Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-04-01<BR>ratio: 0.84<BR>benchmarks:<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Action recognition - NTU RGB+D: Accuracy (CS)<BR>  Action recognition - NTU RGB+D: Accuracy (CV)<BR>  Self-supervised action recognition - HMDB51: Top-1 Accuracy<BR>  Self-supervised action recognition - UCF101: 3-fold Accuracy<BR>  Skeleton based action recognition - Kinetics-Skeleton dataset: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D: Accuracy (CS)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-05-01<BR>ratio: 0.0142<BR>benchmarks:<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-06-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>  Deblurring - GoPro: SSIM<BR>  Deblurring - HIDE (trained on GOPRO): SSIM (sRGB)<BR>  Deblurring - RSBlur: Average PSNR<BR>  Deblurring - RealBlur-J (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-J (trained on GoPro): SSIM (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): PSNR (sRGB)<BR>  Deblurring - RealBlur-R (trained on GoPro): SSIM (sRGB)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-07-01<BR>ratio: 0.7047<BR>benchmarks:<BR>  Action recognition - Real Life Violence Situations Dataset: accuracy<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Skeleton based action recognition - N-UCLA: Accuracy<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Setup)<BR>  Skeleton based action recognition - NTU RGB+D 120: Accuracy (Cross-Subject)<BR>  Skeleton based action recognition - UPenn Action: Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-08-01<BR>ratio: 0.0617<BR>benchmarks:<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-09-01<BR>ratio: 0.2242<BR>benchmarks:<BR>  Action recognition - Something-Something V1: Top 1 Accuracy<BR>  Action recognition - Something-Something V1: Top 5 Accuracy<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>","<BR>task: Video process // Video classification<BR>date: 2021-10-01<BR>ratio: 0.2404<BR>benchmarks:<BR>  Action recognition - Diving-48: Accuracy<BR>  Action recognition - EPIC-KITCHENS-100: Noun@1<BR>","<BR>task: Video process // Video classification<BR>date: 2021-11-01<BR>ratio: 0.3636<BR>benchmarks:<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Deblurring - RealBlur-J: PSNR (sRGB)<BR>  Deblurring - RealBlur-J: SSIM (sRGB)<BR>  Deblurring - RealBlur-R: PSNR (sRGB)<BR>","<BR>task: Video process // Video classification<BR>date: 2021-12-01<BR>ratio: 0.6265<BR>benchmarks:<BR>  Action recognition - AVA v2.2: mAP<BR>  Action recognition - Something-Something V2: Top-1 Accuracy<BR>  Action recognition - Something-Something V2: Top-5 Accuracy<BR>  Deblurring - HIDE (trained on GOPRO): PSNR (sRGB)<BR>  Few Shot Action Recognition - HMDB51: 1:1 Accuracy<BR>  Few Shot Action Recognition - Something-Something-100: 1:1 Accuracy<BR>  Few Shot Action Recognition - UCF101: 1:1 Accuracy<BR>","<BR>task: Video process // Video denoising<BR>date: 2019-07-01<BR>ratio: 0.4565<BR>benchmarks:<BR>  Video denoising - DAVIS sigma10: PSNR<BR>  Video denoising - DAVIS sigma20: PSNR<BR>  Video denoising - Set8 sigma10: PSNR<BR>","<BR>task: Video process // Video denoising<BR>date: 2020-11-01<BR>ratio: 0.8462<BR>benchmarks:<BR>  Video denoising - Set8 sigma30: PSNR<BR>","<BR>task: Video process // Video denoising<BR>date: 2021-03-01<BR>ratio: 0.8571<BR>benchmarks:<BR>  Video denoising - DAVIS sigma10: PSNR<BR>  Video denoising - DAVIS sigma20: PSNR<BR>  Video denoising - Set8 sigma10: PSNR<BR>  Video denoising - Set8 sigma30: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2017-11-01<BR>ratio: 0.0865<BR>benchmarks:<BR>  Video frame interpolation - Middlebury: Interpolation Error<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2018-10-01<BR>ratio: 0.2521<BR>benchmarks:<BR>  Video frame interpolation - Middlebury: Interpolation Error<BR>  Video frame interpolation - Vimeo90K: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2019-04-01<BR>ratio: 0.274<BR>benchmarks:<BR>  Video frame interpolation - Middlebury: Interpolation Error<BR>  Video frame interpolation - Vimeo90K: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2020-03-01<BR>ratio: 0.584<BR>benchmarks:<BR>  Video frame interpolation - Middlebury: Interpolation Error<BR>  Video frame interpolation - Vimeo90K: PSNR<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2020-07-01<BR>ratio: 0.4<BR>benchmarks:<BR>  Video frame interpolation - UCF101: SSIM<BR>  Video frame interpolation - Vimeo90K: SSIM<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2020-11-01<BR>ratio: 0.7347<BR>benchmarks:<BR>  Video frame interpolation - Vimeo90K: SSIM<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2021-03-01<BR>ratio: 0.8696<BR>benchmarks:<BR>  Video frame interpolation - X4K1000FPS: PSNR<BR>  Video frame interpolation - X4K1000FPS: SSIM<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2021-08-01<BR>ratio: 0.6<BR>benchmarks:<BR>  Video frame interpolation - UCF101: SSIM<BR>  Video frame interpolation - Vimeo90K: PSNR<BR>  Video frame interpolation - Vimeo90K: SSIM<BR>  Video frame interpolation - X4K1000FPS: PSNR<BR>  Video frame interpolation - X4K1000FPS: SSIM<BR>","<BR>task: Video process // Video frame interpolation<BR>date: 2021-11-01<BR>ratio: 0.1171<BR>benchmarks:<BR>  Video frame interpolation - X4K1000FPS: PSNR<BR>","<BR>task: Video process // Video generation<BR>date: 2016-11-01<BR>ratio: 0.2492<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, Unconditional, Single GPU: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2017-07-01<BR>ratio: 0.0387<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, Unconditional, Single GPU: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2017-10-01<BR>ratio: 0.1622<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2018-02-01<BR>ratio: 0.0281<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2018-04-01<BR>ratio: 0.669<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2018-11-01<BR>ratio: 0.613<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, Unconditional, Single GPU: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2019-07-01<BR>ratio: 0.0315<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2019-12-01<BR>ratio: 0.5269<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, 64x64, Unconditional: Inception Score<BR>  Video generation - UCF-101 16 frames, Unconditional, Single GPU: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2020-03-01<BR>ratio: 0.031<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2020-11-01<BR>ratio: 0.4731<BR>benchmarks:<BR>  Video generation - UCF-101 16 frames, 64x64, Unconditional: Inception Score<BR>","<BR>task: Video process // Video generation<BR>date: 2021-06-01<BR>ratio: 0.0463<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video generation<BR>date: 2021-11-01<BR>ratio: 0.032<BR>benchmarks:<BR>  Video generation - BAIR Robot Pushing: FVD score<BR>","<BR>task: Video process // Video inpainting<BR>date: 2019-07-01<BR>ratio: 0.2834<BR>benchmarks:<BR>  Video inpainting - YouTube-VOS 2018 val: PSNR<BR>  Video inpainting - YouTube-VOS 2018 val: SSIM<BR>","<BR>task: Video process // Video inpainting<BR>date: 2019-08-01<BR>ratio: 0.4499<BR>benchmarks:<BR>  Video inpainting - DAVIS: PSNR<BR>  Video inpainting - DAVIS: SSIM<BR>  Video inpainting - YouTube-VOS 2018 val: PSNR<BR>  Video inpainting - YouTube-VOS 2018 val: SSIM<BR>","<BR>task: Video process // Video inpainting<BR>date: 2020-07-01<BR>ratio: 0.1943<BR>benchmarks:<BR>  Video inpainting - DAVIS: PSNR<BR>  Video inpainting - DAVIS: SSIM<BR>  Video inpainting - YouTube-VOS 2018 val: PSNR<BR>  Video inpainting - YouTube-VOS 2018 val: SSIM<BR>","<BR>task: Video process // Video inpainting<BR>date: 2020-09-01<BR>ratio: 0.0363<BR>benchmarks:<BR>  Video inpainting - DAVIS: PSNR<BR>","<BR>task: Video process // Video inpainting<BR>date: 2021-09-01<BR>ratio: 0.486<BR>benchmarks:<BR>  Video inpainting - DAVIS: PSNR<BR>  Video inpainting - DAVIS: SSIM<BR>  Video inpainting - YouTube-VOS 2018 val: PSNR<BR>  Video inpainting - YouTube-VOS 2018 val: SSIM<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2018-02-01<BR>ratio: 0.0399<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2019-05-01<BR>ratio: 0.3729<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2020-03-01<BR>ratio: 0.2111<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: AP75<BR>  Video instance segmentation - YouTube-VIS validation: AR10<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2020-07-01<BR>ratio: 0.0483<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2020-11-01<BR>ratio: 0.2094<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: AP75<BR>  Video instance segmentation - YouTube-VIS validation: AR10<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2021-06-01<BR>ratio: 0.6077<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: AP75<BR>  Video instance segmentation - YouTube-VIS validation: AR10<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video instance segmentation<BR>date: 2021-12-01<BR>ratio: 0.2271<BR>benchmarks:<BR>  Video instance segmentation - YouTube-VIS validation: AP50<BR>  Video instance segmentation - YouTube-VIS validation: AP75<BR>  Video instance segmentation - YouTube-VIS validation: AR10<BR>  Video instance segmentation - YouTube-VIS validation: AR1<BR>  Video instance segmentation - YouTube-VIS validation: mask AP<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2015-06-01<BR>ratio: 0.9697<BR>benchmarks:<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2015-12-01<BR>ratio: 0.6103<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2016-06-01<BR>ratio: 0.5602<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - ViSal: Average MAE<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2016-11-01<BR>ratio: 0.6767<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube: mIoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2016-12-01<BR>ratio: 0.6532<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-01-01<BR>ratio: 0.0187<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-03-01<BR>ratio: 0.68<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-04-01<BR>ratio: 0.1517<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-06-01<BR>ratio: 0.2105<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-07-01<BR>ratio: 0.0475<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-09-01<BR>ratio: 0.5528<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2017-12-01<BR>ratio: 0.0909<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-02-01<BR>ratio: 0.1721<BR>benchmarks:<BR>  One-shot visual object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Speed  (FPS)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-03-01<BR>ratio: 0.1093<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube: mIoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-06-01<BR>ratio: 0.7931<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J&F score<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: max E-measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-07-01<BR>ratio: 0.351<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-09-01<BR>ratio: 0.9828<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J&F score<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>  Video salient object detection - DAVIS-2016: AVERAGE MAE<BR>  Video salient object detection - DAVIS-2016: MAX E-MEASURE<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: AVERAGE MAE<BR>  Video salient object detection - MCL: MAX E-MEASURE<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - MCL: S-Measure<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - UVSD: Average MAE<BR>  Video salient object detection - UVSD: S-Measure<BR>  Video salient object detection - UVSD: max E-measure<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-10-01<BR>ratio: 0.0359<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2018-11-01<BR>ratio: 0.005<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-02-01<BR>ratio: 0.8222<BR>benchmarks:<BR>  Semi-supervised video object segmentation - YouTube: mIoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-03-01<BR>ratio: 0.9474<BR>benchmarks:<BR>  One-shot visual object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-04-01<BR>ratio: 0.3015<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J&F score<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-06-01<BR>ratio: 0.9714<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Video salient object detection - DAVIS-2016: S-Measure<BR>  Video salient object detection - DAVSOD-Difficult20: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: Average MAE<BR>  Video salient object detection - DAVSOD-Normal25: S-Measure<BR>  Video salient object detection - DAVSOD-Normal25: max E-measure<BR>  Video salient object detection - DAVSOD-easy35: Average MAE<BR>  Video salient object detection - DAVSOD-easy35: S-Measure<BR>  Video salient object detection - DAVSOD-easy35: max E-Measure<BR>  Video salient object detection - DAVSOD-easy35: max F-Measure<BR>  Video salient object detection - FBMS-59: AVERAGE MAE<BR>  Video salient object detection - FBMS-59: MAX E-MEASURE<BR>  Video salient object detection - FBMS-59: MAX F-MEASURE<BR>  Video salient object detection - FBMS-59: S-Measure<BR>  Video salient object detection - MCL: MAX F-MEASURE<BR>  Video salient object detection - SegTrack v2: AVERAGE MAE<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>  Video salient object detection - ViSal: Average MAE<BR>  Video salient object detection - ViSal: S-Measure<BR>  Video salient object detection - ViSal: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-08-01<BR>ratio: 0.1748<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Video salient object detection - VOS-T: Average MAE<BR>  Video salient object detection - VOS-T: S-Measure<BR>  Video salient object detection - VOS-T: max E-measure<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-09-01<BR>ratio: 0.9128<BR>benchmarks:<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Overall<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Speed  (FPS)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Unsupervised video object segmentation - SegTrack v2: Mean IoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2019-10-01<BR>ratio: 0.5422<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J&F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J&F score<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J&F score<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-01-01<BR>ratio: 0.8457<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-02-01<BR>ratio: 0.1961<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J&F score<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J&F score<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-03-01<BR>ratio: 0.8307<BR>benchmarks:<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J<BR>  Interactive video object segmentation - DAVIS 2017: J@60s<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Overall<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Decay)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-04-01<BR>ratio: 0.4578<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 test-dev (no extra training data): J&F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): F score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J score<BR>  Semi-supervised video object segmentation - DAVIS 2017 val (no extra training data): J&F score<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-07-01<BR>ratio: 0.2417<BR>benchmarks:<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J&F<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J<BR>  Interactive video object segmentation - DAVIS 2017: J@60s<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-08-01<BR>ratio: 0.0957<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2020-10-01<BR>ratio: 0.08<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Overall<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-01-01<BR>ratio: 0.0872<BR>benchmarks:<BR>  Unsupervised video object segmentation - SegTrack v2: Mean IoU<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-02-01<BR>ratio: 0.2592<BR>benchmarks:<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Speed  (FPS)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-03-01<BR>ratio: 0.9211<BR>benchmarks:<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J&F<BR>  Interactive video object segmentation - DAVIS 2017: AUC-J<BR>  Interactive video object segmentation - DAVIS 2017: J@60s<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Unseen)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Unsupervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-06-01<BR>ratio: 0.8216<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2016: J&F<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): F-measure (Recall)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): J&F<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Decay)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (val): Jaccard (Recall)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: F-Measure (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Jaccard (Unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Overall<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val: Speed  (FPS)<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Recall)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: Jaccard (Recall)<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-11-01<BR>ratio: 0.0431<BR>benchmarks:<BR>  Unsupervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Unsupervised video object segmentation - DAVIS 2016: J&F<BR>","<BR>task: Video process // Video object segmentation<BR>date: 2021-12-01<BR>ratio: 0.8926<BR>benchmarks:<BR>  Semi-supervised video object segmentation - DAVIS 2016: F-measure (Mean)<BR>  Semi-supervised video object segmentation - DAVIS 2017 (test-dev): Jaccard (Mean)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): F score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (seen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J score (unseen)<BR>  Semi-supervised video object segmentation - YouTube-VOS 2018 val (no extra training data): J&F score<BR>","<BR>task: Video process // Video prediction<BR>date: 2016-05-01<BR>ratio: 0.5869<BR>benchmarks:<BR>  Video prediction - KTH: PSNR<BR>  Video prediction - KTH: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2017-06-01<BR>ratio: 0.0663<BR>benchmarks:<BR>  Video prediction - KTH: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2017-10-01<BR>ratio: 0.1768<BR>benchmarks:<BR>  Video prediction - KTH: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2017-12-01<BR>ratio: 0.6612<BR>benchmarks:<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2018-02-01<BR>ratio: 0.7338<BR>benchmarks:<BR>  Video prediction - KTH: LPIPS<BR>","<BR>task: Video process // Video prediction<BR>date: 2018-04-01<BR>ratio: 0.2099<BR>benchmarks:<BR>  Video prediction - KTH: LPIPS<BR>  Video prediction - KTH: PSNR<BR>  Video prediction - KTH: SSIM<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2018-10-01<BR>ratio: 0.0016<BR>benchmarks:<BR>  Video prediction - KTH: PSNR<BR>","<BR>task: Video process // Video prediction<BR>date: 2018-11-01<BR>ratio: 0.4709<BR>benchmarks:<BR>  Video prediction - Human3.6M: MAE<BR>  Video prediction - Human3.6M: MSE<BR>  Video prediction - Human3.6M: SSIM<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2019-05-01<BR>ratio: 0.6583<BR>benchmarks:<BR>  Video prediction - Human3.6M: MAE<BR>  Video prediction - Human3.6M: SSIM<BR>  Video prediction - KTH: PSNR<BR>  Video prediction - KTH: SSIM<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>","<BR>task: Video process // Video prediction<BR>date: 2020-02-01<BR>ratio: 0.0861<BR>benchmarks:<BR>  Video prediction - KTH: PSNR<BR>  Video prediction - KTH: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2020-03-01<BR>ratio: 0.5291<BR>benchmarks:<BR>  Video prediction - Human3.6M: MAE<BR>  Video prediction - Human3.6M: MSE<BR>  Video prediction - Human3.6M: SSIM<BR>  Video prediction - Moving MNIST: MAE<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2020-05-01<BR>ratio: 0.0259<BR>benchmarks:<BR>  Video prediction - Moving MNIST: MSE<BR>  Video prediction - Moving MNIST: SSIM<BR>","<BR>task: Video process // Video prediction<BR>date: 2021-07-01<BR>ratio: 0.1727<BR>benchmarks:<BR>  Video prediction - KTH: LPIPS<BR>","<BR>task: Video process // Video question answering<BR>date: 2018-09-01<BR>ratio: 0.846<BR>benchmarks:<BR>  Video question answering - SUTD-TrafficQA: 1/2<BR>  Video question answering - SUTD-TrafficQA: 1/4<BR>","<BR>task: Video process // Video question answering<BR>date: 2020-02-01<BR>ratio: 0.1863<BR>benchmarks:<BR>  Video question answering - SUTD-TrafficQA: 1/2<BR>  Video question answering - SUTD-TrafficQA: 1/4<BR>","<BR>task: Video process // Video question answering<BR>date: 2020-05-01<BR>ratio: 0.5789<BR>benchmarks:<BR>  Video question answering - TVQA: Accuracy<BR>","<BR>task: Video process // Video question answering<BR>date: 2020-11-01<BR>ratio: 0.4211<BR>benchmarks:<BR>  Video question answering - TVQA: Accuracy<BR>","<BR>task: Video process // Video question answering<BR>date: 2021-03-01<BR>ratio: 0.0932<BR>benchmarks:<BR>  Video question answering - SUTD-TrafficQA: 1/2<BR>  Video question answering - SUTD-TrafficQA: 1/4<BR>","<BR>task: Video process // Video retrieval<BR>date: 2016-12-01<BR>ratio: 0.2692<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2017-07-01<BR>ratio: 0.1058<BR>benchmarks:<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2018-04-01<BR>ratio: 0.5429<BR>benchmarks:<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2018-06-01<BR>ratio: 0.2173<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2018-08-01<BR>ratio: 0.3212<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>","<BR>task: Video process // Video retrieval<BR>date: 2019-06-01<BR>ratio: 0.7183<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>  Video retrieval - YouCook2: text-to-video Median Rank<BR>  Video retrieval - YouCook2: text-to-video R-at-10<BR>  Video retrieval - YouCook2: text-to-video R-at-1<BR>  Video retrieval - YouCook2: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2019-07-01<BR>ratio: 0.7418<BR>benchmarks:<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2020-02-01<BR>ratio: 0.75<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>  Video retrieval - YouCook2: text-to-video Median Rank<BR>  Video retrieval - YouCook2: text-to-video R-at-10<BR>  Video retrieval - YouCook2: text-to-video R-at-1<BR>  Video retrieval - YouCook2: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2020-07-01<BR>ratio: 0.54<BR>benchmarks:<BR>  Video retrieval - ActivityNet: text-to-video Mean Rank<BR>  Video retrieval - ActivityNet: text-to-video Median Rank<BR>  Video retrieval - ActivityNet: text-to-video R-at-1<BR>  Video retrieval - ActivityNet: text-to-video R-at-50<BR>  Video retrieval - ActivityNet: text-to-video R-at-5<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-02-01<BR>ratio: 0.5222<BR>benchmarks:<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-03-01<BR>ratio: 0.2886<BR>benchmarks:<BR>  Video retrieval - DiDeMo: text-to-video Mean Rank<BR>  Video retrieval - DiDeMo: text-to-video Median Rank<BR>  Video retrieval - DiDeMo: text-to-video R-at-10<BR>  Video retrieval - DiDeMo: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-04-01<BR>ratio: 0.9783<BR>benchmarks:<BR>  Video retrieval - ActivityNet: text-to-video Mean Rank<BR>  Video retrieval - ActivityNet: text-to-video Median Rank<BR>  Video retrieval - ActivityNet: text-to-video R-at-1<BR>  Video retrieval - ActivityNet: text-to-video R-at-50<BR>  Video retrieval - ActivityNet: text-to-video R-at-5<BR>  Video retrieval - DiDeMo: text-to-video Mean Rank<BR>  Video retrieval - DiDeMo: text-to-video Median Rank<BR>  Video retrieval - DiDeMo: text-to-video R-at-10<BR>  Video retrieval - DiDeMo: text-to-video R-at-1<BR>  Video retrieval - DiDeMo: text-to-video R-at-5<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT-1kA: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT-1kA: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-5<BR>  Video retrieval - MSVD: video-to-text R-at-10<BR>  Video retrieval - MSVD: video-to-text R-at-1<BR>  Video retrieval - MSVD: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-06-01<BR>ratio: 0.3023<BR>benchmarks:<BR>  Video retrieval - MSR-VTT-1kA: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-5<BR>  Video retrieval - MSR-VTT: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-08-01<BR>ratio: 0.0506<BR>benchmarks:<BR>  Video retrieval - YouCook2: text-to-video R-at-10<BR>  Video retrieval - YouCook2: text-to-video R-at-1<BR>  Video retrieval - YouCook2: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-09-01<BR>ratio: 0.7766<BR>benchmarks:<BR>  Video retrieval - ActivityNet: text-to-video Mean Rank<BR>  Video retrieval - ActivityNet: text-to-video Median Rank<BR>  Video retrieval - ActivityNet: text-to-video R-at-1<BR>  Video retrieval - ActivityNet: text-to-video R-at-5<BR>  Video retrieval - DiDeMo: text-to-video Mean Rank<BR>  Video retrieval - DiDeMo: text-to-video R-at-1<BR>  Video retrieval - DiDeMo: text-to-video R-at-5<BR>  Video retrieval - LSMDC: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video R-at-1<BR>  Video retrieval - LSMDC: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT-1kA: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-5<BR>  Video retrieval - MSR-VTT: text-to-video Mean Rank<BR>  Video retrieval - MSR-VTT: text-to-video Median Rank<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT: video-to-text R-at-5<BR>  Video retrieval - MSVD: video-to-text R-at-10<BR>  Video retrieval - MSVD: video-to-text R-at-1<BR>  Video retrieval - MSVD: video-to-text R-at-5<BR>  Video retrieval - YouCook2: text-to-video R-at-10<BR>  Video retrieval - YouCook2: text-to-video R-at-1<BR>  Video retrieval - YouCook2: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-11-01<BR>ratio: 0.5125<BR>benchmarks:<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT-1kA: text-to-video R-at-5<BR>  Video retrieval - MSR-VTT-1kA: video-to-text Median Rank<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-10<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-1<BR>  Video retrieval - MSR-VTT-1kA: video-to-text R-at-5<BR>  Video retrieval - MSR-VTT: text-to-video R-at-10<BR>  Video retrieval - MSR-VTT: text-to-video R-at-1<BR>  Video retrieval - MSR-VTT: text-to-video R-at-5<BR>","<BR>task: Video process // Video retrieval<BR>date: 2021-12-01<BR>ratio: 0.0371<BR>benchmarks:<BR>  Video retrieval - DiDeMo: text-to-video R-at-10<BR>  Video retrieval - LSMDC: text-to-video Median Rank<BR>","<BR>task: Video process // Video segmentation<BR>date: 2018-08-01<BR>ratio: 0.1<BR>benchmarks:<BR>  Camera shot boundary detection - ClipShots: F1 score<BR>","<BR>task: Video process // Video segmentation<BR>date: 2020-08-01<BR>ratio: 0.9<BR>benchmarks:<BR>  Camera shot boundary detection - ClipShots: F1 score<BR>","<BR>task: Video process // Video semantic segmentation<BR>date: 2016-12-01<BR>ratio: 0.9412<BR>benchmarks:<BR>  Video semantic segmentation - Cityscapes val: mIoU<BR>","<BR>task: Video process // Video semantic segmentation<BR>date: 2020-04-01<BR>ratio: 0.0196<BR>benchmarks:<BR>  Video semantic segmentation - Cityscapes val: mIoU<BR>","<BR>task: Video process // Video semantic segmentation<BR>date: 2021-02-01<BR>ratio: 0.0392<BR>benchmarks:<BR>  Video semantic segmentation - Cityscapes val: mIoU<BR>","<BR>task: Video process // Video summarization<BR>date: 2018-11-01<BR>ratio: 0.4333<BR>benchmarks:<BR>  Supervised video summarization - SumMe: F1-score (Canonical)<BR>  Supervised video summarization - TvSum: F1-score (Canonical)<BR>  Unsupervised video summarization - SumMe: training time (s)<BR>  Unsupervised video summarization - TvSum: F1-score<BR>  Unsupervised video summarization - TvSum: training time (s)<BR>","<BR>task: Video process // Video summarization<BR>date: 2018-12-01<BR>ratio: 0.4741<BR>benchmarks:<BR>  Video summarization - SumMe: F1-score (Canonical)<BR>  Video summarization - TvSum: F1-score (Augmented)<BR>  Video summarization - TvSum: F1-score (Canonical)<BR>","<BR>task: Video process // Video summarization<BR>date: 2019-10-01<BR>ratio: 0.2251<BR>benchmarks:<BR>  Unsupervised video summarization - SumMe: training time (s)<BR>  Unsupervised video summarization - TvSum: training time (s)<BR>","<BR>task: Video process // Video summarization<BR>date: 2019-12-01<BR>ratio: 0.1639<BR>benchmarks:<BR>  Unsupervised video summarization - SumMe: training time (s)<BR>  Unsupervised video summarization - TvSum: training time (s)<BR>","<BR>task: Video process // Video summarization<BR>date: 2020-11-01<BR>ratio: 0.6<BR>benchmarks:<BR>  Unsupervised video summarization - SumMe: training time (s)<BR>  Unsupervised video summarization - TvSum: F1-score<BR>  Unsupervised video summarization - TvSum: training time (s)<BR>","<BR>task: Video process // Video summarization<BR>date: 2020-12-01<BR>ratio: 0.7286<BR>benchmarks:<BR>  Video summarization - SumMe: F1-score (Canonical)<BR>  Video summarization - TvSum: F1-score (Augmented)<BR>  Video summarization - TvSum: F1-score (Canonical)<BR>","<BR>task: Video process // Video summarization<BR>date: 2021-04-01<BR>ratio: 0.9574<BR>benchmarks:<BR>  Supervised video summarization - SumMe: F1-score (Canonical)<BR>  Supervised video summarization - TvSum: F1-score (Canonical)<BR>","<BR>task: Video process // Video summarization<BR>date: 2021-12-01<BR>ratio: 0.4821<BR>benchmarks:<BR>  Supervised video summarization - SumMe: F1-score (Canonical)<BR>  Video summarization - SumMe: F1-score (Canonical)<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2016-09-01<BR>ratio: 0.198<BR>benchmarks:<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2016-11-01<BR>ratio: 0.1367<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2017-04-01<BR>ratio: 0.1761<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2017-12-01<BR>ratio: 0.612<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2018-01-01<BR>ratio: 0.4027<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2018-06-01<BR>ratio: 0.3649<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2018-09-01<BR>ratio: 0.393<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: 1 - LPIPS<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2018-12-01<BR>ratio: 0.5506<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-03-01<BR>ratio: 0.3173<BR>benchmarks:<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-05-01<BR>ratio: 0.0133<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-09-01<BR>ratio: 0.0664<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2019-11-01<BR>ratio: 0.8468<BR>benchmarks:<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2020-01-01<BR>ratio: 0.6304<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2020-06-01<BR>ratio: 0.9497<BR>benchmarks:<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over ERQA<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over LPIPS<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over MS-SSIM<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over PSNR<BR>  Video super-resolution - MSU Super-Resolution for Video Compression: BSQ-rate over VMAF<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - Vid4 - 4x upscaling: SSIM<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2020-07-01<BR>ratio: 0.9937<BR>benchmarks:<BR>  Video super-resolution - Ultra Video Group HD - 4x upscaling: Average PSNR<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2020-08-01<BR>ratio: 0.0465<BR>benchmarks:<BR>  Video super-resolution - Vid4 - 4x upscaling: PSNR<BR>","<BR>task: Video process // Video super-resolution<BR>date: 2020-12-01<BR>ratio: 0.5413<BR>benchmarks:<BR>  Video super-resolution - Falling Objects: PSNR<BR>  Video super-resolution - Falling Objects: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: ERQAv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: PSNR<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: QRCRv1.0<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: SSIM<BR>  Video super-resolution - MSU Video Super Resolution Benchmark: Detail Restoration: Subjective score<BR>  Video super-resolution - TbD-3D: PSNR<BR>  Video super-resolution - TbD-3D: SSIM<BR>  Video super-resolution - TbD: PSNR<BR>","<BR>task: Visual Odometry<BR>date: 2015-11-01<BR>ratio: 0.9206<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Visual Odometry<BR>date: 2019-01-01<BR>ratio: 0.0794<BR>benchmarks:<BR>  Face anti-spoofing - Replay-Attack: EER<BR>","<BR>task: Visual Odometry<BR>date: 2021-03-01<BR>ratio: 0.4221<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Visual Odometry<BR>date: 2021-04-01<BR>ratio: 0.5779<BR>benchmarks:<BR>  Face anti-spoofing - OULU-NPU: ACER<BR>","<BR>task: Visual Relationship Detection<BR>date: 2017-02-01<BR>ratio: 0.4347<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Phrase Detection: R@100<BR>  Visual Relationship Detection - VRD Phrase Detection: R@50<BR>  Visual Relationship Detection - VRD Relationship Detection: R@100<BR>  Visual Relationship Detection - VRD Relationship Detection: R@50<BR>","<BR>task: Visual Relationship Detection<BR>date: 2017-03-01<BR>ratio: 0.4671<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Phrase Detection: R@100<BR>  Visual Relationship Detection - VRD Phrase Detection: R@50<BR>  Visual Relationship Detection - VRD Relationship Detection: R@100<BR>  Visual Relationship Detection - VRD Relationship Detection: R@50<BR>","<BR>task: Visual Relationship Detection<BR>date: 2017-04-01<BR>ratio: 0.8502<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Phrase Detection: R@100<BR>  Visual Relationship Detection - VRD Predicate Detection: R@100<BR>  Visual Relationship Detection - VRD Predicate Detection: R@50<BR>  Visual Relationship Detection - VRD Relationship Detection: R@100<BR>","<BR>task: Visual Relationship Detection<BR>date: 2017-07-01<BR>ratio: 0.6405<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Phrase Detection: R@100<BR>  Visual Relationship Detection - VRD Phrase Detection: R@50<BR>  Visual Relationship Detection - VRD Predicate Detection: R@100<BR>  Visual Relationship Detection - VRD Predicate Detection: R@50<BR>  Visual Relationship Detection - VRD Relationship Detection: R@100<BR>  Visual Relationship Detection - VRD Relationship Detection: R@50<BR>","<BR>task: Visual Relationship Detection<BR>date: 2018-04-01<BR>ratio: 0.0096<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Predicate Detection: R@50<BR>","<BR>task: Visual Relationship Detection<BR>date: 2019-01-01<BR>ratio: 0.0147<BR>benchmarks:<BR>  Visual Relationship Detection - VRD Predicate Detection: R@50<BR>","<BR>task: Visual dialog<BR>date: 2016-11-01<BR>ratio: 0.1751<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2017-04-01<BR>ratio: 0.5468<BR>benchmarks:<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: Mean<BR>  Visual dialog - Visual Dialog v1.0 test-std: NDCG (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-10<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2017-06-01<BR>ratio: 0.3931<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2017-09-01<BR>ratio: 0.0043<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>","<BR>task: Visual dialog<BR>date: 2017-11-01<BR>ratio: 0.1665<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2018-09-01<BR>ratio: 0.1963<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-10<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2018-12-01<BR>ratio: 0.2136<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: Mean<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-10<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2019-02-01<BR>ratio: 0.2065<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: Mean<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2019-04-01<BR>ratio: 0.4272<BR>benchmarks:<BR>  Visual dialog - VisDial v0.9 val: MRR<BR>  Visual dialog - VisDial v0.9 val: Mean Rank<BR>  Visual dialog - VisDial v0.9 val: R-at-10<BR>  Visual dialog - VisDial v0.9 val: R-at-1<BR>  Visual dialog - VisDial v0.9 val: R-at-5<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: Mean<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-10<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-5<BR>","<BR>task: Visual dialog<BR>date: 2019-11-01<BR>ratio: 0.6129<BR>benchmarks:<BR>  Visual dialog - Visual Dialog v1.0 test-std: NDCG (x 100)<BR>","<BR>task: Visual dialog<BR>date: 2021-04-01<BR>ratio: 0.153<BR>benchmarks:<BR>  Visual dialog - Visual Dialog v1.0 test-std: MRR (x 100)<BR>  Visual dialog - Visual Dialog v1.0 test-std: R-at-1<BR>","<BR>task: Visual place recognition<BR>date: 2018-12-01<BR>ratio: 0.3749<BR>benchmarks:<BR>  Visual place recognition - Berlin Kudamm: Recall-at-1<BR>","<BR>task: Visual place recognition<BR>date: 2019-11-01<BR>ratio: 0.5626<BR>benchmarks:<BR>  Visual place recognition - Berlin Kudamm: Recall-at-1<BR>","<BR>task: Visual place recognition<BR>date: 2021-07-01<BR>ratio: 0.0626<BR>benchmarks:<BR>  Visual place recognition - Berlin Kudamm: Recall-at-1<BR>","<BR>task: Visual reasoning<BR>date: 2019-09-01<BR>ratio: 0.3096<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Test: Accuracy<BR>","<BR>task: Visual reasoning<BR>date: 2020-06-01<BR>ratio: 0.6481<BR>benchmarks:<BR>  Visual reasoning - PHYRE-1B-Cross: AUCCESS<BR>  Visual reasoning - PHYRE-1B-Within: AUCCESS<BR>","<BR>task: Visual reasoning<BR>date: 2020-08-01<BR>ratio: 0.6842<BR>benchmarks:<BR>  Visual reasoning - PHYRE-1B-Cross: AUCCESS<BR>  Visual reasoning - PHYRE-1B-Within: AUCCESS<BR>","<BR>task: Visual reasoning<BR>date: 2021-02-01<BR>ratio: 0.0745<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>","<BR>task: Visual reasoning<BR>date: 2021-04-01<BR>ratio: 0.0624<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>","<BR>task: Visual reasoning<BR>date: 2021-07-01<BR>ratio: 0.6304<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>  Visual reasoning - NLVR2 Test: Accuracy<BR>","<BR>task: Visual reasoning<BR>date: 2021-08-01<BR>ratio: 0.2439<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>  Visual reasoning - NLVR2 Test: Accuracy<BR>","<BR>task: Visual reasoning<BR>date: 2021-11-01<BR>ratio: 0.1604<BR>benchmarks:<BR>  Visual reasoning - NLVR2 Dev: Accuracy<BR>  Visual reasoning - NLVR2 Test: Accuracy<BR>","<BR>task: Weakly-supervised instance segmentation<BR>date: 2020-09-01<BR>ratio: 0.6462<BR>benchmarks:<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: Average Best Overlap<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>","<BR>task: Weakly-supervised instance segmentation<BR>date: 2021-03-01<BR>ratio: 0.8243<BR>benchmarks:<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: Average Best Overlap<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP-at-0.5<BR>  Weakly-supervised instance segmentation - PASCAL VOC 2012 val: mAP@0.75<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2015-07-01<BR>ratio: 0.0754<BR>benchmarks:<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2015-10-01<BR>ratio: 0.3844<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2016-11-01<BR>ratio: 0.1864<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2017-06-01<BR>ratio: 0.0955<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2017-07-01<BR>ratio: 0.9685<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - Olympics: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2018-03-01<BR>ratio: 0.0818<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2019-07-01<BR>ratio: 0.098<BR>benchmarks:<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2020-03-01<BR>ratio: 0.3773<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>","<BR>task: Zero-Shot Action Recognition<BR>date: 2021-08-01<BR>ratio: 0.1182<BR>benchmarks:<BR>  Zero-Shot Action Recognition - HMDB51: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - Olympics: Top-1 Accuracy<BR>  Zero-Shot Action Recognition - UCF101: Top-1 Accuracy<BR>"],"marker":{"color":[0.6711,0.2895,0.0132,0.0263,0.0148,0.7294,0.1552,0.1864,0.949,0.375,0.4841,0.7947,0.0643,0.1963,0.1206,0.0275,0.8755,0.9951,0.625,0.6195,0.0413,0.2319,0.2232,0.0634,0.2073,0.3693,0.1512,0.1014,0.0257,0.4017,0.3889,0.1659,0.0854,0.3593,0.022,0.8252,0.9993,0.0128,0.109,0.1282,0.75,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.0165,0.1754,0.2083,0.8352,0.0467,0.0442,0.7692,0.877,0.6651,0.5,0.939,0.88,0.967,0.8551,0.872,0.128,0.0412,0.5592,0.024,0.4753,0.2857,0.1204,0.016,0.9111,0.064,0.4167,0.1674,0.1216,0.1867,0.1486,0.0464,0.84,0.0142,0.0079,0.282,0.7835,0.0507,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.2867,0.0467,0.0442,0.7692,0.0044,0.5,0.939,0.88,0.967,0.8551,0.4719,0.5592,0.024,0.4559,0.2857,0.016,0.9111,0.064,0.4167,0.1674,0.0013,0.84,0.0142,0.0079,0.282,0.1716,0.1475,0.4856,0.3931,0.0844,0.2222,0.1593,0.5294,0.2727,0.3009,0.5468,0.3015,0.3368,0.3213,0.1667,0.8641,0.3918,0.0524,0.2199,0.3256,0.0061,0.1868,0.764,0.4188,0.3502,0.4386,0.9448,0.5163,0.2952,0.3321,0.4007,0.8519,0.5217,0.185,0.961,0.7658,0.2,0.7778,0.8359,0.7143,0.1359,0.3407,0.4396,0.8441,0.7648,0.5446,0.8987,0.6355,0.1152,0.0625,0.1394,0.3861,0.9104,0.3409,0.2632,0.1339,0.2308,0.0693,0.1046,0.6957,0.8916,0.1084,0.8125,0.9756,0.8554,0.1644,0.0244,0.2022,0.8757,0.6177,0.3288,0.7978,0.1773,0.3731,0.5075,0.5068,0.0909,0.5142,0.8136,0.2188,0.1475,0.3931,0.0844,0.2222,0.5294,0.2727,0.2563,0.3015,0.8641,0.0061,0.1868,0.764,0.4188,0.0656,0.9448,0.0041,0.039,0.8519,0.5217,0.961,0.7658,0.2,0.7778,0.8359,0.7143,0.3407,0.8441,0.7648,0.0208,0.2529,0.0513,0.2352,0.8333,0.2632,0.2308,0.028,0.8612,0.7375,0.1975,0.2133,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.2867,0.0467,0.0442,0.7692,0.0044,0.404,0.5,0.939,0.4274,0.88,0.967,0.8551,0.4719,0.5592,0.0807,0.024,0.4559,0.2857,0.016,0.9111,0.5977,0.064,0.6757,0.1674,0.0013,0.069,0.1724,0.1667,0.84,0.0142,0.8276,0.6552,0.0259,0.7274,0.4185,0.1627,0.302,0.3253,0.5448,0.869,0.2048,0.0251,0.6505,0.0962,0.25,0.0339,0.188,0.0253,0.5155,0.1325,0.1923,0.0539,0.5391,0.2651,0.493,0.4388,0.3562,0.5382,0.1727,0.0588,0.5256,0.3782,0.507,0.1378,0.1003,0.5006,0.174,0.2192,0.3794,0.4685,0.0235,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.2867,0.0467,0.0442,0.7692,0.0044,0.5,0.939,0.88,0.967,0.8551,0.4719,0.5592,0.024,0.4559,0.2857,0.016,0.9111,0.064,0.4167,0.1674,0.0013,0.84,0.0142,0.0079,0.282,0.4206,0.726,0.2726,0.2462,0.1095,0.1378,0.3057,0.4914,0.1598,0.3082,0.4167,0.5,0.2983,0.2867,0.4678,0.0442,0.7692,0.1673,0.7219,0.2303,0.5,0.939,0.88,0.967,0.8551,0.6555,0.0676,0.0071,0.5667,0.6875,0.3298,0.5592,0.0096,0.049,0.4559,0.8605,0.2857,0.108,0.9701,0.9111,0.1804,0.064,0.348,0.4167,0.4943,0.3977,0.9051,0.1022,0.0909,0.8571,0.7765,0.4753,0.84,0.0142,0.8571,0.7047,0.0617,0.2242,0.2404,0.3636,0.6265,0.0264,0.1806,0.0725,0.1793,0.2181,0.9701,0.348,0.0815,0.0181,0.4753,0.0815,0.6093,0.1674,0.2233,0.3491,0.0898,0.1321,0.357,0.1538,0.724,0.1013,0.1867,0.3964,0.031,0.0286,0.0443,0.2564,0.5294,0.3846,0.2247,0.4726,0.2793,0.0759,0.8385,0.7958,0.0897,0.4209,0.4206,0.726,0.2726,0.2462,0.1095,0.1378,0.3057,0.4914,0.8371,0.3082,0.4167,0.5,0.2983,0.2867,0.4678,0.0442,0.7692,0.1673,0.7219,0.2303,0.5,0.939,0.88,0.967,0.8551,0.6555,0.0676,0.0071,0.5667,0.6875,0.3298,0.5592,0.4124,0.049,0.4559,0.8605,0.2857,0.108,0.9701,0.9111,0.3226,0.0911,0.459,0.348,0.4167,0.4943,0.3977,0.9051,0.7836,0.0909,0.8571,0.7765,0.4753,0.84,0.0142,0.8571,0.7047,0.0617,0.2242,0.2404,0.3636,0.6265,0.1571,0.2692,0.8169,0.0344,0.0887,0.3057,0.9286,0.9509,0.2166,0.1592,0.0818,0.5164,0.679,0.7751,0.6753,0.5737,0.9628,0.1727,0.375,0.3527,0.3956,0.785,0.0602,0.3036,0.215,0.5476,0.7184,0.1985,0.2462,0.2795,0.1082,0.1378,0.061,0.4914,0.3082,0.4167,0.0165,0.1149,0.1754,0.2083,0.8352,0.0467,0.0442,0.8093,0.877,0.6651,0.5,0.939,0.0559,0.88,0.967,0.8551,0.213,0.3514,0.872,0.128,0.0412,0.621,0.024,0.4753,0.0782,0.2857,0.6745,0.6864,0.9111,0.3553,0.3739,0.3255,0.4167,0.1674,0.1434,0.7017,0.1486,0.8612,0.7375,0.84,0.4759,0.0079,0.282,0.7095,0.9369,0.0507,0.2744,0.5926,0.4636,0.5517,0.779,0.874,0.9095,0.6809,0.8529,0.5378,0.2533,0.264,0.0608,0.4622,0.0905,0.8699,0.2,0.1061,0.3765,0.5628,0.5633,0.8479,0.9995,0.6235,0.7256,0.7792,0.463,0.9399,0.5519,0.8784,0.3696,0.8766,0.9494,0.0012,0.5065,0.9988,0.7858,0.5012,0.2602,0.9942,0.3911,0.0846,0.0773,0.5793,0.0942,0.8858,0.3465,0.1188,0.5187,0.1348,0.5089,0.0954,0.5533,0.1962,0.3487,0.5548,0.1177,0.249,0.7414,0.3658,0.193,0.0202,0.1351,0.0856,0.1796,0.0428,0.8895,0.0846,0.1939,0.1105,0.9206,0.0794,0.4221,0.5779,0.8795,0.1205,0.3678,0.7065,0.5747,0.5269,0.2935,0.6854,0.4731,0.2673,0.5644,0.3791,0.0693,0.159,0.841,0.0693,0.6327,0.9412,0.7853,0.2941,0.2391,0.9987,0.1772,0.8352,0.3418,0.9434,0.011,0.0617,0.9673,0.1066,0.2143,0.7609,0.0292,0.0457,0.4949,0.7097,0.1899,0.8742,0.0991,0.1774,0.0644,0.4392,0.7143,0.0131,0.0187,0.9206,0.0211,0.1725,0.373,0.5366,0.5,0.1549,0.0794,0.3408,0.119,0.632,0.5753,0.0238,0.0604,0.5392,0.949,0.8252,0.9167,0.65,0.4221,0.5779,0.7895,0.051,0.4444,0.6364,0.9993,0.9656,0.3958,0.7187,0.7143,0.4171,0.6303,0.5116,0.96,0.0028,0.4231,0.0303,0.5156,0.2009,0.2923,0.1372,0.5987,0.8955,0.4555,0.2121,0.5789,0.2857,0.6682,0.0718,0.4444,0.575,0.1045,0.3333,0.2646,0.381,0.8229,0.2,0.3125,0.4844,0.0109,0.0838,0.3452,0.6473,0.711,0.1929,0.3831,0.1211,0.2589,0.8,0.8338,0.6648,0.181,0.8913,0.2667,0.9257,0.1176,0.8824,0.2844,0.5917,0.023,0.377,0.6316,0.3923,0.0367,0.3438,0.0079,0.7735,0.369,0.0824,0.9112,0.2667,0.4515,0.6795,0.2265,0.1876,0.3684,0.1042,0.3077,0.4437,0.5563,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.8352,0.0467,0.0442,0.7692,0.0044,0.5,0.939,0.88,0.967,0.8551,0.872,0.128,0.5592,0.024,0.4753,0.2857,0.016,0.9111,0.064,0.4167,0.1674,0.0013,0.84,0.0142,0.0079,0.282,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.8352,0.0467,0.5558,0.7692,0.0044,0.6475,0.5452,0.5,0.939,0.88,0.967,0.8551,0.6512,0.872,0.128,0.5592,0.0826,0.024,0.4753,0.4409,0.2958,0.016,0.9111,0.064,0.4167,0.1801,0.72,0.0013,0.1268,0.84,0.0142,0.0079,0.282,0.0194,0.96,0.0638,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.1754,0.2083,0.2867,0.9173,0.0442,0.7692,0.0044,0.5,0.939,0.88,0.967,0.8551,0.662,0.4719,0.5592,0.2124,0.024,0.4559,0.2857,0.016,0.9111,0.064,0.3805,0.4167,0.1674,0.0013,0.4071,0.84,0.0142,0.0079,0.282,0.3018,0.0861,0.9139,0.2049,0.1024,0.1896,0.2238,0.1941,0.1579,0.0344,0.9641,0.1273,0.3267,0.1149,0.1406,0.3366,0.266,0.4987,0.1657,0.1657,0.3717,0.2955,0.3164,0.0146,0.0525,0.1105,0.3983,0.051,0.0415,0.0998,0.0433,0.0169,0.1225,0.0373,0.2633,0.3312,0.0972,0.9513,0.0515,0.2743,0.8515,0.0461,0.2203,0.25,0.018,0.0539,0.4601,0.1266,0.3493,0.4511,0.7889,0.5542,0.9485,0.5031,0.9486,0.5803,0.9343,0.4167,0.1259,0.067,0.0847,0.9821,0.0752,0.3873,0.9367,0.3383,0.3284,0.0236,0.208,0.6889,0.955,0.8889,0.3011,0.7147,0.4969,0.0524,0.2385,0.2385,0.1865,0.2346,0.1019,0.8273,0.1186,0.1778,0.8222,0.267,0.5012,0.234,0.4298,0.2,0.6674,0.5693,0.9015,0.2783,0.4148,0.7432,0.0836,0.7342,0.2956,0.2676,0.95,0.9242,0.8565,0.1703,0.3689,0.366,0.7,0.7127,0.3986,0.012,0.5799,0.7432,0.5706,0.1191,0.9984,0.268,0.5568,0.0467,0.303,0.1543,0.55,0.6227,0.2131,0.499,0.2974,0.0318,0.4482,0.286,0.0098,0.2687,0.1061,0.0196,0.7906,0.7167,0.1608,0.9204,0.3355,0.2572,0.9349,0.8918,0.0643,0.9815,0.1548,0.0185,0.0403,0.7769,0.9838,0.1514,0.0159,0.0558,0.4674,0.7691,0.0528,0.5609,0.0791,0.2388,0.1045,0.6567,0.7063,0.9091,0.3588,0.4051,0.5949,0.4289,0.1963,0.1019,0.5571,0.0478,0.186,0.1416,0.1555,0.4496,0.8944,0.4024,0.0853,0.1799,0.1628,0.1163,0.2965,0.6485,0.1793,0.7756,0.241,0.0789,0.4581,0.058,0.188,0.6158,0.3341,0.0058,0.8127,0.0185,0.4017,0.0949,0.9051,0.041,0.4563,0.4415,0.076,0.1074,0.3878,0.1065,0.2835,0.028,0.0143,0.2241,0.5783,0.5176,0.073,0.0491,0.3602,0.6995,0.7118,0.2882,0.9301,0.2257,0.3774,0.2486,0.0943,0.6571,0.1213,0.4138,0.3043,0.8033,0.912,0.3301,0.8631,0.1835,0.5149,0.7927,0.8629,0.75,0.2916,0.0235,0.1304,0.3288,0.1213,0.4192,0.9765,0.1968,0.0833,0.6284,0.0336,0.1463,0.3591,0.6327,0.9412,0.2941,0.2391,0.9987,0.1772,0.8352,0.3418,0.9434,0.011,0.0617,0.9673,0.1066,0.2143,0.7609,0.0292,0.0457,0.4949,0.7097,0.1899,0.8409,0.396,0.5217,0.4176,0.9559,0.6924,0.0441,0.6096,0.3202,0.0148,0.4807,0.3842,0.0407,0.7294,0.1552,0.0646,0.2431,0.1654,0.1864,0.0509,0.877,0.949,0.9041,0.375,0.4841,0.7947,0.3697,0.8489,0.0643,0.5338,0.1206,0.7979,0.7805,0.4013,0.8755,0.9951,0.625,0.6195,0.2,0.1429,0.3825,0.2232,0.9091,0.1661,0.7742,0.3693,0.7978,0.4545,0.7143,0.8571,0.4878,0.6638,0.6488,0.6992,0.1321,0.8549,0.6364,0.8612,0.5652,0.037,0.1388,0.0854,0.897,0.818,0.2414,0.093,0.383,0.4419,0.3462,0.715,0.2326,0.0192,0.3404,0.7539,0.4252,0.2257,0.4848,0.4351,0.4234,0.5748,0.6216,0.7045,0.8522,0.0691,0.1261,0.4868,0.4474,0.0658,0.0843,0.2881,0.1571,0.2982,0.5714,0.0169,0.2456,0.2696,0.1545,0.1355,0.6159,0.4828,0.0094,0.2414,0.3594,0.1333,0.3271,0.0326,0.7877,0.1784,0.6897,0.2321,0.2277,0.0321,0.713,0.68,0.5909,0.0132,0.1117,0.2317,0.5613,0.7684,0.0536,0.5888,0.9003,0.0303,0.0347,0.3164,0.0347,0.4444,0.5556,0.9545,0.6315,0.5238,0.7059,0.8942,0.6842,0.5635,0.1607,0.519,0.714,0.1765,0.286,0.3963,0.0927,0.6636,0.0879,0.222,0.1681,0.0667,0.2778,0.9402,0.0317,0.2848,0.4922,0.1273,0.5833,0.2,0.6962,0.5316,0.9532,0.1854,0.9624,0.273,0.85,0.0317,0.4132,0.0317,0.5581,0.09,0.0244,0.7188,0.4286,0.3961,0.6104,0.929,0.6436,0.8881,0.6111,0.1119,0.8421,0.1579,0.3333,0.4667,0.0479,0.7906,0.933,0.9521,0.2366,0.9856,0.0779,0.328,0.3822,0.1932,0.3494,0.0357,0.4159,0.5107,0.2026,0.8223,0.3568,0.6051,0.6452,0.0464,0.2098,0.5459,0.7196,0.4379,0.75,0.2727,0.2746,0.7976,0.64,0.7407,0.1158,0.3899,0.2903,0.9182,0.444,0.7615,0.1721,0.1579,0.9758,0.36,0.5843,0.9975,0.3302,0.844,0.6667,0.1443,0.3333,0.0117,0.9167,0.0833,0.3333,0.6364,0.4444,0.5222,0.1667,0.0889,0.0278,0.3636,0.1111,0.0093,0.9048,0.8986,0.0071,0.0704,0.2609,0.0648,0.2032,0.4063,0.1219,0.2367,0.7278,0.0722,0.158,0.0045,0.0023,0.0355,0.874,0.2,0.8273,0.1186,0.1778,0.8222,0.267,0.5012,0.234,0.4298,0.2,0.6674,0.5693,0.9015,0.2783,0.4292,0.7432,0.0836,0.7342,0.2956,0.2676,0.1143,0.95,0.9242,0.8565,0.1703,0.3689,0.366,0.7,0.7127,0.6116,0.3986,0.012,0.5799,0.7432,0.7059,0.1191,0.9984,0.268,0.5568,0.0467,0.303,0.1543,0.55,0.0613,0.5126,0.5659,0.3247,0.4874,0.25,0.7049,0.75,0.2951,0.0794,0.9206,0.4767,0.3232,0.3086,0.1232,0.0779,0.2955,0.2457,0.7543,0.3852,0.783,0.0438,0.2773,0.0649,0.2518,0.2222,0.4562,0.2514,0.2536,0.0805,0.0091,0.459,0.5438,0.1511,0.2115,0.0701,0.4746,0.2222,0.9505,0.0167,0.6829,0.0168,0.2045,0.4615,0.0617,0.1727,0.2911,0.0495,0.8803,0.1111,0.1169,0.6104,0.929,0.6436,0.8881,0.6111,0.1119,0.8421,0.1579,0.3333,0.4667,0.0479,0.7906,0.933,0.9521,0.7091,0.2489,0.4954,0.2507,0.2323,0.2776,0.3839,0.3144,0.3622,0.2738,0.4776,0.8205,0.4446,0.3185,0.6062,0.308,0.5835,0.0933,0.9157,0.2012,0.3364,0.4524,0.6006,0.8076,0.1016,0.6159,0.0256,0.6578,0.6667,0.8812,0.7094,0.287,0.7432,0.0168,0.7273,0.1478,0.618,0.3529,0.191,0.75,0.2803,0.4924,0.2273,0.1725,0.8233,0.4985,0.1463,0.2604,0.0921,0.6457,0.807,0.0614,0.019,0.0789,0.0135,0.0439,0.0157,0.0056,0.1558,0.3571,0.2037,0.0802,0.0909,0.3961,0.5062,0.6037,0.2535,0.1223,0.0204,0.4365,0.5635,0.5451,0.1593,0.1124,0.0258,0.0122,0.0934,0.029,0.0143,0.0086,0.5985,0.4015,0.9697,0.1574,0.5602,0.0616,0.8567,0.2671,0.5018,0.785,0.9828,0.4982,0.9714,0.1748,0.4894,0.8841,0.9524,0.4167,0.9556,0.137,0.5455,0.5,0.8053,0.0508,0.8657,0.9565,0.3415,0.8837,0.5217,0.1364,0.0952,0.254,0.8795,0.1205,0.2673,0.5644,0.3791,0.0693,0.159,0.841,0.9697,0.1574,0.5602,0.785,0.9828,0.9714,0.1748,0.75,0.8571,0.5378,0.4622,0.6415,0.1509,0.2075,0.4689,0.3359,0.0144,0.1421,0.2514,0.6641,0.0164,0.0951,0.0977,0.1186,0.0055,0.2678,0.108,0.0929,0.7965,0.0437,0.0254,0.8421,0.082,0.4981,0.5019,0.8432,0.1568,0.7273,0.628,0.1909,0.9956,0.8408,0.0818,0.68,0.618,0.3529,0.191,0.75,0.0517,0.9483,0.0874,0.9126,0.2265,0.7735,0.8346,0.5085,0.7391,0.3555,0.5238,0.4482,0.0261,0.608,0.3754,0.9739,0.3163,0.5759,0.0892,0.1222,0.1827,0.6234,0.2373,0.275,0.0401,0.0244,0.4559,0.8088,0.0221,0.4817,0.0025,0.6,0.2887,0.3871,0.2757,0.6483,0.0849,0.2652,0.9199,0.4242,0.9865,0.9565,0.8957,0.5149,0.1066,0.0942,0.1795,0.2821,0.5699,0.4422,0.1329,0.0955,0.1383,0.8344,0.5723,0.98,0.6515,0.0879,0.0538,0.0598,0.96,0.0587,0.6,0.4,0.2588,0.5294,0.2118,0.9416,0.3208,0.869,0.6233,0.0377,0.0943,0.1529,0.0127,0.3909,0.1394,0.0437,0.1818,0.2171,0.0566,0.2264,0.1152,0.0873,0.9863,0.4937,0.1887,0.5063,0.7547,0.1582,0.7247,0.5537,0.8416,0.2915,0.7594,0.4678,0.0253,0.2812,0.9518,0.0482,0.2195,0.2155,0.7387,0.0242,0.0051,0.1997,0.187,0.7836,0.2164,0.7253,0.6218,0.9135,0.0989,0.3628,0.2838,0.3001,0.0796,0.1853,0.6957,0.8916,0.3876,0.1084,0.4351,0.5,0.5,0.7915,0.2164,0.4894,0.8841,0.9524,0.7143,0.2857,0.9556,0.2812,0.4604,0.1038,0.0612,0.8537,0.2168,0.0321,0.8757,0.7779,0.6177,0.3766,0.2987,0.028,0.19,0.0765,0.2547,0.042,0.2902,0.9655,0.2679,0.1119,0.042,0.4835,0.2308,0.4812,0.7154,0.1818,0.3933,0.2972,0.4059,0.2807,0.1791,0.8637,0.7472,0.7578,0.1823,0.6441,0.3559,0.6148,0.6215,0.8295,0.4058,0.2675,0.6,0.0134,0.3852,0.1856,0.2725,0.2416,0.0797,0.0458,0.3972,0.3102,0.1841,0.1414,0.0458,0.0458,0.4776,0.4583,0.2689,0.994,0.1381,0.4727,0.8126,0.0779,0.727,0.1374,0.4539,0.7333,0.2139,0.3657,0.146,0.3234,0.7143,0.0683,0.0229,0.7456,0.0652,0.1791,0.2422,0.5844,0.2226,0.273,0.95,0.6351,0.0364,0.4148,0.2667,0.2857,0.3636,0.0467,0.0199,0.5806,0.6109,0.2542,0.8464,0.0522,0.9533,0.202,0.2763,0.4744,0.798,0.3684,0.1715,0.0629,0.1772,0.268,0.2679,0.5549,0.5057,0.8799,0.1165,0.1043,0.1199,0.0037,0.7967,0.0169,0.9341,0.4561,0.1125,0.0441,0.0097,0.0678,0.0877,0.339,0.4561,0.2881,0.0237,0.4678,0.1673,0.1139,0.0676,0.5322,0.108,0.0125,0.4943,0.3977,0.0445,0.0338,0.4292,0.1143,0.0377,0.4169,0.0909,0.6116,0.1792,0.7059,0.5111,0.0613,0.4483,0.2101,0.2448,0.4542,0.0969,0.5458,0.8685,0.1315,0.8012,0.178,0.6053,0.4944,0.0655,0.771,0.5238,0.9286,0.0826,0.7973,0.7625,0.161,0.2375,0.2566,0.1603,0.8263,0.1917,0.4008,0.5029,0.0906,0.7499,0.1676,0.0347,0.0058,0.0405,0.0231,0.2601,0.8085,0.0311,0.0777,0.1731,0.0155,0.098,0.0196,0.2647,0.2647,0.3039,0.049,0.7781,0.8627,0.7231,0.5321,0.4521,0.8553,0.8206,0.7573,0.2427,0.2793,0.0613,0.6595,0.4286,0.2738,0.1333,0.4933,0.118,0.2528,0.2022,0.0133,0.3684,0.7809,0.6316,0.2191,0.8267,0.5463,0.1733,0.4537,0.97,0.03,0.7668,0.2332,0.1475,0.5128,0.0844,0.2222,0.5294,0.2727,0.2563,0.3015,0.1761,0.8641,0.612,0.4027,0.2253,0.1868,0.764,0.4188,0.3649,0.0656,0.9448,0.2179,0.0041,0.5506,0.039,0.8519,0.5217,0.0133,0.961,0.7658,0.2,0.7778,0.8359,0.8468,0.7143,0.6304,0.3407,0.8441,0.9497,0.9937,0.0465,0.2529,0.5413,0.2352,0.8333,0.2632,0.2308,0.028,0.9344,0.0656,0.2824,0.5079,0.1746,0.1454,0.1415,0.1863,0.022,0.0344,0.9286,0.9509,0.4883,0.0491,0.3527,0.2462,0.1378,0.061,0.4914,0.3082,0.4167,0.0797,0.1754,0.2083,0.6114,0.0467,0.0442,0.7692,0.0044,0.3438,0.0156,0.5,0.939,0.88,0.967,0.8551,0.7234,0.138,0.4719,0.5592,0.3438,0.3793,0.4559,0.2766,0.2857,0.1259,0.016,0.9111,0.0774,0.064,0.4167,0.3755,0.0962,0.0013,0.1887,0.6617,0.84,0.2543,0.2414,0.282,0.0625,0.2069,0.3981,0.5039,0.4961,0.6093,0.1674,0.2233,0.3149,0.0433,0.0835,0.1489,0.0459,0.259,0.6164,0.8511,0.1325,0.0131,0.2289,0.5471,0.1867,0.2761,0.0179,0.3843,0.5646,0.2318,0.7239,0.3148,0.4255,0.0502,0.6897,0.6067,0.28,0.034,0.0806,0.0138,0.72,0.9674,0.4091,0.3427,0.5225,0.2056,0.1563,0.2969,0.125,0.4219,0.7342,0.1006,0.1194,0.3725,0.7639,0.1189,0.9963,0.7949,0.3138,0.2051,0.8866,0.4206,0.726,0.2726,0.2462,0.1095,0.1378,0.3057,0.4914,0.1598,0.3082,0.4167,0.5,0.2983,0.2867,0.4678,0.0442,0.7692,0.1673,0.7219,0.2303,0.5,0.939,0.88,0.967,0.8551,0.6555,0.0676,0.0071,0.5667,0.6875,0.9489,0.3298,0.5592,0.0096,0.0511,0.4559,0.8605,0.2857,0.108,0.9701,0.9111,0.1804,0.064,0.348,0.4167,0.4943,0.3977,0.9051,0.1022,0.0909,0.8571,0.7765,0.4753,0.84,0.0142,0.8571,0.7047,0.0617,0.2242,0.2404,0.3636,0.6265,0.4565,0.8462,0.8571,0.0865,0.2521,0.274,0.584,0.4,0.7347,0.8696,0.6,0.1171,0.2492,0.0387,0.1622,0.0281,0.669,0.613,0.0315,0.5269,0.031,0.4731,0.0463,0.032,0.2834,0.4499,0.1943,0.0363,0.486,0.0399,0.3729,0.2111,0.0483,0.2094,0.6077,0.2271,0.9697,0.6103,0.5602,0.6767,0.6532,0.0187,0.68,0.1517,0.2105,0.0475,0.5528,0.0909,0.1721,0.1093,0.7931,0.351,0.9828,0.0359,0.005,0.8222,0.9474,0.3015,0.9714,0.1748,0.9128,0.5422,0.8457,0.1961,0.8307,0.4578,0.2417,0.0957,0.08,0.0872,0.2592,0.9211,0.8216,0.0431,0.8926,0.5869,0.0663,0.1768,0.6612,0.7338,0.2099,0.0016,0.4709,0.6583,0.0861,0.5291,0.0259,0.1727,0.846,0.1863,0.5789,0.4211,0.0932,0.2692,0.1058,0.5429,0.2173,0.3212,0.7183,0.7418,0.75,0.54,0.5222,0.2886,0.9783,0.3023,0.0506,0.7766,0.5125,0.0371,0.1,0.9,0.9412,0.0196,0.0392,0.4333,0.4741,0.2251,0.1639,0.6,0.7286,0.9574,0.4821,0.198,0.1367,0.1761,0.612,0.4027,0.3649,0.393,0.5506,0.3173,0.0133,0.0664,0.8468,0.6304,0.9497,0.9937,0.0465,0.5413,0.9206,0.0794,0.4221,0.5779,0.4347,0.4671,0.8502,0.6405,0.0096,0.0147,0.1751,0.5468,0.3931,0.0043,0.1665,0.1963,0.2136,0.2065,0.4272,0.6129,0.153,0.3749,0.5626,0.0626,0.3096,0.6481,0.6842,0.0745,0.0624,0.6304,0.2439,0.1604,0.6462,0.8243,0.0754,0.3844,0.1864,0.0955,0.9685,0.0818,0.098,0.3773,0.1182],"colorbar":{"len":500,"lenmode":"pixels","thickness":10,"title":{"text":"ratio"},"x":-0.1},"colorscale":[[0.0,"rgb(255,255,229)"],[0.125,"rgb(247,252,185)"],[0.25,"rgb(217,240,163)"],[0.375,"rgb(173,221,142)"],[0.5,"rgb(120,198,121)"],[0.625,"rgb(65,171,93)"],[0.75,"rgb(35,132,67)"],[0.875,"rgb(0,104,55)"],[1.0,"rgb(0,69,41)"]],"opacity":0.7,"showscale":true,"size":17,"symbol":"diamond-tall","line":{"color":"gray","width":1}},"mode":"markers","x":["2017-12-01T00:00:00","2019-01-01T00:00:00","2020-08-01T00:00:00","2021-09-01T00:00:00","2015-12-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-07-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-08-01T00:00:00","2021-12-01T00:00:00","2019-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-10-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2014-09-01T00:00:00","2015-08-01T00:00:00","2015-09-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-01-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-07-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2021-08-01T00:00:00","2016-06-01T00:00:00","2016-12-01T00:00:00","2017-12-01T00:00:00","2018-04-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-01-01T00:00:00","2019-08-01T00:00:00","2020-05-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-09-01T00:00:00","2021-12-01T00:00:00","2017-11-01T00:00:00","2018-04-01T00:00:00","2015-08-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-07-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-11-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2018-06-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2016-01-01T00:00:00","2016-09-01T00:00:00","2017-03-01T00:00:00","2017-05-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-09-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2014-12-01T00:00:00","2015-03-01T00:00:00","2015-05-01T00:00:00","2015-06-01T00:00:00","2016-01-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-08-01T00:00:00","2018-11-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2014-12-01T00:00:00","2015-05-01T00:00:00","2016-05-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00","2021-10-01T00:00:00","2017-12-01T00:00:00","2018-07-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2020-06-01T00:00:00","2021-02-01T00:00:00","2021-08-01T00:00:00","2014-12-01T00:00:00","2015-03-01T00:00:00","2015-05-01T00:00:00","2015-06-01T00:00:00","2016-01-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00","2021-12-01T00:00:00","2017-01-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-01-01T00:00:00","2015-06-01T00:00:00","2015-07-01T00:00:00","2015-10-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-02-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-11-01T00:00:00","2016-02-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-09-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-12-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2020-05-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2017-11-01T00:00:00","2018-01-01T00:00:00","2018-03-01T00:00:00","2018-09-01T00:00:00","2019-02-01T00:00:00","2019-06-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-12-01T00:00:00","2015-11-01T00:00:00","2015-12-01T00:00:00","2016-01-01T00:00:00","2016-08-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2018-02-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2021-07-01T00:00:00","2015-11-01T00:00:00","2019-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2016-05-01T00:00:00","2017-08-01T00:00:00","2018-03-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-05-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-07-01T00:00:00","2015-11-01T00:00:00","2016-07-01T00:00:00","2017-04-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00","2014-09-01T00:00:00","2015-02-01T00:00:00","2015-05-01T00:00:00","2016-05-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-02-01T00:00:00","2020-11-01T00:00:00","2021-12-01T00:00:00","2020-08-01T00:00:00","2021-01-01T00:00:00","2015-01-01T00:00:00","2017-07-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2016-08-01T00:00:00","2018-09-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2017-05-01T00:00:00","2017-09-01T00:00:00","2018-04-01T00:00:00","2016-04-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-12-01T00:00:00","2013-02-01T00:00:00","2013-12-01T00:00:00","2014-06-01T00:00:00","2014-09-01T00:00:00","2014-12-01T00:00:00","2015-02-01T00:00:00","2015-06-01T00:00:00","2015-09-01T00:00:00","2015-12-01T00:00:00","2016-02-01T00:00:00","2016-03-01T00:00:00","2016-05-01T00:00:00","2016-08-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2017-01-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2019-12-01T00:00:00","2020-12-01T00:00:00","2021-06-01T00:00:00","2014-09-01T00:00:00","2016-03-01T00:00:00","2016-05-01T00:00:00","2017-03-01T00:00:00","2017-06-01T00:00:00","2017-11-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-07-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2014-07-01T00:00:00","2015-12-01T00:00:00","2016-11-01T00:00:00","2017-05-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-07-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2018-07-01T00:00:00","2019-02-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2018-04-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-10-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-11-01T00:00:00","2017-05-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2015-12-01T00:00:00","2019-11-01T00:00:00","2018-03-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-02-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2016-06-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2021-03-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2017-10-01T00:00:00","2018-03-01T00:00:00","2018-10-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00","2017-11-01T00:00:00","2019-05-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2017-02-01T00:00:00","2018-02-01T00:00:00","2018-07-01T00:00:00","2020-05-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2015-11-01T00:00:00","2016-05-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2018-04-01T00:00:00","2018-07-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-04-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-09-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-07-01T00:00:00","2021-11-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-01-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2014-06-01T00:00:00","2015-12-01T00:00:00","2016-01-01T00:00:00","2016-06-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-02-01T00:00:00","2017-03-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-12-01T00:00:00","2019-01-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-10-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-09-01T00:00:00","2016-04-01T00:00:00","2017-04-01T00:00:00","2018-10-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-05-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2018-03-01T00:00:00","2018-05-01T00:00:00","2018-10-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-03-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2017-03-01T00:00:00","2018-01-01T00:00:00","2018-09-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2017-06-01T00:00:00","2019-03-01T00:00:00","2016-06-01T00:00:00","2017-08-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-12-01T00:00:00","2020-04-01T00:00:00","2021-04-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2015-04-01T00:00:00","2015-11-01T00:00:00","2016-04-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2017-06-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2017-12-01T00:00:00","2018-03-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-04-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2017-09-01T00:00:00","2019-09-01T00:00:00","2018-08-01T00:00:00","2019-01-01T00:00:00","2020-01-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-08-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2018-12-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-09-01T00:00:00","2017-12-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00","2021-06-01T00:00:00","2017-07-01T00:00:00","2019-10-01T00:00:00","2014-09-01T00:00:00","2016-03-01T00:00:00","2016-05-01T00:00:00","2017-03-01T00:00:00","2017-06-01T00:00:00","2017-11-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-07-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2020-07-01T00:00:00","2021-04-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00","2021-10-01T00:00:00","2016-09-01T00:00:00","2017-07-01T00:00:00","2019-04-01T00:00:00","2021-07-01T00:00:00","2014-06-01T00:00:00","2015-04-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-05-01T00:00:00","2016-09-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-08-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-03-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2017-06-01T00:00:00","2018-02-01T00:00:00","2018-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2016-06-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-10-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2017-12-01T00:00:00","2018-12-01T00:00:00","2019-07-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2021-04-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2021-02-01T00:00:00","2019-04-01T00:00:00","2019-11-01T00:00:00","2020-07-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-12-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2019-05-01T00:00:00","2020-11-01T00:00:00","2021-07-01T00:00:00","2018-12-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2020-05-01T00:00:00","2021-03-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2020-04-01T00:00:00","2020-08-01T00:00:00","2021-01-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2017-12-01T00:00:00","2021-11-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-06-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-12-01T00:00:00","2018-06-01T00:00:00","2018-09-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-03-01T00:00:00","2021-06-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00","2018-06-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-09-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2016-05-01T00:00:00","2017-08-01T00:00:00","2020-10-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-06-01T00:00:00","2018-06-01T00:00:00","2018-09-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2020-08-01T00:00:00","2021-05-01T00:00:00","2018-06-01T00:00:00","2018-12-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00","2020-03-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-04-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2020-04-01T00:00:00","2020-10-01T00:00:00","2018-04-01T00:00:00","2018-07-01T00:00:00","2019-08-01T00:00:00","2019-11-01T00:00:00","2016-12-01T00:00:00","2018-02-01T00:00:00","2019-01-01T00:00:00","2019-05-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2021-08-01T00:00:00","2019-10-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2021-02-01T00:00:00","2018-11-01T00:00:00","2021-06-01T00:00:00","2017-09-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2020-11-01T00:00:00","2018-07-01T00:00:00","2018-12-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-10-01T00:00:00","2016-03-01T00:00:00","2016-04-01T00:00:00","2016-07-01T00:00:00","2016-10-01T00:00:00","2016-11-01T00:00:00","2017-01-01T00:00:00","2017-03-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2021-09-01T00:00:00","2021-12-01T00:00:00","2017-06-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2016-08-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2018-01-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2020-05-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-05-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2018-08-01T00:00:00","2019-06-01T00:00:00","2019-11-01T00:00:00","2020-06-01T00:00:00","2021-08-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2019-10-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-09-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2019-08-01T00:00:00","2021-02-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2019-02-01T00:00:00","2019-05-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2021-03-01T00:00:00","2021-11-01T00:00:00","2018-02-01T00:00:00","2018-07-01T00:00:00","2019-01-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-02-01T00:00:00","2021-05-01T00:00:00","2021-11-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2021-08-01T00:00:00","2021-12-01T00:00:00","2020-03-01T00:00:00","2021-02-01T00:00:00","2020-07-01T00:00:00","2021-04-01T00:00:00","2019-10-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-10-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00","2015-07-01T00:00:00","2016-03-01T00:00:00","2016-09-01T00:00:00","2017-11-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-04-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2015-04-01T00:00:00","2016-04-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-09-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-08-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-05-01T00:00:00","2021-11-01T00:00:00","2014-11-01T00:00:00","2014-12-01T00:00:00","2015-02-01T00:00:00","2015-03-01T00:00:00","2015-04-01T00:00:00","2015-09-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-05-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2017-11-01T00:00:00","2018-04-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2016-12-01T00:00:00","2017-06-01T00:00:00","2018-01-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-12-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2019-04-01T00:00:00","2019-10-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2021-02-01T00:00:00","2019-03-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-12-01T00:00:00","2017-08-01T00:00:00","2018-05-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00","2019-10-01T00:00:00","2019-04-01T00:00:00","2020-04-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-05-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-09-01T00:00:00","2021-12-01T00:00:00","2015-02-01T00:00:00","2015-11-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2018-08-01T00:00:00","2018-09-01T00:00:00","2018-11-01T00:00:00","2019-03-01T00:00:00","2019-09-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-04-01T00:00:00","2021-11-01T00:00:00","2016-12-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2021-05-01T00:00:00","2021-10-01T00:00:00","2019-06-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-12-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2020-07-01T00:00:00","2021-03-01T00:00:00","2021-10-01T00:00:00","2019-10-01T00:00:00","2020-09-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-05-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2020-05-01T00:00:00","2020-07-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2020-04-01T00:00:00","2021-09-01T00:00:00","2020-07-01T00:00:00","2021-09-01T00:00:00","2015-08-01T00:00:00","2015-11-01T00:00:00","2016-03-01T00:00:00","2016-06-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00","2018-09-01T00:00:00","2019-06-01T00:00:00","2020-03-01T00:00:00","2020-10-01T00:00:00","2021-01-01T00:00:00","2021-05-01T00:00:00","2021-10-01T00:00:00","2017-01-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2021-01-01T00:00:00","2015-06-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-12-01T00:00:00","2021-02-01T00:00:00","2014-12-01T00:00:00","2015-05-01T00:00:00","2016-05-01T00:00:00","2015-04-01T00:00:00","2016-08-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-01-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-02-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-07-01T00:00:00","2021-10-01T00:00:00","2021-12-01T00:00:00","2019-02-01T00:00:00","2019-05-01T00:00:00","2020-04-01T00:00:00","2021-04-01T00:00:00","2019-08-01T00:00:00","2020-05-01T00:00:00","2020-08-01T00:00:00","2020-11-01T00:00:00","2021-01-01T00:00:00","2019-04-01T00:00:00","2020-02-01T00:00:00","2020-05-01T00:00:00","2020-06-01T00:00:00","2020-11-01T00:00:00","2021-07-01T00:00:00","2014-12-01T00:00:00","2015-03-01T00:00:00","2015-05-01T00:00:00","2015-06-01T00:00:00","2016-01-01T00:00:00","2016-04-01T00:00:00","2016-06-01T00:00:00","2016-07-01T00:00:00","2016-08-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-05-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-08-01T00:00:00","2017-10-01T00:00:00","2017-11-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-05-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-05-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2019-11-01T00:00:00","2019-12-01T00:00:00","2020-01-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-05-01T00:00:00","2021-06-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-10-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2019-07-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00","2017-11-01T00:00:00","2018-10-01T00:00:00","2019-04-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2016-11-01T00:00:00","2017-07-01T00:00:00","2017-10-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-11-01T00:00:00","2019-07-01T00:00:00","2019-12-01T00:00:00","2020-03-01T00:00:00","2020-11-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2019-07-01T00:00:00","2019-08-01T00:00:00","2020-07-01T00:00:00","2020-09-01T00:00:00","2021-09-01T00:00:00","2018-02-01T00:00:00","2019-05-01T00:00:00","2020-03-01T00:00:00","2020-07-01T00:00:00","2020-11-01T00:00:00","2021-06-01T00:00:00","2021-12-01T00:00:00","2015-06-01T00:00:00","2015-12-01T00:00:00","2016-06-01T00:00:00","2016-11-01T00:00:00","2016-12-01T00:00:00","2017-01-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2017-09-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-03-01T00:00:00","2018-06-01T00:00:00","2018-07-01T00:00:00","2018-09-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-02-01T00:00:00","2019-03-01T00:00:00","2019-04-01T00:00:00","2019-06-01T00:00:00","2019-08-01T00:00:00","2019-09-01T00:00:00","2019-10-01T00:00:00","2020-01-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-04-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-10-01T00:00:00","2021-01-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-06-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2016-05-01T00:00:00","2017-06-01T00:00:00","2017-10-01T00:00:00","2017-12-01T00:00:00","2018-02-01T00:00:00","2018-04-01T00:00:00","2018-10-01T00:00:00","2018-11-01T00:00:00","2019-05-01T00:00:00","2020-02-01T00:00:00","2020-03-01T00:00:00","2020-05-01T00:00:00","2021-07-01T00:00:00","2018-09-01T00:00:00","2020-02-01T00:00:00","2020-05-01T00:00:00","2020-11-01T00:00:00","2021-03-01T00:00:00","2016-12-01T00:00:00","2017-07-01T00:00:00","2018-04-01T00:00:00","2018-06-01T00:00:00","2018-08-01T00:00:00","2019-06-01T00:00:00","2019-07-01T00:00:00","2020-02-01T00:00:00","2020-07-01T00:00:00","2021-02-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2021-06-01T00:00:00","2021-08-01T00:00:00","2021-09-01T00:00:00","2021-11-01T00:00:00","2021-12-01T00:00:00","2018-08-01T00:00:00","2020-08-01T00:00:00","2016-12-01T00:00:00","2020-04-01T00:00:00","2021-02-01T00:00:00","2018-11-01T00:00:00","2018-12-01T00:00:00","2019-10-01T00:00:00","2019-12-01T00:00:00","2020-11-01T00:00:00","2020-12-01T00:00:00","2021-04-01T00:00:00","2021-12-01T00:00:00","2016-09-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-12-01T00:00:00","2018-01-01T00:00:00","2018-06-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-03-01T00:00:00","2019-05-01T00:00:00","2019-09-01T00:00:00","2019-11-01T00:00:00","2020-01-01T00:00:00","2020-06-01T00:00:00","2020-07-01T00:00:00","2020-08-01T00:00:00","2020-12-01T00:00:00","2015-11-01T00:00:00","2019-01-01T00:00:00","2021-03-01T00:00:00","2021-04-01T00:00:00","2017-02-01T00:00:00","2017-03-01T00:00:00","2017-04-01T00:00:00","2017-07-01T00:00:00","2018-04-01T00:00:00","2019-01-01T00:00:00","2016-11-01T00:00:00","2017-04-01T00:00:00","2017-06-01T00:00:00","2017-09-01T00:00:00","2017-11-01T00:00:00","2018-09-01T00:00:00","2018-12-01T00:00:00","2019-02-01T00:00:00","2019-04-01T00:00:00","2019-11-01T00:00:00","2021-04-01T00:00:00","2018-12-01T00:00:00","2019-11-01T00:00:00","2021-07-01T00:00:00","2019-09-01T00:00:00","2020-06-01T00:00:00","2020-08-01T00:00:00","2021-02-01T00:00:00","2021-04-01T00:00:00","2021-07-01T00:00:00","2021-08-01T00:00:00","2021-11-01T00:00:00","2020-09-01T00:00:00","2021-03-01T00:00:00","2015-07-01T00:00:00","2015-10-01T00:00:00","2016-11-01T00:00:00","2017-06-01T00:00:00","2017-07-01T00:00:00","2018-03-01T00:00:00","2019-07-01T00:00:00","2020-03-01T00:00:00","2021-08-01T00:00:00"],"y":["3D Point Cloud Linear Classification","3D Point Cloud Linear Classification","3D Point Cloud Linear Classification","3D Point Cloud Linear Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Classification","3D vision process // 3D Depth Estimation","3D vision process // 3D Depth Estimation","3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation","3D vision process // 3D Shape Representation","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D car instance understanding","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human action recognition","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human pose estimation","3D vision process // 3D human reconstruction","3D vision process // 3D human reconstruction","3D vision process // 3D human reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D reconstruction","3D vision process // 3D shape classification","3D vision process // 3D shape classification","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // 3D shape reconstruction","3D vision process // Motion forecasting","3D vision process // Motion forecasting","3D vision process // Motion forecasting","3D vision process // Motion forecasting","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action detection","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Action segmentation","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // 3D human action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Action recognition in videos","Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Activity recognition in videos","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Temporal action proposal generation","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action localization // Temporal action localization // Weakly supervised action localization","Action quality assessment","Action quality assessment","Action quality assessment","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Ad-hoc video search","Ad-hoc video search","Ad-hoc video search","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Anomaly detection","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Autonomous vehicle task","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Biomedical vision process","Blind face restoration","Blind face restoration","Blind face restoration","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Continual learning","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Crowds","Deception detection","Deception detection","Deception detection","Deception detection","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Dehazing","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Denoising","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth completion","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Depth estimation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Domain adaptation","Edge detection","Edge detection","Edge detection","Emotion classification","Emotion classification","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Hand-related vision process","Horizon line estimation","Horizon line estimation","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human interaction recognition","Human parsing","Human parsing","Human parsing","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Human-object interaction detection","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Document image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Few-shot image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Fine-grained image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Hyperspectral image classification","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Learning with noisy labels","Image classification // Photo geolocation estimation","Image classification // Photo geolocation estimation","Image classification // Photo geolocation estimation","Image classification // Satellite image classification","Image classification // Satellite image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Self-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Semi-supervised image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Sequential image classification","Image classification // Superpixel image classification","Image classification // Superpixel image classification","Image classification // Superpixel image classification","Image classification // Superpixel image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image classification // Unsupervised image classification","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image clustering","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image denoising","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image enhancement","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matching","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image matting","Image quality assessment","Image quality assessment","Image quality assessment","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image question answering","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image recognition","Image reconstruction","Image reconstruction","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image restoration","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image retrieval","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image tagging","Image/document clustering","Image/document clustering","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Instance segmentation","Intelligent surveillance","Intelligent surveillance","Intelligent surveillance","Intelligent surveillance","Intelligent surveillance","Interest point detection","Interest point detection","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Keyword spotting","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Line segment detection","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Material property prediction","Medical diagnosis","Medical diagnosis","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Meta-learning","Multi-target domain adaptation","Multi-target domain adaptation","Multi-target domain adaptation","Multi-target domain adaptation","Multimodal machine translation","Multimodal machine translation","Multimodal machine translation","Multimodal machine translation","Multispectral Object Detection","Multispectral Object Detection","Object counting","Object counting","Object counting","Object counting","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 2D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // 3D object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Birds eye view object detection","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Camouflaged object segmentation","Object detection // Dense object detection","Object detection // Dense object detection","Object detection // Dense object detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Face detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Few-shot object detection","Object detection // Medical object detection","Object detection // Medical object detection","Object detection // Medical object detection","Object detection // Medical object detection","Object detection // Object Detection In Indoor Scenes","Object detection // Object Detection In Indoor Scenes","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object detection in aerial images","Object detection // Object proposal generation","Object detection // Object proposal generation","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // RGB-D salient object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Real-time object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Salient object detection","Object detection // Surgical tool detection","Object detection // Surgical tool detection","Object detection // Video object detection","Object detection // Video object detection","Object detection // Video object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object detection // Weakly supervised object detection","Object localization","Object localization","Object localization","Object localization","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object reconstruction","Object segmentation","Object segmentation","Object segmentation","Object segmentation","Optical character recognition","Optical character recognition","Optical flow estimation","Optical flow estimation","Optical flow estimation","Optical flow estimation","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Out-of-distribution detection","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person re-identification","Person search","Person search","Point Cloud Segmentation","Point Cloud Segmentation","Point Cloud Segmentation","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud classification","Point cloud generation","Point cloud generation","Point cloud generation","Point cloud generation","Point cloud generation","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud registration","Point cloud super resolution","Point cloud super resolution","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking","Quantization","Quantization","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Rain removal","Reconstruction","Reconstruction","Reconstruction","Reconstruction","Reconstruction","Remote sensing","Remote sensing","Robot navigation","Robot navigation","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Saliency detection","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene parsing","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Scene text detection","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 2D Semantic Segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D part segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // 3D semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Few-shot semantic segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // Human part segmentation","Semantic segmentation // LIDAR semantic segmentation","Semantic segmentation // LIDAR semantic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Panoptic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Real-time semantic segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Scene segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Semi-supervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Unsupervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semantic segmentation // Weakly-supervised semantic segmentation","Semi-supervised object detection","Semi-supervised object detection","Semi-supervised object detection","Semi-supervised object detection","Sign language recognition","Sign language recognition","Sign language recognition","Sign language recognition","Sign language translation","Sign language translation","Single-object discovery","Single-object discovery","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Super-resolution","Surface normals estimation","Surface normals estimation","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Text based person retrieval","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Abnormal event detection in video","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action classification","Video process // Action spotting","Video process // Action spotting","Video process // Activity recognition in videos","Video process // Activity recognition in videos","Video process // Activity recognition in videos","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Object tracking","Video process // Video Enhancement","Video process // Video Enhancement","Video process // Video Enhancement","Video process // Video Enhancement","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video Quality Assessment","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video captioning","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video classification","Video process // Video denoising","Video process // Video denoising","Video process // Video denoising","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video frame interpolation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video generation","Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting","Video process // Video inpainting","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video instance segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video object segmentation","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video prediction","Video process // Video question answering","Video process // Video question answering","Video process // Video question answering","Video process // Video question answering","Video process // Video question answering","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video retrieval","Video process // Video segmentation","Video process // Video segmentation","Video process // Video semantic segmentation","Video process // Video semantic segmentation","Video process // Video semantic segmentation","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video summarization","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Video process // Video super-resolution","Visual Odometry","Visual Odometry","Visual Odometry","Visual Odometry","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual Relationship Detection","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual dialog","Visual place recognition","Visual place recognition","Visual place recognition","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Visual reasoning","Weakly-supervised instance segmentation","Weakly-supervised instance segmentation","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition","Zero-Shot Action Recognition"],"type":"scatter","line":{"color":"black","width":0}}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"},"showgrid":true,"gridcolor":"lightBlue","tickmode":"auto"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{},"showgrid":true,"gridcolor":"lightBlue","side":"right"},"legend":{"title":{"text":"color"},"tracegroupgap":0},"margin":{"t":60},"title":{"text":"Vision process","y":0.995},"font":{"size":14},"showlegend":false,"plot_bgcolor":"white","height":5872.5,"width":1500},                        {"responsive": true}                    )                };                            </script>        </div>
</body>
</html>