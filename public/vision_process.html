<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="4e891758-b8ec-4b1f-bd3d-265b58cbe76d" class="plotly-graph-div" style="height:4023.0000000000005px; width:1500px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("4e891758-b8ec-4b1f-bd3d-265b58cbe76d")) {                    Plotly.newPlot(                        "4e891758-b8ec-4b1f-bd3d-265b58cbe76d",                        [{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Action localization: Temporal Action Localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Action localization: Temporal Action Localization","orientation":"v","showlegend":true,"x":["2017-03","2018-04","2017-05","2019-04","2016-09","2016-01","2019-06","2019-07","2019-09","2019-11","2018-06","2019-03"],"xaxis":"x","y":["Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Action localization: Action Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Action localization: Action Segmentation","orientation":"v","showlegend":true,"x":["2016-11","2018-06","2019-03","2020-03"],"xaxis":"x","y":["Action localization: Action Segmentation","Action localization: Action Segmentation","Action localization: Action Segmentation","Action localization: Action Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity detection: Action Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity detection: Action Detection","orientation":"v","showlegend":true,"x":["2017-03","2017-12","2018-03","2019-04"],"xaxis":"x","y":["Activity detection: Action Detection","Activity detection: Action Detection","Activity detection: Action Detection","Activity detection: Action Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity localization: Weakly Supervised Action Localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity localization: Weakly Supervised Action Localization","orientation":"v","showlegend":true,"x":["2019-06","2017-12","2018-07","2019-05","2019-08","2019-11"],"xaxis":"x","y":["Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity localization: Temporal Action Proposal Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity localization: Temporal Action Proposal Generation","orientation":"v","showlegend":true,"x":["2019-07","2018-11","2018-06"],"xaxis":"x","y":["Activity localization: Temporal Action Proposal Generation","Activity localization: Temporal Action Proposal Generation","Activity localization: Temporal Action Proposal Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Action Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Action Classification","orientation":"v","showlegend":true,"x":["2019-06","2019-05","2019-04","2018-11","2018-12","2018-10","2018-07","2018-06","2017-12","2017-11","2017-05","2016-12"],"xaxis":"x","y":["Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Action Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Action Recognition","orientation":"v","showlegend":true,"x":["2014-06","2016-04","2015-05","2015-12","2017-03","2016-08","2016-01","2014-12","2017-04","2015-03","2017-05","2019-12","2019-08","2019-07","2019-06","2019-05","2019-04","2019-01","2018-12","2018-11","2018-10","2018-07","2018-06","2018-01","2017-12","2017-11"],"xaxis":"x","y":["Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Group Activity Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Group Activity Recognition","orientation":"v","showlegend":true,"x":["2018-11","2019-04"],"xaxis":"x","y":["Activity recognition: Group Activity Recognition","Activity recognition: Group Activity Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Human Interaction Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Human Interaction Recognition","orientation":"v","showlegend":true,"x":["2017-06","2018-11"],"xaxis":"x","y":["Activity recognition: Human Interaction Recognition","Activity recognition: Human Interaction Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Egocentric Activity Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Egocentric Activity Recognition","orientation":"v","showlegend":true,"x":["2018-11","2018-12","2019-05","2019-08","2020-02"],"xaxis":"x","y":["Activity recognition: Egocentric Activity Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Egocentric Activity Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Multimodal Activity Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Multimodal Activity Recognition","orientation":"v","showlegend":true,"x":["2016-03","2016-08","2017-04","2018-01","2019-01"],"xaxis":"x","y":["Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition: Skeleton Based Action Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition: Skeleton Based Action Recognition","orientation":"v","showlegend":true,"x":["2019-09","2019-07","2019-06","2019-04","2019-01","2018-12","2019-11","2018-11","2018-05","2018-04","2017-03","2017-04","2018-02","2018-01","2018-06","2019-12","2016-11","2017-05","2016-09","2016-06","2016-04","2013-02","2017-08"],"xaxis":"x","y":["Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Emotion recognition: Emotion Recognition in Conversation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Emotion recognition: Emotion Recognition in Conversation","orientation":"v","showlegend":true,"x":["2019-04","2019-08","2019-09","2018-11","2018-06","2018-10"],"xaxis":"x","y":["Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Facial Landmark Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Facial Landmark Detection","orientation":"v","showlegend":true,"x":["2019-02","2018-03","2015-11"],"xaxis":"x","y":["Facial recognition and modelling: Facial Landmark Detection","Facial recognition and modelling: Facial Landmark Detection","Facial recognition and modelling: Facial Landmark Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Face Verification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Face Verification","orientation":"v","showlegend":true,"x":["2018-03","2017-03","2016-03","2015-11","2018-09","2018-12","2019-03","2019-04","2019-08","2019-10","2017-12","2014-06","2014-12","2015-02","2015-03","2018-01","2015-08","2017-10","2017-04"],"xaxis":"x","y":["Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Facial Expression Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Facial Expression Recognition","orientation":"v","showlegend":true,"x":["2019-11","2019-05","2019-02","2013-07","2017-08","2018-05"],"xaxis":"x","y":["Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Face Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Face Detection","orientation":"v","showlegend":true,"x":["2017-08","2018-10","2018-09","2017-09","2016-03"],"xaxis":"x","y":["Facial recognition and modelling: Face Detection","Facial recognition and modelling: Face Detection","Facial recognition and modelling: Face Detection","Facial recognition and modelling: Face Detection","Facial recognition and modelling: Face Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Face Alignment","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Face Alignment","orientation":"v","showlegend":true,"x":["2017-07","2019-08","2019-06","2019-04","2019-02","2018-05","2018-04","2018-03","2017-11"],"xaxis":"x","y":["Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Face Identification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Face Identification","orientation":"v","showlegend":true,"x":["2015-11","2017-04","2018-01","2018-03","2018-12"],"xaxis":"x","y":["Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Identification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling: Unsupervised Facial Landmark Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling: Unsupervised Facial Landmark Detection","orientation":"v","showlegend":true,"x":["2017-05","2017-06","2018-04","2018-06","2018-08","2019-08"],"xaxis":"x","y":["Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Gesture recognition: Hand Gesture Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Gesture recognition: Hand Gesture Recognition","orientation":"v","showlegend":true,"x":["2017-05","2018-04","2019-01","2018-12"],"xaxis":"x","y":["Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Image Classification","orientation":"v","showlegend":true,"x":["2019-12","2020-01","2013-12","2013-11","2013-02","2012-12","2014-09","2019-10","2019-08","2019-06","2019-05","2014-12","2015-02","2015-06","2015-11","2015-12","2016-02","2016-03","2016-05","2016-08","2016-10","2014-06","2016-11","2017-08","2017-09","2017-10","2017-12","2018-02","2018-05","2014-04","2018-11","2019-01","2019-04","2017-07","2018-07"],"xaxis":"x","y":["Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Unsupervised Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Unsupervised Image Classification","orientation":"v","showlegend":true,"x":["2018-02","2017-02"],"xaxis":"x","y":["Image classification: Unsupervised Image Classification","Image classification: Unsupervised Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Hyperspectral Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Hyperspectral Image Classification","orientation":"v","showlegend":true,"x":["2019-02","2018-07"],"xaxis":"x","y":["Image classification: Hyperspectral Image Classification","Image classification: Hyperspectral Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Retinal OCT Disease Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Retinal OCT Disease Classification","orientation":"v","showlegend":true,"x":["2015-12","2018-01"],"xaxis":"x","y":["Image classification: Retinal OCT Disease Classification","Image classification: Retinal OCT Disease Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Satellite Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Satellite Image Classification","orientation":"v","showlegend":true,"x":["2015-12","2019-11"],"xaxis":"x","y":["Image classification: Satellite Image Classification","Image classification: Satellite Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Sequential Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Sequential Image Classification","orientation":"v","showlegend":true,"x":["2018-10","2018-03","2017-10","2016-03","2015-11"],"xaxis":"x","y":["Image classification: Sequential Image Classification","Image classification: Sequential Image Classification","Image classification: Sequential Image Classification","Image classification: Sequential Image Classification","Image classification: Sequential Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification: Document Image Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification: Document Image Classification","orientation":"v","showlegend":true,"x":["2019-08","2018-01","2017-04"],"xaxis":"x","y":["Image classification: Document Image Classification","Image classification: Document Image Classification","Image classification: Document Image Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image generation: Image Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image generation: Image Generation","orientation":"v","showlegend":true,"x":["2016-06","2019-12","2019-11","2019-07","2019-04","2019-03","2018-12","2018-11","2018-09","2018-07","2018-03","2018-02","2017-10","2017-09","2017-06","2017-03","2016-01","2019-08","2017-02"],"xaxis":"x","y":["Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image generation: Conditional Image Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image generation: Conditional Image Generation","orientation":"v","showlegend":true,"x":["2018-09","2018-05","2018-02","2017-09","2017-03","2016-12","2016-10","2016-06"],"xaxis":"x","y":["Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image generation: Pose Transfer","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image generation: Pose Transfer","orientation":"v","showlegend":true,"x":["2017-12","2019-04"],"xaxis":"x","y":["Image generation: Pose Transfer","Image generation: Pose Transfer"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image-to-image translation: Fundus to Angiography Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image-to-image translation: Fundus to Angiography Generation","orientation":"v","showlegend":true,"x":["2019-07","2017-11"],"xaxis":"x","y":["Image-to-image translation: Fundus to Angiography Generation","Image-to-image translation: Fundus to Angiography Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: RGB Salient Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: RGB Salient Object Detection","orientation":"v","showlegend":true,"x":["2016-06","2019-06","2019-04","2018-06","2017-08","2017-07","2017-04","2016-11","2017-10"],"xaxis":"x","y":["Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Video Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Video Object Detection","orientation":"v","showlegend":true,"x":["2018-11","2019-07","2020-03"],"xaxis":"x","y":["Object detection: Video Object Detection","Object detection: Video Object Detection","Object detection: Video Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: 3D Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: 3D Object Detection","orientation":"v","showlegend":true,"x":["2019-03","2019-04","2019-07","2020-01","2018-02","2016-06","2017-11","2017-12","2018-12","2020-03"],"xaxis":"x","y":["Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Lane Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Lane Detection","orientation":"v","showlegend":true,"x":["2017-12","2018-06","2019-08","2017-10"],"xaxis":"x","y":["Object detection: Lane Detection","Object detection: Lane Detection","Object detection: Lane Detection","Object detection: Lane Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Birds Eye View Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Birds Eye View Object Detection","orientation":"v","showlegend":true,"x":["2018-12","2017-12","2017-11","2016-11","2016-08","2019-07","2019-10"],"xaxis":"x","y":["Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Weakly Supervised Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Weakly Supervised Object Detection","orientation":"v","showlegend":true,"x":["2019-11","2019-10","2019-04","2018-11","2018-07","2018-06","2018-04","2018-03","2018-02","2017-11","2017-08","2015-11","2016-03","2016-09","2016-11","2017-04","2017-06","2017-07"],"xaxis":"x","y":["Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Pedestrian Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Pedestrian Detection","orientation":"v","showlegend":true,"x":["2014-12","2017-02","2018-07"],"xaxis":"x","y":["Object detection: Pedestrian Detection","Object detection: Pedestrian Detection","Object detection: Pedestrian Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Object Detection","orientation":"v","showlegend":true,"x":["2019-11","2019-10","2019-09","2015-06","2015-12","2016-12","2017-03","2017-07","2017-08","2017-11","2018-03","2018-05","2018-11","2018-12","2019-01","2019-06","2017-12","2019-08"],"xaxis":"x","y":["Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection: Dense Object Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection: Dense Object Detection","orientation":"v","showlegend":true,"x":["2019-04","2017-08","2016-12"],"xaxis":"x","y":["Object detection: Dense Object Detection","Object detection: Dense Object Detection","Object detection: Dense Object Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object recognition: Traffic Sign Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object recognition: Traffic Sign Recognition","orientation":"v","showlegend":true,"x":["2019-04","2018-06"],"xaxis":"x","y":["Object recognition: Traffic Sign Recognition","Object recognition: Traffic Sign Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object recognition: Pedestrian Attribute Recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object recognition: Pedestrian Attribute Recognition","orientation":"v","showlegend":true,"x":["2016-08","2019-10"],"xaxis":"x","y":["Object recognition: Pedestrian Attribute Recognition","Object recognition: Pedestrian Attribute Recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object tracking: Visual Object Tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object tracking: Visual Object Tracking","orientation":"v","showlegend":true,"x":["2019-07","2018-12","2019-06","2017-06","2017-04","2016-11","2018-03","2017-10","2018-02","2018-11","2018-06"],"xaxis":"x","y":["Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object tracking: Multiple Object Tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object tracking: Multiple Object Tracking","orientation":"v","showlegend":true,"x":["2018-11","2019-09","2018-02"],"xaxis":"x","y":["Object tracking: Multiple Object Tracking","Object tracking: Multiple Object Tracking","Object tracking: Multiple Object Tracking"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task: 3D Point Cloud Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task: 3D Point Cloud Classification","orientation":"v","showlegend":true,"x":["2018-12","2018-03","2018-01","2017-06","2017-04","2019-04"],"xaxis":"x","y":["Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task: 3D Object Reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task: 3D Object Reconstruction","orientation":"v","showlegend":true,"x":["2019-01","2018-02","2016-12"],"xaxis":"x","y":["Other 3D task: 3D Object Reconstruction","Other 3D task: 3D Object Reconstruction","Other 3D task: 3D Object Reconstruction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task: 3D Reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task: 3D Reconstruction","orientation":"v","showlegend":true,"x":["2016-12","2018-11","2018-08"],"xaxis":"x","y":["Other 3D task: 3D Reconstruction","Other 3D task: 3D Reconstruction","Other 3D task: 3D Reconstruction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task: 3D Room Layouts From A Single RGB Panorama","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task: 3D Room Layouts From A Single RGB Panorama","orientation":"v","showlegend":true,"x":["2018-11","2019-01"],"xaxis":"x","y":["Other 3D task: 3D Room Layouts From A Single RGB Panorama","Other 3D task: 3D Room Layouts From A Single RGB Panorama"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task: 3D Shape Classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task: 3D Shape Classification","orientation":"v","showlegend":true,"x":["2018-04","2017-11"],"xaxis":"x","y":["Other 3D task: 3D Shape Classification","Other 3D task: 3D Shape Classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Color Image Denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Color Image Denoising","orientation":"v","showlegend":true,"x":["2018-07","2019-04","2018-02","2017-10","2017-04"],"xaxis":"x","y":["Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Image Clustering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Image Clustering","orientation":"v","showlegend":true,"x":["2019-04","2018-10","2012-08","2015-11","2016-04","2018-04","2017-03","2017-04","2018-07","2017-09","2017-10","2018-11","2018-12","2019-01"],"xaxis":"x","y":["Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Aesthetics Quality Assessment","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Aesthetics Quality Assessment","orientation":"v","showlegend":true,"x":["2018-10","2017-04","2016-04"],"xaxis":"x","y":["Other image process: Aesthetics Quality Assessment","Other image process: Aesthetics Quality Assessment","Other image process: Aesthetics Quality Assessment"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Grayscale Image Denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Grayscale Image Denoising","orientation":"v","showlegend":true,"x":["2019-10","2018-06","2018-05","2017-10","2017-04","2016-08"],"xaxis":"x","y":["Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Image Retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Image Retrieval","orientation":"v","showlegend":true,"x":["2018-03","2017-12","2016-12","2016-11","2016-08","2016-04","2015-11","2015-04","2018-01","2018-11","2019-02","2019-03","2019-09","2018-04"],"xaxis":"x","y":["Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process: Image Reconstruction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process: Image Reconstruction","orientation":"v","showlegend":true,"x":["2019-03","2017-06"],"xaxis":"x","y":["Other image process: Image Reconstruction","Other image process: Image Reconstruction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other video process: Video Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other video process: Video Generation","orientation":"v","showlegend":true,"x":["2017-07","2019-12","2016-11"],"xaxis":"x","y":["Other video process: Video Generation","Other video process: Video Generation","Other video process: Video Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other video process: Video Frame Interpolation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other video process: Video Frame Interpolation","orientation":"v","showlegend":true,"x":["2020-03","2019-04","2018-10"],"xaxis":"x","y":["Other video process: Video Frame Interpolation","Other video process: Video Frame Interpolation","Other video process: Video Frame Interpolation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other video process: Video Retrieval","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other video process: Video Retrieval","orientation":"v","showlegend":true,"x":["2018-04","2016-12","2017-07","2018-06","2018-08","2019-06","2019-07"],"xaxis":"x","y":["Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Formation Energy","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Formation Energy","orientation":"v","showlegend":true,"x":["2019-05","2017-09","2018-06","2017-06","2017-04"],"xaxis":"x","y":["Other vision process: Formation Energy","Other vision process: Formation Energy","Other vision process: Formation Energy","Other vision process: Formation Energy","Other vision process: Formation Energy"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Domain Adaptation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Domain Adaptation","orientation":"v","showlegend":true,"x":["2016-08","2016-05","2015-02","2014-09","2015-05","2019-11","2017-04","2017-05","2017-11","2017-12","2018-07","2018-11","2019-01","2019-03","2019-05","2019-06","2019-08","2019-09"],"xaxis":"x","y":["Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Visual Question Answering","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Visual Question Answering","orientation":"v","showlegend":true,"x":["2016-11","2017-05","2016-06","2016-03","2018-05","2017-08","2018-03","2016-05","2019-02","2019-05","2020-02","2019-09","2019-08","2019-07","2019-04","2017-07","2019-06","2017-04"],"xaxis":"x","y":["Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Scene Text Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Scene Text Detection","orientation":"v","showlegend":true,"x":["2016-04","2017-07","2017-08","2017-09","2018-01","2018-02","2018-04","2018-06","2018-07","2018-11","2019-03","2019-04","2019-10","2019-11","2017-04","2017-03","2015-04"],"xaxis":"x","y":["Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Domain Generalization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Domain Generalization","orientation":"v","showlegend":true,"x":["2017-08","2017-10","2019-03","2019-05"],"xaxis":"x","y":["Other vision process: Domain Generalization","Other vision process: Domain Generalization","Other vision process: Domain Generalization","Other vision process: Domain Generalization"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Scene Graph Generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Scene Graph Generation","orientation":"v","showlegend":true,"x":["2018-06","2018-08","2018-12","2020-02"],"xaxis":"x","y":["Other vision process: Scene Graph Generation","Other vision process: Scene Graph Generation","Other vision process: Scene Graph Generation","Other vision process: Scene Graph Generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Depth Completion","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Depth Completion","orientation":"v","showlegend":true,"x":["2019-02","2018-08","2018-11","2019-05"],"xaxis":"x","y":["Other vision process: Depth Completion","Other vision process: Depth Completion","Other vision process: Depth Completion","Other vision process: Depth Completion"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Multivariate Time Series Imputation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Multivariate Time Series Imputation","orientation":"v","showlegend":true,"x":["2018-06","2018-12"],"xaxis":"x","y":["Other vision process: Multivariate Time Series Imputation","Other vision process: Multivariate Time Series Imputation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Curved Text Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Curved Text Detection","orientation":"v","showlegend":true,"x":["2015-05","2019-04"],"xaxis":"x","y":["Other vision process: Curved Text Detection","Other vision process: Curved Text Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Crowd Counting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Crowd Counting","orientation":"v","showlegend":true,"x":["2019-08","2019-06","2016-08","2015-12"],"xaxis":"x","y":["Other vision process: Crowd Counting","Other vision process: Crowd Counting","Other vision process: Crowd Counting","Other vision process: Crowd Counting"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Monocular Depth Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Monocular Depth Estimation","orientation":"v","showlegend":true,"x":["2019-07","2019-03","2018-12","2018-06","2018-05","2018-03","2017-04","2016-07"],"xaxis":"x","y":["Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Metric Learning","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Metric Learning","orientation":"v","showlegend":true,"x":["2019-08","2018-04","2017-06","2016-11"],"xaxis":"x","y":["Other vision process: Metric Learning","Other vision process: Metric Learning","Other vision process: Metric Learning","Other vision process: Metric Learning"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Video Prediction","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Video Prediction","orientation":"v","showlegend":true,"x":["2018-11","2019-05"],"xaxis":"x","y":["Other vision process: Video Prediction","Other vision process: Video Prediction"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Visual Dialog","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Visual Dialog","orientation":"v","showlegend":true,"x":["2017-09","2017-11","2018-09","2019-02","2019-04"],"xaxis":"x","y":["Other vision process: Visual Dialog","Other vision process: Visual Dialog","Other vision process: Visual Dialog","Other vision process: Visual Dialog","Other vision process: Visual Dialog"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Unsupervised Domain Adaptation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Unsupervised Domain Adaptation","orientation":"v","showlegend":true,"x":["2015-02","2015-05","2018-11","2018-12","2019-11","2020-01"],"xaxis":"x","y":["Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Horizon Line Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Horizon Line Estimation","orientation":"v","showlegend":true,"x":["2016-08","2019-05"],"xaxis":"x","y":["Other vision process: Horizon Line Estimation","Other vision process: Horizon Line Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Object Counting","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Object Counting","orientation":"v","showlegend":true,"x":["2016-04","2016-09","2017-07","2019-04"],"xaxis":"x","y":["Other vision process: Object Counting","Other vision process: Object Counting","Other vision process: Object Counting","Other vision process: Object Counting"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process: Denoising","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process: Denoising","orientation":"v","showlegend":true,"x":["2017-05","2019-04","2018-07"],"xaxis":"x","y":["Other vision process: Denoising","Other vision process: Denoising","Other vision process: Denoising"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: Weakly-supervised 3D Human Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: Weakly-supervised 3D Human Pose Estimation","orientation":"v","showlegend":true,"x":["2017-04","2019-03","2018-11"],"xaxis":"x","y":["Pose estimation: Weakly-supervised 3D Human Pose Estimation","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Pose estimation: Weakly-supervised 3D Human Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: Pose Estimation","orientation":"v","showlegend":true,"x":["2017-11","2017-12","2018-03","2018-04","2018-05","2018-11","2018-12","2019-01","2019-02","2020-01","2020-02","2017-08","2017-07","2017-04","2015-11","2016-01","2016-03","2016-09","2016-12","2017-01","2017-02","2017-05","2019-10"],"xaxis":"x","y":["Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: Head Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: Head Pose Estimation","orientation":"v","showlegend":true,"x":["2019-01","2018-12","2017-10","2019-06"],"xaxis":"x","y":["Pose estimation: Head Pose Estimation","Pose estimation: Head Pose Estimation","Pose estimation: Head Pose Estimation","Pose estimation: Head Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: Keypoint Detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: Keypoint Detection","orientation":"v","showlegend":true,"x":["2017-03","2017-11","2018-04","2018-12","2019-01","2019-02","2016-12","2016-11","2016-08","2017-01"],"xaxis":"x","y":["Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: Hand Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: Hand Pose Estimation","orientation":"v","showlegend":true,"x":["2017-12","2017-08","2017-07"],"xaxis":"x","y":["Pose estimation: Hand Pose Estimation","Pose estimation: Hand Pose Estimation","Pose estimation: Hand Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: 3D Human Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: 3D Human Pose Estimation","orientation":"v","showlegend":true,"x":["2020-04","2020-02","2019-12","2019-09","2018-04","2018-06","2017-09","2017-05"],"xaxis":"x","y":["Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: 6D Pose Estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: 6D Pose Estimation","orientation":"v","showlegend":true,"x":["2019-01","2019-11"],"xaxis":"x","y":["Pose estimation: 6D Pose Estimation","Pose estimation: 6D Pose Estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: 6D Pose Estimation using RGB","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: 6D Pose Estimation using RGB","orientation":"v","showlegend":true,"x":["2017-11","2018-03","2018-12","2019-02","2019-09"],"xaxis":"x","y":["Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation: 6D Pose Estimation using RGBD","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation: 6D Pose Estimation using RGBD","orientation":"v","showlegend":true,"x":["2018-03","2019-01","2019-11"],"xaxis":"x","y":["Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGBD"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose tracking: Pose Tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose tracking: Pose Tracking","orientation":"v","showlegend":true,"x":["2017-12","2018-02","2018-04","2019-02","2019-05"],"xaxis":"x","y":["Pose tracking: Pose Tracking","Pose tracking: Pose Tracking","Pose tracking: Pose Tracking","Pose tracking: Pose Tracking","Pose tracking: Pose Tracking"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmenation: 3D Instance Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmenation: 3D Instance Segmentation","orientation":"v","showlegend":true,"x":["2019-06","2019-12","2020-03","2019-03"],"xaxis":"x","y":["Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Video Semantic Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Video Semantic Segmentation","orientation":"v","showlegend":true,"x":["2016-12","2020-04"],"xaxis":"x","y":["Semantic segmentation: Video Semantic Segmentation","Semantic segmentation: Video Semantic Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Instance Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Instance Segmentation","orientation":"v","showlegend":true,"x":["2019-06","2019-09","2020-01","2017-04","2019-02","2018-03","2017-12","2017-11","2019-08","2019-11","2016-11","2017-03"],"xaxis":"x","y":["Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Electron Microscopy Image Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Electron Microscopy Image Segmentation","orientation":"v","showlegend":true,"x":["2019-06","2019-08"],"xaxis":"x","y":["Semantic segmentation: Electron Microscopy Image Segmentation","Semantic segmentation: Electron Microscopy Image Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Human Part Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Human Part Segmentation","orientation":"v","showlegend":true,"x":["2019-10","2017-08","2018-05","2018-09","2018-11"],"xaxis":"x","y":["Semantic segmentation: Human Part Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Human Part Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Brain Tumor Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Brain Tumor Segmentation","orientation":"v","showlegend":true,"x":["2019-08","2019-06"],"xaxis":"x","y":["Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Brain Tumor Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: 3D Semantic Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: 3D Semantic Segmentation","orientation":"v","showlegend":true,"x":["2019-04","2018-09","2018-07","2017-10","2017-06"],"xaxis":"x","y":["Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: 3D Semantic Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: 3D Semantic Instance Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: 3D Semantic Instance Segmentation","orientation":"v","showlegend":true,"x":["2020-03","2018-12"],"xaxis":"x","y":["Semantic segmentation: 3D Semantic Instance Segmentation","Semantic segmentation: 3D Semantic Instance Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: 3D Part Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: 3D Part Segmentation","orientation":"v","showlegend":true,"x":["2019-09","2019-04","2018-01","2017-11","2017-06","2016-12"],"xaxis":"x","y":["Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Semantic Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Semantic Segmentation","orientation":"v","showlegend":true,"x":["2016-11","2019-08","2019-09","2019-10","2019-11","2020-04","2016-05","2019-06","2016-03","2015-09","2015-04","2015-03","2015-02","2014-12","2014-11","2015-11","2019-04","2019-01","2016-12","2017-03","2017-04","2017-06","2017-10","2017-11","2019-03","2017-12","2018-03","2018-04","2018-06","2018-08","2018-09","2018-12","2018-02","2016-06"],"xaxis":"x","y":["Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Lesion Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Lesion Segmentation","orientation":"v","showlegend":true,"x":["2018-10","2020-03"],"xaxis":"x","y":["Semantic segmentation: Lesion Segmentation","Semantic segmentation: Lesion Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Scene Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Scene Segmentation","orientation":"v","showlegend":true,"x":["2019-08","2018-03"],"xaxis":"x","y":["Semantic segmentation: Scene Segmentation","Semantic segmentation: Scene Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Retinal Vessel Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Retinal Vessel Segmentation","orientation":"v","showlegend":true,"x":["2019-07","2019-12","2018-10","2018-06","2018-02","2017-11"],"xaxis":"x","y":["Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Lung Nodule Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Lung Nodule Segmentation","orientation":"v","showlegend":true,"x":["2017-11","2019-08"],"xaxis":"x","y":["Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Lung Nodule Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Medical Image Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Medical Image Segmentation","orientation":"v","showlegend":true,"x":["2015-11","2018-07"],"xaxis":"x","y":["Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Medical Image Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Multi-tissue Nucleus Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Multi-tissue Nucleus Segmentation","orientation":"v","showlegend":true,"x":["2015-05","2017-03"],"xaxis":"x","y":["Semantic segmentation: Multi-tissue Nucleus Segmentation","Semantic segmentation: Multi-tissue Nucleus Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Nuclear Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Nuclear Segmentation","orientation":"v","showlegend":true,"x":["2016-11","2017-03","2018-09"],"xaxis":"x","y":["Semantic segmentation: Nuclear Segmentation","Semantic segmentation: Nuclear Segmentation","Semantic segmentation: Nuclear Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Pancreas Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Pancreas Segmentation","orientation":"v","showlegend":true,"x":["2018-04","2017-09"],"xaxis":"x","y":["Semantic segmentation: Pancreas Segmentation","Semantic segmentation: Pancreas Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Panoptic Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Panoptic Segmentation","orientation":"v","showlegend":true,"x":["2019-01","2018-12","2019-11","2019-09","2019-05"],"xaxis":"x","y":["Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Real-time Instance Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Real-time Instance Segmentation","orientation":"v","showlegend":true,"x":["2020-01","2019-11","2019-04"],"xaxis":"x","y":["Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Real-time Instance Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation: Skin Cancer Segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation: Skin Cancer Segmentation","orientation":"v","showlegend":true,"x":["2017-11","2018-02"],"xaxis":"x","y":["Semantic segmentation: Skin Cancer Segmentation","Semantic segmentation: Skin Cancer Segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":["<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.2 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>  UCF101-24 - Temporal Action Localization benchmarking: Video-mAP 0.5<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  MEXaction2 - Temporal Action Localization benchmarking: mAP<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: mAP<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>","<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@1000<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@200<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@500<BR>  THUMOS' 14 - Temporal Action Proposal Generation benchmarking: AR@50<BR>","<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.5<BR>  THUMOS\u201914 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: Mean mAP<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  ActionNet-VE - Action Recognition benchmarking: F-measure (%)<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  UTD-MHAD - Multimodal Activity Recognition benchmarking: Accuracy (CS)<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top-1 Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS\u201914 - Action Classification benchmarking: mAP<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Action Classification benchmarking: Object Top 5 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Object Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-1 Accuracy<BR>  YouCook2 - Action Classification benchmarking: Verb Top-5 Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MiniKinetics - Action Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  THUMOS'14 - Action Classification benchmarking: mAP<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  VIRAT Ground 2.0 - Action Recognition benchmarking: Average Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV1<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CV2<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Clip Hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-1<BR>  miniSports - Action Recognition benchmarking: Video hit-at-5<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  miniSports - Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: Speed  (FPS)<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  MSR Action3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UPenn Action - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: No. parameters<BR>","<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Volleyball - Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Accuracy<BR>  Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking: Train F-measure<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-1 Accuracy<BR>  EgoGesture - Action Recognition benchmarking: Top-5 Accuracy<BR>","<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: GFlops<BR>  AVA v2.1 - Action Recognition benchmarking: Params (M)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>  Diving-48 - Action Recognition benchmarking: Accuracy<BR>  UTD-MHAD - Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  DailyDialog - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>  EmoryNLP - Emotion Recognition in Conversation benchmarking: Weighted Macro-F1<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Arousal)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Cohn-Kanade - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000-3D - Face Alignment benchmarking: Mean NME<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: FR-at-0.1(%, all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Face Alignment benchmarking: Error rate<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Real-World Affective Faces - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  3DFAW - Face Alignment benchmarking: CVGTCE<BR>  3DFAW - Face Alignment benchmarking: GTE<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  FERG - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  RAF-DB - Facial Expression Recognition benchmarking: Overall Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: Mean Error Rate<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Front - Facial Landmark Detection benchmarking: Mean NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  AFLW-Full - Face Alignment benchmarking: Mean NME<BR>  LS3D-W Balanced - Face Alignment benchmarking: AUC0.07<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>  IJB-B - Face Identification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Oulu-CASIA - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.01<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.1<BR>  IJB-B - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  IIIT-D Viewed Sketch - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  CelebA Aligned - Face Alignment benchmarking: MOS<BR>  CelebA Aligned - Face Alignment benchmarking: MS-SSIM<BR>  CelebA Aligned - Face Alignment benchmarking: PSNR<BR>  CelebA Aligned - Face Alignment benchmarking: SSIM<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  IBUG - Face Alignment benchmarking: Mean Error Rate<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Face Alignment benchmarking: Mean NME<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  COFW - Face Alignment benchmarking: Mean Error Rate<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  AFLW-LFPA - Face Alignment benchmarking: Mean NME<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Failure private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>  300W - Face Alignment benchmarking: Mean Error Rate private<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  MMI - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ChaLean test - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>  Jester val - Hand Gesture Recognition benchmarking: Top 5 Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Northwestern University - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DHG-14 - Hand Gesture Recognition benchmarking: Accuracy<BR>  DHG-28 - Hand Gesture Recognition benchmarking: Accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 - Hand Gesture Recognition benchmarking: 28 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking: 14 gestures accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  BUAA - Hand Gesture Recognition benchmarking: Accuracy<BR>  MGB - Hand Gesture Recognition benchmarking: Accuracy<BR>  SmartWatch - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Image classification: Document Image Classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Document Image Classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>","<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Salinas Scene - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Letters - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  CINIC-10 - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Clothing1M - Image Classification benchmarking: Accuracy<BR>  Food-101N - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  EMNIST-Balanced - Image Classification benchmarking: Accuracy<BR>  MultiMNIST - Image Classification benchmarking: Percentage error<BR>  smallNORB - Image Classification benchmarking: Classification Error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Flowers-102 - Image Classification benchmarking: Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ObjectNet - Image Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>  iNaturalist - Image Classification benchmarking: Top 5 Accuracy<BR>  iNaturalist 2018 - Image Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>  CIFAR-20 - Unsupervised Image Classification benchmarking: Accuracy<BR>  STL-10 - Unsupervised Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST - Unsupervised Image Classification benchmarking: Accuracy<BR>","<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>","<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: # of clusters (k)<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA 128 x 128 - Image Generation benchmarking: FID<BR>  Stacked MNIST - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: FID<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: DS<BR>  Deep-Fashion - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: DS<BR>  Market-1501 - Pose Transfer benchmarking: IS<BR>  Market-1501 - Pose Transfer benchmarking: PCKh<BR>  Market-1501 - Pose Transfer benchmarking: SSIM<BR>  Market-1501 - Pose Transfer benchmarking: mask-IS<BR>  Market-1501 - Pose Transfer benchmarking: mask-SSIM<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: LPIPS<BR>  Deep-Fashion - Pose Transfer benchmarking: Retrieval Top10 Recall<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Car 512 x 384 - Image Generation benchmarking: FID<BR>  LSUN Horse 256 x 256 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  ADE-Indoor - Image Generation benchmarking: FID<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  Cityscapes-25K 256x512 - Image Generation benchmarking: FID<BR>  Cityscapes-5K 256x512 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: FID<BR>  Fashion-MNIST - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 64x64 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: bits/dimension<BR>","<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>","<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom - Image Generation benchmarking: FID-50k<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>  MNIST - Image Generation benchmarking: bits/dimension<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  LSUN Bedroom 64 x 64 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  ImageNet 128x128 - Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Image Generation benchmarking: IS<BR>  ImageNet 256x256 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  CelebA-HQ 256x256 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  STL-10 - Image Generation benchmarking: FID<BR>","<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: Kernel Inception Distance<BR>","<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ISTD - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Dense Object Detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Clipart1k - Weakly Supervised Object Detection benchmarking: MAP<BR>  Comic2k - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-test - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-test - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  ECSSD - RGB Salient Object Detection benchmarking: MAE<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  SOD - RGB Salient Object Detection benchmarking: F-measure<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  IconArt - Weakly Supervised Object Detection benchmarking: MAP<BR>  PeopleArt - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: mAP<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  nuScenes-F - 3D Object Detection benchmarking: AP50<BR>  nuScenes-F - 3D Object Detection benchmarking: AP75<BR>  nuScenes-F - 3D Object Detection benchmarking: AP<BR>  nuScenes-F - 3D Object Detection benchmarking: AR<BR>  nuScenes-F - 3D Object Detection benchmarking: ARI<BR>  nuScenes-F - 3D Object Detection benchmarking: ARm<BR>  nuScenes-F - 3D Object Detection benchmarking: ARs<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP50<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP75<BR>  nuScenes-FB - 3D Object Detection benchmarking: AP<BR>  nuScenes-FB - 3D Object Detection benchmarking: AR<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARI<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARm<BR>  nuScenes-FB - 3D Object Detection benchmarking: ARs<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  NYU Depth v2 - 3D Object Detection benchmarking: MAP<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Heavy MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Large MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  CULane - Lane Detection benchmarking: F1 score<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Lane Detection benchmarking: Accuracy<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  COCO 2017 - Object Detection benchmarking: Mean mAP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Video Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>  ImageNet VID - Video Object Detection benchmarking: runtime (ms)<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  BDD100K - Object Detection benchmarking: mAP-at-0.5<BR>  India Driving Dataset - Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>","<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO minival - Object Detection benchmarking: box AP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  PeopleArt - Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>","<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  GTSRB - Traffic Sign Recognition benchmarking: Accuracy<BR>","<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>","<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>","<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: Precision<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  GOT-10k - Visual Object Tracking benchmarking: Average Overlap<BR>  GOT-10k - Visual Object Tracking benchmarking: Success Rate 0.5<BR>  LaSOT - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Unseen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: O (Average of Measures)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  VOT2019 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>","<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>","<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>","<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>","<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Mean Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>  NUS-WIDE - Image Retrieval benchmarking: MAP<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DeepFashion - Image Retrieval benchmarking: Recall-at-20<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  street2shop - topwear - Image Retrieval benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>","<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: HP<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: MMD<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: HP<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: MMD<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  FRGC - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  UMist - Image Clustering benchmarking: NMI<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Coil-20 - Image Clustering benchmarking: Accuracy<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  Fashion-MNIST - Image Clustering benchmarking: Accuracy<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: SSIM<BR>  Urban100 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Set12 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma75 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma35 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Clip300 sigma60 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: SSIM<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: SSIM<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma5 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma25 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma35 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma50 - Color Image Denoising benchmarking: PSNR<BR>  Kodak25 sigma75 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma15 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma25 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma35 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma50 - Color Image Denoising benchmarking: PSNR<BR>  McMaster sigma75 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Image Reconstruction<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>","<BR>task: Other image process: Image Reconstruction<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Clothes - Image Reconstruction benchmarking: LPIPS<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  LetterA-J - Image Clustering benchmarking: Accuracy<BR>  LetterA-J - Image Clustering benchmarking: NMI<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking: Inception Score<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video Median Rank<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video Median Rank<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-10<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-1<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-50<BR>  ActivityNet - Video Retrieval benchmarking: text-to-video R-at-5<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video Median Rank<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-10<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-1<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-50<BR>  DiDeMo - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video Median Rank<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-50<BR>  MSVD - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other video process: Video Generation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  BAIR Robot Pushing - Video Generation benchmarking: FVD score<BR>  Kinetics-600 12 frames, 128x128 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 12 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: FID<BR>  Kinetics-600 48 frames, 64x64 - Video Generation benchmarking: Inception Score<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: Interpolation Error<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  UCF101 - Video Frame Interpolation benchmarking: SSIM<BR>  Vimeo90k - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: PSNR<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: SSIM<BR>  X4K1000FPS - Video Frame Interpolation benchmarking: tOF<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Middlebury - Video Frame Interpolation benchmarking: PSNR<BR>  Middlebury - Video Frame Interpolation benchmarking: SSIM<BR>","<BR>task: Other video process: Video Generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  TrailerFaces - Video Generation benchmarking: FID<BR>","<BR>task: Other video process: Video Generation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Mean Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text Median Rank<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Classification Accuracy<BR>  Synth Objects-to-LINEMOD - Domain Adaptation benchmarking: Mean Angle Error<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  TDIUC - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech-10 - Domain Adaptation benchmarking: Accuracy (%)<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>  ImageNet-R - Domain Generalization benchmarking: Top-1 Error Rate<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech A - Crowd Counting benchmarking: MSE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  VOID - Depth Completion benchmarking: MAE<BR>  VOID - Depth Completion benchmarking: RMSE<BR>  VOID - Depth Completion benchmarking: iMAE<BR>  VOID - Depth Completion benchmarking: iRMSE<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>","<BR>task: Other vision process: Denoising<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  IC19-ReCTs - Scene Text Detection benchmarking: F-Measure<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2015 - Scene Text Detection benchmarking: H-Mean<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: H-Mean<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: TIoU<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: mean Recall @20<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^\u22123)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Player Distance<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^\u22123)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Beijing Air Quality - Multivariate Time Series Imputation benchmarking: MAE (PM2.5)<BR>  KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking: MSE (10% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: MAE (10% of data as GT)<BR>  UCI localization data - Multivariate Time Series Imputation benchmarking: MAE (10% missing)<BR>","<BR>task: Other vision process: Video Prediction<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: number<BR>  VizWiz 2018 - Visual Question Answering benchmarking: other<BR>  VizWiz 2018 - Visual Question Answering benchmarking: unanswerable<BR>  VizWiz 2018 - Visual Question Answering benchmarking: yes/no<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  GQA test-dev - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>","<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  CMU Mocap-1 - Video Prediction benchmarking: Test Error<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Make3D - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Make3D - Monocular Depth Estimation benchmarking: RMSE<BR>  Make3D - Monocular Depth Estimation benchmarking: Sq Rel<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking: absolute relative error<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: Mean Rank<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: NDCG (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CLEVR - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  100 sleep nights of 8 caregivers - Visual Question Answering benchmarking: 14 gestures accuracy<BR>  HowmanyQA - Visual Question Answering benchmarking: Accuracy<BR>  TallyQA - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>","<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>","<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: ATV<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: AUC<BR>  KITTI Horizon - Horizon Line Estimation benchmarking: MSE<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: PCK3D<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJAE<BR>  3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  CHALL H80K - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: PA-MPJPE<BR>  3DPW - 3D Human Pose Estimation benchmarking: acceleration error<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  HumanEva-I - 3D Human Pose Estimation benchmarking: Mean Reconstruction Error (mm)<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Mean Recall<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGB benchmarking: Recall (VSD)<BR>","<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean IoU<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: IoU-3D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-2D<BR>  Tejani - 6D Pose Estimation using RGBD benchmarking: VSS-3D<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADI<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  CAMERA25 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 10cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 10, 5cm<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-25<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 3DIou-at-50<BR>  REAL275 - 6D Pose Estimation using RGBD benchmarking: mAP 5, 5cm<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  T-LESS - 6D Pose Estimation using RGBD benchmarking: Mean Recall<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: MPJPE (CS)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean AUC<BR>","<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean IoU<BR>  OCCLUSION - 6D Pose Estimation using RGB benchmarking: MAP<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: 5\u00b05 cm<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: IOU25<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Rerr<BR>  NOCS-REAL275 - 6D Pose Estimation benchmarking: Terr<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPVPE<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADI<BR>","<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  BJUT-3D - Head Pose Estimation benchmarking: MAE<BR>  Pointing'04 - Head Pose Estimation benchmarking: MAE<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  UAV-Human - Pose Estimation benchmarking: mAP<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with other data)<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Pose Estimation benchmarking: AP<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  BIWI - Head Pose Estimation benchmarking: MAE (trained with BIWI data)<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: FPS<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2019 - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: FPS<BR>  K2HPD - Hand Pose Estimation benchmarking: PDJ@5mm<BR>  NYU Hands - Hand Pose Estimation benchmarking: FPS<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  MSRA Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  PoseTrack2018 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2018 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTA<BR>  Multi-Person PoseTrack - Pose Tracking benchmarking: MOTP<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Instance Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>  ScanNet - 3D Instance Segmentation benchmarking: mAP<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mCov<BR>  S3DIS - 3D Instance Segmentation benchmarking: mWCov<BR>","<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: Dice<BR>  2018 Data Science Bowl - Medical Image Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VInfo<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: VRand<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: Dice Score<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: MSD<BR>  CHAOS MRI Dataset - Medical Image Segmentation benchmarking: VS<BR>  HSVM - Medical Image Segmentation benchmarking: Dice Score<BR>  HSVM - Medical Image Segmentation benchmarking: MSD<BR>  HSVM - Medical Image Segmentation benchmarking: VS<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Cell - Medical Image Segmentation benchmarking: IoU<BR>  EM - Medical Image Segmentation benchmarking: IoU<BR>","<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Dice<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  MHP v2.0 - Human Part Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Medical Image Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  NYU Depth v2 - Instance Segmentation benchmarking: mAP-at-0.5<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  iSAID - Instance Segmentation benchmarking: Average Precision<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  LVIS v1.0 - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  ISLES-2015 - Lesion Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2017 - Lesion Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  iSEG 2017 Challenge - Medical Image Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BUS 2017 Dataset B - Lesion Segmentation benchmarking: Dice Score<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  NIH - Lung Nodule Segmentation benchmarking: AVD<BR>  NIH - Lung Nodule Segmentation benchmarking: Dice Score<BR>  NIH - Lung Nodule Segmentation benchmarking: Precision<BR>  NIH - Lung Nodule Segmentation benchmarking: Recall<BR>  NIH - Lung Nodule Segmentation benchmarking: VS<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>","<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  ScanNetV1 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: mIoU<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: Accuracy<BR>  Montgomery County - Lung Nodule Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CVC-ClinicDB - Medical Image Segmentation benchmarking: mean Dice<BR>  ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking: Warping Error<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: Average MAE<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>  RITE - Medical Image Segmentation benchmarking: Dice<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: F1-Score<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: Dice<BR>  LIDC-IDRI - Lung Nodule Segmentation benchmarking: IoU<BR>  Lung Nodule  - Lung Nodule Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: 3DIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: DSC<BR>  Kvasir-Instrument - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  HRF - Retinal Vessel Segmentation benchmarking: AUC<BR>  HRF - Retinal Vessel Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2011 - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  BDD - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Category mIoU<BR>  GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  ParisLille3D - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: Pixel Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  KITTI Semantic Segmentation - Semantic Segmentation benchmarking: Mean IoU (class)<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  SUN-RGBD - Semantic Segmentation benchmarking: Mean IoU<BR>  SYNTHIA-CVPR\u201916 - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: oAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: Frame (fps)<BR>","<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQst<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  COCO panoptic - Panoptic Segmentation benchmarking: PQ<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Mapillary val - Panoptic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>","<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2014 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: MSD<BR>  BRATS 2018 - Brain Tumor Segmentation benchmarking: VS<BR>  BRATS 2018 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  PH2 - Skin Cancer Segmentation benchmarking: IoU<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>"],"marker":{"line":{"width":1,"color":"black"},"size":20,"symbol":42},"mode":"markers","x":["2016-09","2020-01","2016-02","2018-06","2019-03","2018-04","2017-05","2017-03","2016-04","2016-01","2015-11","2015-06","2019-04","2017-03","2016-12","2015-07","2018-06","2017-07","2019-11","2019-08","2018-07","2017-12","2017-03","2012-07","2017-11","2017-08","2017-06","2018-06","2014-06","2016-08","2017-12","2017-03","2017-12","2018-12","2019-04","2019-06","2019-08","2018-07","2018-01","2015-11","2014-03","2012-12","2014-06","2015-11","2015-12","2017-05","2018-01","2019-05","2019-04","2017-04","2015-11","2012-10","2014-06","2014-11","2016-04","2016-06","2016-12","2017-04","2017-10","2018-01","2018-02","2019-01","2019-06","2019-07","2014-11","2018-12","2016-11","2018-11","2019-09","2019-01","2020-04","2013-06","2018-12","2017-05","2019-09","2017-07","2019-03","2016-10","2017-01","2017-08","2015-11","2015-06","2014-06","2017-10","2018-08","2018-05","2016-09","2019-02","2019-05","2015-11","2017-08","2018-03","2018-04","2016-07","2016-04","2017-03","2017-08","2014-04","2014-12","2015-03","2015-07","2015-11","2016-04","2017-08","2017-10","2019-03","2019-07","2015-03","2019-08","2018-12","2018-04","2018-02","2017-09","2017-06","2013-07","2015-05","2014-08","2015-09","2017-05","2014-08","2014-06","2018-04","2019-01","2019-07","2017-05","2017-01","2017-07","2013-03","2017-11","2018-06","2015-02","2016-12","2019-02","2019-04","2019-01","2018-10","2018-05","2017-11","2017-10","2019-12","2017-08","2016-03","2013-12","2013-06","2013-01","2012-12","2012-02","2017-07","2019-06","2019-10","2018-07","2018-03","2015-09","2015-04","2018-02","2015-12","2015-11","2016-10","2015-11","2019-10","2019-10","2019-04","2017-12","2017-05","2019-12","2019-11","2018-02","2019-05","2019-03","2014-10","2016-01","2016-06","2018-12","2018-11","2017-03","2017-06","2018-09","2018-07","2017-09","2017-10","2018-02","2017-11","2016-11","2016-11","2014-07","2015-11","2016-09","2014-06","2016-11","2015-05","2015-06","2016-03","2018-03","2017-07","2017-08","2018-06","2019-04","2018-10","2014-03","2015-11","2016-11","2016-08","2019-08","2019-05","2019-04","2019-03","2017-11","2016-11","2015-11","2017-11","2017-02","2014-11","2015-04","2017-08","2017-12","2019-08","2019-12","2015-12","2017-03","2017-11","2017-12","2019-07","2019-09","2017-03","2016-12","2016-10","2015-12","2018-12","2015-12","2012-02","2018-06","2019-04","2017-09","2019-09","2016-06","2017-06","2018-11","2015-04","2015-12","2016-11","2019-01","2019-07","2017-04","2016-10","2016-03","2016-04","2018-03","2016-04","2016-12","2016-04","2019-02","2019-08","2016-11","2019-01","2018-11","2018-04","2017-06","2016-04","2019-03","2015-04","2017-04","2017-03","2016-04","2015-11","2013-12","2012-08","2012-03","2019-08","2018-10","2018-05","2018-04","2017-10","2016-06","2015-08","2015-12","2015-08","2016-08","2016-11","2017-04","2017-10","2015-11","2017-04","2018-10","2016-08","2016-11","2019-10","2018-12","2014-12","2019-12","2015-06","2016-09","2016-10","2018-08","2019-07","2019-07","2017-08","2019-04","2020-03","2019-04","2016-09","2018-06","2016-05","2016-08","2019-02","2017-05","2018-07","2018-11","2019-04","2015-12","2018-11","2012-12","2017-11","2017-02","2015-02","2017-07","2019-05","2019-02","2017-08","2015-08","2014-09","2019-06","2019-04","2018-11","2018-06","2018-01","2017-09","2017-04","2017-03","2016-06","2016-04","2014-12","2015-05","2020-02","2017-07","2016-07","2014-12","2015-12","2015-11","2018-03","2018-01","2016-06","2010-11","2017-12","2019-08","2019-07","2019-04","2019-05","2018-06","2018-03","2016-09","2014-11","2017-06","2016-10","2016-05","2017-04","2015-11","2016-06","2016-12","2017-04","2017-07","2018-08","2018-10","2015-06","2016-04","2018-06","2016-04","2018-12","2017-02","2018-06","2019-05","2019-04","2019-05","2019-07","2014-06","2020-04","2019-07","2018-09","2017-12","2017-05","2010-03","2013-12","2019-02","2019-08","2017-11","2017-11","2018-03","2019-01","2019-02","2017-04","2016-01","2018-12","2019-01","2017-11","2017-03","2019-10","2019-09","2018-03","2017-01","2016-11","2018-02","2017-12","2016-12","2015-11","2016-11","2016-03","2016-01","2014-11","2014-07","2019-01","2017-10","2017-01","2016-12","2020-01","2019-08","2017-02","2016-05","2016-11","2019-01","2017-08","2017-10","2018-04","2016-11","2018-01","2019-04","2019-02","2019-12","2016-03","2018-08","2018-07","2019-03","2019-06","2019-12","2014-11","2018-11","2015-11","2019-08","2017-06","2016-05","2015-12","2016-04","2016-11","2017-03","2017-11","2018-03","2019-12","2016-03","2017-03","2018-04","2018-10","2015-05","2019-03","2019-04","2016-06","2016-12","2017-11","2016-12","2019-07","2019-07","2015-05","2019-08","2019-08","2015-05","2019-03","2019-07","2014-12","2019-04","2014-07","2014-11","2014-12","2015-05","2015-11","2015-12","2018-06","2016-03","2016-11","2016-12","2017-02","2019-12","2019-04","2019-01","2018-12","2018-08","2017-11","2017-07","2016-06","2015-05","2020-01","2015-05","2017-03","2017-03","2017-04","2018-01","2018-08","2018-09","2019-01","2019-09","2019-06","2015-05","2016-03","2017-09","2018-10","2019-06","2019-04","2019-11","2015-05","2016-12"],"y":["Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Action Segmentation","Action localization: Action Segmentation","Action localization: Action Segmentation","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Action localization: Temporal Action Localization","Activity detection: Action Detection","Activity detection: Action Detection","Activity detection: Action Detection","Activity detection: Action Detection","Activity localization: Temporal Action Proposal Generation","Activity localization: Temporal Action Proposal Generation","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity localization: Weakly Supervised Action Localization","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Recognition","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Action Classification","Activity recognition: Egocentric Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Classification","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Activity recognition: Group Activity Recognition","Activity recognition: Group Activity Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Human Interaction Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Action Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Action Recognition","Activity recognition: Human Interaction Recognition","Activity recognition: Action Recognition","Activity recognition: Action Recognition","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Emotion recognition: Emotion Recognition in Conversation","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Landmark Detection","Facial recognition and modelling: Facial Landmark Detection","Facial recognition and modelling: Facial Landmark Detection","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Detection","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Gesture recognition: Hand Gesture Recognition","Image classification: Document Image Classification","Image classification: Document Image Classification","Image classification: Hyperspectral Image Classification","Image classification: Hyperspectral Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Image classification: Unsupervised Image Classification","Image classification: Sequential Image Classification","Image classification: Satellite Image Classification","Image classification: Sequential Image Classification","Image classification: Unsupervised Image Classification","Image classification: Retinal OCT Disease Classification","Image classification: Unsupervised Image Classification","Image generation: Conditional Image Generation","Image generation: Conditional Image Generation","Image generation: Image Generation","Image generation: Pose Transfer","Image generation: Pose Transfer","Image generation: Pose Transfer","Image generation: Pose Transfer","Image generation: Image Generation","Image generation: Image Generation","Image generation: Conditional Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image generation: Image Generation","Image-to-image translation: Fundus to Angiography Generation","Image-to-image translation: Fundus to Angiography Generation","Object detection: Weakly Supervised Object Detection","Object detection: Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Object Detection","Object detection: RGB Salient Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Dense Object Detection","Object detection: RGB Salient Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: RGB Salient Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Weakly Supervised Object Detection","Object detection: Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: 3D Object Detection","Object detection: Pedestrian Detection","Object detection: Pedestrian Detection","Object detection: Pedestrian Detection","Object detection: Lane Detection","Object detection: Lane Detection","Object detection: Lane Detection","Object detection: Lane Detection","Object detection: Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Video Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: Object Detection","Object detection: 3D Object Detection","Object recognition: Pedestrian Attribute Recognition","Object recognition: Traffic Sign Recognition","Object recognition: Traffic Sign Recognition","Object recognition: Traffic Sign Recognition","Object recognition: Pedestrian Attribute Recognition","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Multiple Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Object tracking: Visual Object Tracking","Other 3D task: 3D Shape Classification","Other 3D task: 3D Reconstruction","Other 3D task: 3D Reconstruction","Other 3D task: 3D Room Layouts From A Single RGB Panorama","Other 3D task: 3D Object Reconstruction","Other 3D task: 3D Point Cloud Classification","Other 3D task: 3D Point Cloud Classification","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Retrieval","Other image process: Image Reconstruction","Other image process: Image Retrieval","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Image Clustering","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Image Clustering","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Grayscale Image Denoising","Other image process: Aesthetics Quality Assessment","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Color Image Denoising","Other image process: Image Retrieval","Other image process: Grayscale Image Denoising","Other image process: Image Clustering","Other image process: Grayscale Image Denoising","Other image process: Image Reconstruction","Other image process: Image Reconstruction","Other image process: Image Clustering","Other image process: Image Retrieval","Other video process: Video Generation","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Retrieval","Other video process: Video Generation","Other video process: Video Frame Interpolation","Other video process: Video Frame Interpolation","Other video process: Video Frame Interpolation","Other video process: Video Generation","Other video process: Video Generation","Other video process: Video Retrieval","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Visual Question Answering","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Generalization","Other vision process: Domain Generalization","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Crowd Counting","Other vision process: Depth Completion","Other vision process: Depth Completion","Other vision process: Depth Completion","Other vision process: Denoising","Other vision process: Domain Adaptation","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Scene Text Detection","Other vision process: Curved Text Detection","Other vision process: Scene Graph Generation","Other vision process: Scene Graph Generation","Other vision process: Scene Graph Generation","Other vision process: Domain Adaptation","Other vision process: Domain Adaptation","Other vision process: Crowd Counting","Other vision process: Unsupervised Domain Adaptation","Other vision process: Multivariate Time Series Imputation","Other vision process: Multivariate Time Series Imputation","Other vision process: Multivariate Time Series Imputation","Other vision process: Video Prediction","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Video Prediction","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Monocular Depth Estimation","Other vision process: Metric Learning","Other vision process: Metric Learning","Other vision process: Visual Dialog","Other vision process: Visual Dialog","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Visual Question Answering","Other vision process: Object Counting","Other vision process: Object Counting","Other vision process: Multivariate Time Series Imputation","Other vision process: Horizon Line Estimation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Formation Energy","Other vision process: Formation Energy","Other vision process: Formation Energy","Other vision process: Unsupervised Domain Adaptation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Horizon Line Estimation","Other vision process: Domain Adaptation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 3D Human Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Pose estimation: Head Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Head Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Head Pose Estimation","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Hand Pose Estimation","Pose estimation: Hand Pose Estimation","Pose estimation: Hand Pose Estimation","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Keypoint Detection","Pose estimation: Hand Pose Estimation","Pose tracking: Pose Tracking","Pose tracking: Pose Tracking","Pose tracking: Pose Tracking","Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation","Semantic segmenation: 3D Instance Segmentation","Semantic segmentation: Nuclear Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Multi-tissue Nucleus Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Video Semantic Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Semantic Instance Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Pancreas Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Scene Segmentation","Semantic segmentation: Scene Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Skin Cancer Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Electron Microscopy Image Segmentation","Semantic segmentation: Electron Microscopy Image Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Skin Cancer Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Scene Segmentation"],"type":"scatter","line":{"color":"black","width":0}},{"hovertemplate":["<BR>task: Other image process: Image Clustering<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>","<BR>task: Image classification: Image Classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CAD-120 - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  FER2013 - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-Olympic - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech - Pedestrian Detection benchmarking: Reasonable Miss Rate<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.8858<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-02<BR>ratio: 0.3958<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.3991<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Curved Text Detection<BR>date: 2015-05<BR>ratio: 0.2321<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2015-05<BR>ratio: 0.1142<BR>benchmarks:<BR>  Office-Home - Unsupervised Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Object detection: Object Detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2015-11<BR>ratio: 0.117<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2015-11<BR>ratio: 0.4701<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2015-11<BR>ratio: 0.0854<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2015-11<BR>ratio: 1.0<BR>benchmarks:<BR>  RITE - Medical Image Segmentation benchmarking: Jaccard Index<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2015-11<BR>ratio: 0.1888<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2015-11<BR>ratio: 0.3222<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2015-11<BR>ratio: 0.15<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2015-11<BR>ratio: 0.1785<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Image classification: Image Classification<BR>date: 2015-11<BR>ratio: 0.1637<BR>benchmarks:<BR>  SVHN - Image Classification benchmarking: Percentage error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2015-12<BR>ratio: 0.0779<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>","<BR>task: Image classification: Satellite Image Classification<BR>date: 2015-12<BR>ratio: 0.4051<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet - Action Recognition benchmarking: mAP<BR>","<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2015-12<BR>ratio: 0.9509<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>","<BR>task: Object detection: Object Detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2012 - Object Detection benchmarking: MAP<BR>","<BR>task: Image generation: Image Generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Binarized MNIST - Image Generation benchmarking: nats<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-03<BR>ratio: 0.0258<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2016-03<BR>ratio: 0.2119<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2016-03<BR>ratio: 0.4252<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2016-03<BR>ratio: 0.5954<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  FLIC Elbows - Pose Estimation benchmarking: PCK-at-0.2<BR>  FLIC Wrists - Pose Estimation benchmarking: PCK-at-0.2<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2016-04<BR>ratio: 0.4868<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-04<BR>ratio: 0.4832<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-04<BR>ratio: 0.4271<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2016-04<BR>ratio: 0.6898<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: NMI<BR>  Coil-20 - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: NMI<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2016-04<BR>ratio: 0.0092<BR>benchmarks:<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2016-04<BR>ratio: 0.8889<BR>benchmarks:<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: m-reIRMSE<BR>  COCO count-test - Object Counting benchmarking: mRMSE-nz<BR>  COCO count-test - Object Counting benchmarking: mRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-reIRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: m-relRMSE<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE-nz<BR>  Pascal VOC 2007 count-test - Object Counting benchmarking: mRMSE<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2016-04<BR>ratio: 0.3005<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-05<BR>ratio: 0.2762<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-05<BR>ratio: 0.072<BR>benchmarks:<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-06<BR>ratio: 0.572<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-06<BR>ratio: 0.3146<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2016-06<BR>ratio: 0.3235<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>","<BR>task: Image generation: Image Generation<BR>date: 2016-06<BR>ratio: 0.3625<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2016-08<BR>ratio: 0.8371<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Eurasian Cities Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>  York Urban Dataset - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2016-08<BR>ratio: 0.6547<BR>benchmarks:<BR>  MNIST-to-MNIST-M - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  Synth Digits-to-SVHN - Domain Adaptation benchmarking: Accuracy<BR>  Synth Signs-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Backpack<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Gender<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: Hat<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: LCS<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCC<BR>  UAV-Human - Pedestrian Attribute Recognition benchmarking: UCS<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2016-08<BR>ratio: 0.2523<BR>benchmarks:<BR>  UCF-QNRF - Crowd Counting benchmarking: MAE<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2016-08<BR>ratio: 0.5511<BR>benchmarks:<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-08<BR>ratio: 0.1736<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2016-08<BR>ratio: 0.0915<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Image classification: Image Classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2016-11<BR>ratio: 0.3326<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2016-11<BR>ratio: 0.0<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2016-11<BR>ratio: 0.2824<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Other video process: Video Generation<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2016-11<BR>ratio: 0.2329<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2016-11<BR>ratio: 0.2215<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2016-11<BR>ratio: 0.5084<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  Visual7W - Visual Question Answering benchmarking: Percentage correct<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2016-11<BR>ratio: 0.6006<BR>benchmarks:<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-11<BR>ratio: 0.2304<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Global Accuracy<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2016-12<BR>ratio: 0.6019<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  MPII Multi-Person - Keypoint Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.3137<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2016-12<BR>ratio: 0.5259<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>","<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2016-12<BR>ratio: 0.4898<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>","<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2016-12<BR>ratio: 0.5647<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>","<BR>task: Object detection: Dense Object Detection<BR>date: 2016-12<BR>ratio: 0.1125<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2016-12<BR>ratio: 0.1929<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Object detection: Object Detection<BR>date: 2016-12<BR>ratio: 0.0388<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>","<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2016-12<BR>ratio: 0.9756<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-01<BR>ratio: 0.1328<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-01<BR>ratio: 0.2025<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>","<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Medium MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Small MR^-2<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  coil-100 - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Multi-tissue Nucleus Segmentation<BR>date: 2017-03<BR>ratio: 0.1574<BR>benchmarks:<BR>  Kumar - Multi-tissue Nucleus Segmentation benchmarking: Hausdorff Distance (mm)<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-03<BR>ratio: 0.0303<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2017-03<BR>ratio: 0.6528<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-03<BR>ratio: 0.5668<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-03<BR>ratio: 0.4081<BR>benchmarks:<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-03<BR>ratio: 0.2484<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-03<BR>ratio: 0.21<BR>benchmarks:<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-03<BR>ratio: 0.5877<BR>benchmarks:<BR>  SBU - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-04<BR>ratio: 0.2106<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-04<BR>ratio: 0.0331<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-04<BR>ratio: 0.4378<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  FRGC - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>  YouTube Faces DB - Image Clustering benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-04<BR>ratio: 0.2112<BR>benchmarks:<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-04<BR>ratio: 0.2041<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-04<BR>ratio: 0.1026<BR>benchmarks:<BR>  VQA v1 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v1 test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2017-04<BR>ratio: 0.3846<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-04<BR>ratio: 0.7172<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2017-04<BR>ratio: 0.1968<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2017-04<BR>ratio: 0.1629<BR>benchmarks:<BR>  EV-Action - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-04<BR>ratio: 0.2194<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2017-04<BR>ratio: 0.1332<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  Sydney Urban Objects - 3D Point Cloud Classification benchmarking: F1<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-04<BR>ratio: 0.1767<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>","<BR>task: Image classification: Document Image Classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2017-04<BR>ratio: 1.0<BR>benchmarks:<BR>  BSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-04<BR>ratio: 0.4565<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-04<BR>ratio: 0.1679<BR>benchmarks:<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-04<BR>ratio: 0.3172<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.1<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.2<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-04<BR>ratio: 0.0758<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2017-04<BR>ratio: 0.4474<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-05<BR>ratio: 0.4983<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-05<BR>ratio: 0.528<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Other vision process: Denoising<BR>date: 2017-05<BR>ratio: 0.7853<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-05<BR>ratio: 0.4284<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CA)<BR>  Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking: PCK3D (CS)<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  J-HMDB-21 - Temporal Action Localization benchmarking: Frame-mAP<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>  UCF101-24 - Temporal Action Localization benchmarking: Frame-mAP<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-05<BR>ratio: 0.0559<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-05<BR>ratio: 0.0607<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-05<BR>ratio: 0.2867<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: F-Measure (Seen)<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2017-06<BR>ratio: 0.3412<BR>benchmarks:<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1244<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-06<BR>ratio: 0.5909<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2017-06<BR>ratio: 0.4142<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  COCO - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-06<BR>ratio: 0.1161<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>","<BR>task: Other image process: Image Reconstruction<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Visual Genome - Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-07<BR>ratio: 0.02<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-07<BR>ratio: 0.6614<BR>benchmarks:<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-07<BR>ratio: 0.0095<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>","<BR>task: Other video process: Video Generation<BR>date: 2017-07<BR>ratio: 0.0387<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2017-07<BR>ratio: 0.4581<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video Median Rank<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-07<BR>ratio: 0.2874<BR>benchmarks:<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2017-07<BR>ratio: 0.1758<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-07<BR>ratio: 1.0<BR>benchmarks:<BR>  NYU Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-07<BR>ratio: 0.3268<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>  WFLW - Face Alignment benchmarking: ME (%, all)<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2017-08<BR>ratio: 0.0645<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-08<BR>ratio: 0.2688<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  FDDB - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV II)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-08<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-08<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL Hands - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-08<BR>ratio: 0.0624<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2017-08<BR>ratio: 0.88<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2017-08<BR>ratio: 0.3456<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-08<BR>ratio: 0.0951<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-08<BR>ratio: 0.1108<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Object detection: Dense Object Detection<BR>date: 2017-08<BR>ratio: 0.6932<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-08<BR>ratio: 0.5449<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  SOC - RGB Salient Object Detection benchmarking: Average MAE<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2017-09<BR>ratio: 0.1598<BR>benchmarks:<BR>  QM9 - Formation Energy benchmarking: MAE<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2017-09<BR>ratio: 0.474<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2017-09<BR>ratio: 0.3904<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-09<BR>ratio: 0.0103<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Total Capture - 3D Human Pose Estimation benchmarking: Average MPJPE (mm)<BR>","<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking: Dice Score<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2017-09<BR>ratio: 0.0758<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma35 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2017-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Caltech Lanes Cordova - Lane Detection benchmarking: F1<BR>  Caltech Lanes Washington - Lane Detection benchmarking: F1<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2017-10<BR>ratio: 0.0909<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2017-10<BR>ratio: 0.9952<BR>benchmarks:<BR>  SBU - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>  UCF - RGB Salient Object Detection benchmarking: Balanced Error Rate<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-10<BR>ratio: 0.4958<BR>benchmarks:<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2127<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2017-10<BR>ratio: 0.4337<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-10<BR>ratio: 0.2492<BR>benchmarks:<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2017-10<BR>ratio: 0.2941<BR>benchmarks:<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>","<BR>task: Image generation: Image Generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2017-11<BR>ratio: 0.2656<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-11<BR>ratio: 0.6273<BR>benchmarks:<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  ShapeNet - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Clip Hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2017-11<BR>ratio: 0.619<BR>benchmarks:<BR>  COCO test-dev - Weakly Supervised Object Detection benchmarking: AP50<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2017-11<BR>ratio: 0.5452<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-11<BR>ratio: 0.6998<BR>benchmarks:<BR>  KITTI Cars Easy val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2017-11<BR>ratio: 0.1491<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>","<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-11<BR>ratio: 0.1976<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP50<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>","<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2017-11<BR>ratio: 0.5062<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-11<BR>ratio: 0.6503<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Toyota Smarthome dataset - Action Classification benchmarking: CS<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2017-11<BR>ratio: 0.3521<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2017-11<BR>ratio: 0.383<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-11<BR>ratio: 0.3953<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  ITOP front-view - Pose Estimation benchmarking: Mean mAP<BR>  ITOP top-view - Pose Estimation benchmarking: Mean mAP<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2017-11<BR>ratio: 0.1702<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-11<BR>ratio: 0.6857<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-11<BR>ratio: 0.069<BR>benchmarks:<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2017-11<BR>ratio: 0.2578<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2017-12<BR>ratio: 0.0546<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2017-12<BR>ratio: 0.0332<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  J-HMDB - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>","<BR>task: Pose estimation: Hand Pose Estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  HANDS 2017 - Hand Pose Estimation benchmarking: Average 3D Error<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: IS<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2017-12<BR>ratio: 0.7094<BR>benchmarks:<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2017-12<BR>ratio: 0.0238<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2017-12<BR>ratio: 0.1097<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2017-12<BR>ratio: 0.5417<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>","<BR>task: Object detection: Object Detection<BR>date: 2017-12<BR>ratio: 0.3524<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  SYNSIG-to-GTSRB - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2017-12<BR>ratio: 0.3532<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  ScanObjectNN - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Image classification: Document Image Classification<BR>date: 2018-01<BR>ratio: 0.5145<BR>benchmarks:<BR>  RVL-CDIP - Document Image Classification benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-01<BR>ratio: 0.4342<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-01<BR>ratio: 0.0837<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2018-01<BR>ratio: 0.3333<BR>benchmarks:<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-1 (%)<BR>  Moments in Time Dataset - Multimodal Activity Recognition benchmarking: Top-5 (%)<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Precision<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>","<BR>task: Image classification: Retinal OCT Disease Classification<BR>date: 2018-01<BR>ratio: 0.3661<BR>benchmarks:<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Acc<BR>  OCT2017 - Retinal OCT Disease Classification benchmarking: Sensitivity<BR>  Srinivasan2014 - Retinal OCT Disease Classification benchmarking: Acc<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-01<BR>ratio: 0.6486<BR>benchmarks:<BR>  MegaFace - Face Identification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-01<BR>ratio: 0.4232<BR>benchmarks:<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>  MegaFace - Face Verification benchmarking: Accuracy<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-02<BR>ratio: 0.1321<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-02<BR>ratio: 0.6229<BR>benchmarks:<BR>  OTB-2013 - Visual Object Tracking benchmarking: AUC<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>  OTB-50 - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-02<BR>ratio: 0.0351<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>","<BR>task: Semantic segmentation: Skin Cancer Segmentation<BR>date: 2018-02<BR>ratio: 0.4938<BR>benchmarks:<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: AUC<BR>  Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking: F1 score<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-02<BR>ratio: 0.2543<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  PASCAL VOC 2012 test - Semantic Segmentation benchmarking: Mean IoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-02<BR>ratio: 0.0865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-02<BR>ratio: 0.6875<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  STARE - Retinal Vessel Segmentation benchmarking: AUC<BR>  STARE - Retinal Vessel Segmentation benchmarking: F1 score<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-02<BR>ratio: 0.5545<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  ImageNet 32x32 - Image Generation benchmarking: bpd<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma50 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-02<BR>ratio: 0.9199<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>","<BR>task: Image classification: Unsupervised Image Classification<BR>date: 2018-02<BR>ratio: 0.3005<BR>benchmarks:<BR>  SVHN - Unsupervised Image Classification benchmarking: Acc<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Florence 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  J-HMBD Early Action - Skeleton Based Action Recognition benchmarking: 10%<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2018-02<BR>ratio: 0.2075<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-03<BR>ratio: 0.6418<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2018-03<BR>ratio: 0.0929<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>","<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ScanNet - Scene Segmentation benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-03<BR>ratio: 0.2249<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-03<BR>ratio: 0.1587<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-03<BR>ratio: 0.0083<BR>benchmarks:<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2018-03<BR>ratio: 0.2131<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-03<BR>ratio: 0.1771<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Charades - Action Detection benchmarking: mAP<BR>  Multi-THUMOS - Action Detection benchmarking: mAP<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-03<BR>ratio: 0.5083<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential MNIST - Sequential Image Classification benchmarking: Permuted Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-03<BR>ratio: 0.0203<BR>benchmarks:<BR>  YouTube Faces DB - Face Verification benchmarking: Accuracy<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Watercolor2k - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-03<BR>ratio: 1.0<BR>benchmarks:<BR>  IJB-A - Face Identification benchmarking: Accuracy<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2018-03<BR>ratio: 0.4588<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Object detection: Object Detection<BR>date: 2018-03<BR>ratio: 0.2555<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>  iSAID - Object Detection benchmarking: Average Precision<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-04<BR>ratio: 0.1925<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR75<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARL<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-04<BR>ratio: 0.1608<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2018-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Semantic segmentation: Pancreas Segmentation<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CT-150 - Pancreas Segmentation benchmarking: Dice Score<BR>  CT-150 - Pancreas Segmentation benchmarking: Precision<BR>  CT-150 - Pancreas Segmentation benchmarking: Recall<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.6<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.7<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UWA3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-04<BR>ratio: 0.0647<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>","<BR>task: Other 3D task: 3D Shape Classification<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-16<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-1<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-2<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-32<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-4<BR>  Pix3D - 3D Shape Classification benchmarking: R-at-8<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  ChaLearn val - Hand Gesture Recognition benchmarking: Accuracy<BR>  Jester test - Hand Gesture Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-04<BR>ratio: 0.0985<BR>benchmarks:<BR>  COCO-Text - Scene Text Detection benchmarking: F-Measure<BR>  COCO-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-04<BR>ratio: 0.5604<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-04<BR>ratio: 0.5045<BR>benchmarks:<BR>  Extended Yale-B - Image Clustering benchmarking: Accuracy<BR>  Extended Yale-B - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-04<BR>ratio: 0.3034<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL VOC 2012 val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-04<BR>ratio: 0.1429<BR>benchmarks:<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-04<BR>ratio: 0.2694<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: Fullset (public)<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  UAV-Human - Skeleton Based Action Recognition benchmarking: Average Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2018-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2018-05<BR>ratio: 0.714<BR>benchmarks:<BR>  300W - Face Alignment benchmarking: AUC0.08 private<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-05<BR>ratio: 0.4536<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>","<BR>task: Object detection: Object Detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  PASCAL VOC 2007 - Object Detection benchmarking: MAP<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2018-05<BR>ratio: 0.0149<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-06<BR>ratio: 0.1992<BR>benchmarks:<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-06<BR>ratio: 0.1822<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-06<BR>ratio: 0.4686<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: F1 score<BR>","<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking: MAP<BR>  Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking: MAP<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-06<BR>ratio: 0.7711<BR>benchmarks:<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: Abs Rel<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: RMSE log<BR>  Mid-Air Dataset - Monocular Depth Estimation benchmarking: SQ Rel<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (test)<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: F1 score<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-06<BR>ratio: 0.5515<BR>benchmarks:<BR>  J-HMDB - Skeleton Based Action Recognition benchmarking: Accuracy (RGB+pose)<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  UT-Kinect - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-06<BR>ratio: 0.1235<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  MAFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: 3DPCK<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: MJPE<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Expectancy)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Power)<BR>  SEMAINE - Emotion Recognition in Conversation benchmarking: MAE (Valence)<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-06<BR>ratio: 0.3698<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  MuJoCo - Multivariate Time Series Imputation benchmarking: MSE (10^2, 50% missing)<BR>  PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking: mse (10^-3)<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  VRD - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-06<BR>ratio: 0.297<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2018-06<BR>ratio: 0.5603<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2018-06<BR>ratio: 0.2185<BR>benchmarks:<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD68 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Set12 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma15 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma25 - Grayscale Image Denoising benchmarking: PSNR<BR>  Urban100 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2018-06<BR>ratio: 0.3576<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2018-06<BR>ratio: 0.5207<BR>benchmarks:<BR>  Materials Project - Formation Energy benchmarking: MAE<BR>  QM9 - Formation Energy benchmarking: MAE<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-07<BR>ratio: 0.4007<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>  ImageNet - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-07<BR>ratio: 0.9856<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-07<BR>ratio: 0.1059<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-07<BR>ratio: 0.1448<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-07<BR>ratio: 0.5625<BR>benchmarks:<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Denoising<BR>date: 2018-07<BR>ratio: 0.1158<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  CAT 256x256 - Image Generation benchmarking: FID<BR>  ImageNet 64x64 - Image Generation benchmarking: Bits per dim<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  ActivityNet-1.2 - Action Classification benchmarking: mAP<BR>  THUMOS\u201914 - Action Classification benchmarking: mAP<BR>","<BR>task: Object detection: Pedestrian Detection<BR>date: 2018-07<BR>ratio: 0.5357<BR>benchmarks:<BR>  CityPersons - Pedestrian Detection benchmarking: Bare MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Partial MR^-2<BR>  CityPersons - Pedestrian Detection benchmarking: Reasonable MR^-2<BR>","<BR>task: Semantic segmentation: Medical Image Segmentation<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: S-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: max E-Measure<BR>  Kvasir-SEG - Medical Image Segmentation benchmarking: mean Dice<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-07<BR>ratio: 0.3854<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: ARI<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-10 - Image Clustering benchmarking: NMI<BR>","<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2018-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Pavia University - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2018-07<BR>ratio: 0.5<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: PSNR (sRGB)<BR>  Darmstadt Noise Dataset - Color Image Denoising benchmarking: SSIM (sRGB)<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-07<BR>ratio: 0.0631<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Freiburg Forest - Semantic Segmentation benchmarking: Mean IoU<BR>  ScanNetV2 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2018-08<BR>ratio: 0.4543<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>  KITTI Depth Completion - Depth Completion benchmarking: Runtime [ms]<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-08<BR>ratio: 0.0321<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Reconstruction benchmarking: 3DIoU<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>","<BR>task: Image generation: Conditional Image Generation<BR>date: 2018-09<BR>ratio: 0.6183<BR>benchmarks:<BR>  CIFAR-10 - Conditional Image Generation benchmarking: FID<BR>  CIFAR-10 - Conditional Image Generation benchmarking: Inception score<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: FID<BR>  ImageNet 128x128 - Conditional Image Generation benchmarking: Inception score<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-09<BR>ratio: 0.1346<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  CIFAR-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Annotated Faces in the Wild - Face Detection benchmarking: AP<BR>  PASCAL Face - Face Detection benchmarking: AP<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.086<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-09<BR>ratio: 0.1071<BR>benchmarks:<BR>  COCO-Stuff test - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-09<BR>ratio: 0.2687<BR>benchmarks:<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-09<BR>ratio: 0.8995<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.01<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Semantic segmentation: Nuclear Segmentation<BR>date: 2018-09<BR>ratio: 0.1255<BR>benchmarks:<BR>  Cell17 - Nuclear Segmentation benchmarking: Dice<BR>  Cell17 - Nuclear Segmentation benchmarking: F1-score<BR>  Cell17 - Nuclear Segmentation benchmarking: Hausdorff<BR>","<BR>task: Facial recognition and modelling: Face Detection<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  FDDB - Face Detection benchmarking: AP<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2018-10<BR>ratio: 0.4091<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>","<BR>task: Other image process: Aesthetics Quality Assessment<BR>date: 2018-10<BR>ratio: 0.0658<BR>benchmarks:<BR>  AVA - Aesthetics Quality Assessment benchmarking: Accuracy<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-10<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>","<BR>task: Image classification: Sequential Image Classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential CIFAR-10 - Sequential Image Classification benchmarking: Unpermuted Accuracy<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2018-10<BR>ratio: 0.1139<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-10<BR>ratio: 0.2227<BR>benchmarks:<BR>  CMU-PIE - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-10<BR>ratio: 0.0087<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-11<BR>ratio: 0.7924<BR>benchmarks:<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>","<BR>task: Image classification: Image Classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.34<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Object detection: Object Detection<BR>date: 2018-11<BR>ratio: 0.2295<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: AP75<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: box AP<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Video Object Detection<BR>date: 2018-11<BR>ratio: 0.6415<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2018-11<BR>ratio: 0.3352<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  Oxf105k - Image Retrieval benchmarking: MAP<BR>  Oxf5k - Image Retrieval benchmarking: MAP<BR>  Par106k - Image Retrieval benchmarking: mAP<BR>  Par6k - Image Retrieval benchmarking: mAP<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-11<BR>ratio: 0.5789<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Video Prediction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Video Prediction benchmarking: MAE<BR>  Human3.6M - Video Prediction benchmarking: MSE<BR>  Human3.6M - Video Prediction benchmarking: SSIM<BR>","<BR>task: Other 3D task: 3D Reconstruction<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Scan2CAD - 3D Reconstruction benchmarking: Average Accuracy<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-11<BR>ratio: 0.2788<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-1 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2018-11<BR>ratio: 0.7527<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-11<BR>ratio: 0.6675<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2018-11<BR>ratio: 0.0755<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2018-11<BR>ratio: 0.3967<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Human Interaction Recognition<BR>date: 2018-11<BR>ratio: 0.3724<BR>benchmarks:<BR>  BIT - Human Interaction Recognition benchmarking: Accuracy<BR>  UT - Human Interaction Recognition benchmarking: Accuracy<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-11<BR>ratio: 0.7361<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>","<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>","<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Frames Per View<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  CUB 128 x 128 - Image Generation benchmarking: FID<BR>  CUB 128 x 128 - Image Generation benchmarking: Inception score<BR>  Stanford Cars - Image Generation benchmarking: FID<BR>  Stanford Cars - Image Generation benchmarking: Inception score<BR>  Stanford Dogs - Image Generation benchmarking: FID<BR>  Stanford Dogs - Image Generation benchmarking: Inception score<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2018-11<BR>ratio: 0.2526<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: F-Measure<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Precision<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-11<BR>ratio: 0.0286<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-11<BR>ratio: 0.5387<BR>benchmarks:<BR>  CUB Birds - Image Clustering benchmarking: Accuracy<BR>  CUB Birds - Image Clustering benchmarking: NMI<BR>  Stanford Cars - Image Clustering benchmarking: Accuracy<BR>  Stanford Cars - Image Clustering benchmarking: NMI<BR>  Stanford Dogs - Image Clustering benchmarking: Accuracy<BR>  Stanford Dogs - Image Clustering benchmarking: NMI<BR>","<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2018-11<BR>ratio: 0.0423<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  DensePose-COCO - Pose Estimation benchmarking: AP<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2018-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: MAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2018-12<BR>ratio: 0.1735<BR>benchmarks:<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - 3D Object Detection benchmarking: AP<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2018-12<BR>ratio: 0.0597<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2018-12<BR>ratio: 0.1767<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Fashion-MNIST - Image Clustering benchmarking: NMI<BR>  MNIST-full - Image Clustering benchmarking: Accuracy<BR>  MNIST-full - Image Clustering benchmarking: NMI<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2018-12<BR>ratio: 0.5811<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQth<BR>  Cityscapes val - Panoptic Segmentation benchmarking: AP<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>","<BR>task: Other vision process: Multivariate Time Series Imputation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: OOB Rate (10^\u22123)<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Difference<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Path Length<BR>  Basketball Players Movement - Multivariate Time Series Imputation benchmarking: Step Change (10^\u22123)<BR>  PEMS-SF - Multivariate Time Series Imputation benchmarking: L2 Loss (10^-4)<BR>","<BR>task: Object detection: Object Detection<BR>date: 2018-12<BR>ratio: 0.4227<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2018-12<BR>ratio: 0.2451<BR>benchmarks:<BR>  CamVid - Semantic Segmentation benchmarking: Mean IoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2018-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking: mAP-at-0.5<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2018-12<BR>ratio: 0.549<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>  YCB-Video - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2018-12<BR>ratio: 0.5<BR>benchmarks:<BR>  AVA v2.1 - Action Recognition benchmarking: mAP (Val)<BR>  AVA v2.2 - Action Recognition benchmarking: mAP<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2018-12<BR>ratio: 0.8571<BR>benchmarks:<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2018-12<BR>ratio: 0.2902<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Verification benchmarking: Accuracy<BR>","<BR>task: Facial recognition and modelling: Face Identification<BR>date: 2018-12<BR>ratio: 0.3195<BR>benchmarks:<BR>  Trillion Pairs Dataset - Face Identification benchmarking: Accuracy<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2018-12<BR>ratio: 0.7779<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  TrackingNet - Visual Object Tracking benchmarking: Accuracy<BR>  TrackingNet - Visual Object Tracking benchmarking: Normalized Precision<BR>  TrackingNet - Visual Object Tracking benchmarking: Precision<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  NVGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>  VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2018-12<BR>ratio: 0.5095<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2018-12<BR>ratio: 0.221<BR>benchmarks:<BR>  AFLW - Head Pose Estimation benchmarking: MAE<BR>","<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2018-12<BR>ratio: 0.5107<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2018-12<BR>ratio: 0.2741<BR>benchmarks:<BR>  COCO - Keypoint Detection benchmarking: Test AP<BR>  COCO - Keypoint Detection benchmarking: Validation AP<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2018-12<BR>ratio: 0.3083<BR>benchmarks:<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2018-12<BR>ratio: 0.1343<BR>benchmarks:<BR>  Charades - Action Classification benchmarking: MAP<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Image generation: Image Generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  CelebA 256x256 - Image Generation benchmarking: bpd<BR>  CelebA-HQ 1024x1024 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Bedroom 256 x 256 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: Inception score<BR>","<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0323<BR>benchmarks:<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>","<BR>task: Activity recognition: Multimodal Activity Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  LboroHAR - Multimodal Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-01<BR>ratio: 0.4<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-01<BR>ratio: 0.1641<BR>benchmarks:<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AP<BR>  COCO test-challenge - Keypoint Detection benchmarking: APL<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR50<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR75<BR>  COCO test-challenge - Keypoint Detection benchmarking: AR<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARL<BR>  COCO test-challenge - Keypoint Detection benchmarking: ARM<BR>  COCO test-dev - Keypoint Detection benchmarking: AP50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR50<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>  COCO test-dev - Keypoint Detection benchmarking: ARM<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-01<BR>ratio: 0.3272<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-01<BR>ratio: 0.0894<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP50<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Office-31 - Domain Adaptation benchmarking: Average Accuracy<BR>  VisDA2017 - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  MNIST-test - Image Clustering benchmarking: Accuracy<BR>  MNIST-test - Image Clustering benchmarking: NMI<BR>  USPS - Image Clustering benchmarking: Accuracy<BR>  USPS - Image Clustering benchmarking: NMI<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  Kuzushiji-MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Other 3D task: 3D Room Layouts From A Single RGB Panorama<BR>date: 2019-01<BR>ratio: 0.371<BR>benchmarks:<BR>  PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>  Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking: 3DIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-01<BR>ratio: 0.1086<BR>benchmarks:<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Gesture recognition: Hand Gesture Recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Cambridge - Hand Gesture Recognition benchmarking: Accuracy<BR>  EgoGesture - Hand Gesture Recognition benchmarking: Accuracy<BR>","<BR>task: Other 3D task: 3D Object Reconstruction<BR>date: 2019-01<BR>ratio: 0.1212<BR>benchmarks:<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: 3DIoU<BR>  Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking: Avg F1<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-01<BR>ratio: 0.2126<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQst<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-01<BR>ratio: 1.0<BR>benchmarks:<BR>  ICVL-4 - Action Recognition benchmarking: Accuracy<BR>  IRD - Action Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-01<BR>ratio: 0.7353<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-02<BR>ratio: 0.068<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  INRIA Holidays - Image Retrieval benchmarking: Mean mAP<BR>","<BR>task: Pose estimation: Keypoint Detection<BR>date: 2019-02<BR>ratio: 0.036<BR>benchmarks:<BR>  COCO test-dev - Keypoint Detection benchmarking: AP75<BR>  COCO test-dev - Keypoint Detection benchmarking: APL<BR>  COCO test-dev - Keypoint Detection benchmarking: APM<BR>  COCO test-dev - Keypoint Detection benchmarking: AR<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-02<BR>ratio: 0.0018<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-02<BR>ratio: 0.1324<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy (ADD)<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Mean ADD<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>  PoseTrack2017 - Pose Tracking benchmarking: mAP<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-02<BR>ratio: 0.4587<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2019-02<BR>ratio: 0.069<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: RMSE<BR>","<BR>task: Image classification: Hyperspectral Image Classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Indian Pines - Hyperspectral Image Classification benchmarking: Overall Accuracy<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (7 emotion)<BR>  JAFFE - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2019-02<BR>ratio: 0.1426<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Facial recognition and modelling: Facial Landmark Detection<BR>date: 2019-02<BR>ratio: 0.6096<BR>benchmarks:<BR>  300W - Facial Landmark Detection benchmarking: NME<BR>  AFLW-Full - Facial Landmark Detection benchmarking: Mean NME<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  ImageNet-C - Domain Generalization benchmarking: mean Corruption Error (mCE)<BR>","<BR>task: Pose estimation: Weakly-supervised 3D Human Pose Estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking: Number of Views<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-03<BR>ratio: 0.098<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  CelebA-HQ 128x128 - Image Generation benchmarking: FID<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mRec<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-03<BR>ratio: 0.3109<BR>benchmarks:<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2017 MLT - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2019-03<BR>ratio: 0.3213<BR>benchmarks:<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  BUAA-VisNir - Face Verification benchmarking: TAR at FAR=0.001<BR>  CASIA NIR-VIS 2.0 - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.001<BR>  Oulu-CASIA NIR-VIS - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-03<BR>ratio: 0.6399<BR>benchmarks:<BR>  CARS196 - Image Retrieval benchmarking: R-at-1<BR>  CUB-200-2011 - Image Retrieval benchmarking: R-at-1<BR>  In-Shop - Image Retrieval benchmarking: R-at-1<BR>  SOP - Image Retrieval benchmarking: R-at-1<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - 3D Object Detection benchmarking: AP<BR>  nuScenes - 3D Object Detection benchmarking: NDS<BR>","<BR>task: Other image process: Image Reconstruction<BR>date: 2019-03<BR>ratio: 0.2896<BR>benchmarks:<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: FID<BR>  Edge-to-Handbags - Image Reconstruction benchmarking: LPIPS<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: FID<BR>  Edge-to-Shoes - Image Reconstruction benchmarking: LPIPS<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-03<BR>ratio: 0.4466<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-03<BR>ratio: 0.5464<BR>benchmarks:<BR>  KITTI Eigen split - Monocular Depth Estimation benchmarking: absolute relative error<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-03<BR>ratio: 0.9015<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>","<BR>task: Activity recognition: Group Activity Recognition<BR>date: 2019-04<BR>ratio: 0.6033<BR>benchmarks:<BR>  Collective Activity - Group Activity Recognition benchmarking: Accuracy<BR>  Volleyball - Group Activity Recognition benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-04<BR>ratio: 0.2881<BR>benchmarks:<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.1<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.2<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.3<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.4<BR>  JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking: PCK-at-0.5<BR>  N-UCLA - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  SYSU 3D - Skeleton Based Action Recognition benchmarking: Accuracy<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (AV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CS)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV I)<BR>  Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking: Accuracy (CV II)<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>","<BR>task: Other image process: Image Clustering<BR>date: 2019-04<BR>ratio: 0.3733<BR>benchmarks:<BR>  CIFAR-10 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: Accuracy<BR>  CIFAR-100 - Image Clustering benchmarking: NMI<BR>  ImageNet-10 - Image Clustering benchmarking: Accuracy<BR>  ImageNet-10 - Image Clustering benchmarking: NMI<BR>  Imagenet-dog-15 - Image Clustering benchmarking: Accuracy<BR>  Imagenet-dog-15 - Image Clustering benchmarking: NMI<BR>  STL-10 - Image Clustering benchmarking: Accuracy<BR>  STL-10 - Image Clustering benchmarking: NMI<BR>  Tiny-ImageNet - Image Clustering benchmarking: Accuracy<BR>  Tiny-ImageNet - Image Clustering benchmarking: NMI<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-04<BR>ratio: 0.6718<BR>benchmarks:<BR>  IJB-A - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.001<BR>  IJB-C - Face Verification benchmarking: TAR at FAR=0.01<BR>","<BR>task: Other image process: Color Image Denoising<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  CBSD68 sigma15 - Color Image Denoising benchmarking: PSNR<BR>  CBSD68 sigma75 - Color Image Denoising benchmarking: PSNR<BR>","<BR>task: Other 3D task: 3D Point Cloud Classification<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  ModelNet40 - 3D Point Cloud Classification benchmarking: Overall Accuracy<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-04<BR>ratio: 0.162<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@5<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  EC - Emotion Recognition in Conversation benchmarking: Micro-F1<BR>","<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-04<BR>ratio: 0.6555<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP50<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: AP75<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Object detection: Dense Object Detection<BR>date: 2019-04<BR>ratio: 0.1943<BR>benchmarks:<BR>  SKU-110K - Dense Object Detection benchmarking: AP75<BR>  SKU-110K - Dense Object Detection benchmarking: AP<BR>","<BR>task: Image generation: Pose Transfer<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Deep-Fashion - Pose Transfer benchmarking: SSIM<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-04<BR>ratio: 0.0663<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-1<BR>  Sports-1M - Action Recognition benchmarking: Video hit-at-5<BR>","<BR>task: Other vision process: Visual Dialog<BR>date: 2019-04<BR>ratio: 0.4213<BR>benchmarks:<BR>  VisDial v0.9 val - Visual Dialog benchmarking: MRR<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-10<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-1<BR>  VisDial v0.9 val - Visual Dialog benchmarking: R-at-5<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: MRR (x 100)<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: Mean<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-10<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-1<BR>  Visual Dialog v1.0 test-std - Visual Dialog benchmarking: R-at-5<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-04<BR>ratio: 0.1123<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Class Average IoU<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Activity detection: Action Detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.1<BR>  UCF101-24 - Action Detection benchmarking: Video-mAP 0.2<BR>","<BR>task: Semantic segmentation: 3D Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.4321<BR>benchmarks:<BR>  SemanticKITTI - 3D Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Object recognition: Traffic Sign Recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP at-0.5:0.95<BR>  DFG traffic-sign dataset - Traffic Sign Recognition benchmarking: mAP-at-0.50<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-04<BR>ratio: 0.6714<BR>benchmarks:<BR>  Charades - Weakly Supervised Object Detection benchmarking: MAP<BR>  HICO-DET - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-04<BR>ratio: 0.3147<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-04<BR>ratio: 0.3574<BR>benchmarks:<BR>  SUN-RGBD val - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-04<BR>ratio: 0.339<BR>benchmarks:<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2013 - Scene Text Detection benchmarking: Recall<BR>  MSRA-TD500 - Scene Text Detection benchmarking: H-Mean<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>  SCUT-CTW1500 - Scene Text Detection benchmarking: H-Mean<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>","<BR>task: Other vision process: Object Counting<BR>date: 2019-04<BR>ratio: 0.3247<BR>benchmarks:<BR>  CARPK - Object Counting benchmarking: MAE<BR>  CARPK - Object Counting benchmarking: RMSE<BR>","<BR>task: Other vision process: Curved Text Detection<BR>date: 2019-04<BR>ratio: 0.7679<BR>benchmarks:<BR>  SCUT-CTW1500 - Curved Text Detection benchmarking: F-Measure<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-04<BR>ratio: 0.1341<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: Mean IoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS - Semantic Segmentation benchmarking: oAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mAcc<BR>  S3DIS Area5 - Semantic Segmentation benchmarking: mIoU<BR>  ScanNet - Semantic Segmentation benchmarking: 3DIoU<BR>","<BR>task: Other vision process: Denoising<BR>date: 2019-04<BR>ratio: 0.0989<BR>benchmarks:<BR>  Darmstadt Noise Dataset - Denoising benchmarking: PSNR<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-04<BR>ratio: 0.7784<BR>benchmarks:<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: F-measure<BR>  DUT-OMRON - RGB Salient Object Detection benchmarking: MAE<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: F-measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: MAE<BR>  ECSSD - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: F-measure<BR>  HKU-IS - RGB Salient Object Detection benchmarking: MAE<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: F-measure<BR>  PASCAL-S - RGB Salient Object Detection benchmarking: MAE<BR>  SOD - RGB Salient Object Detection benchmarking: MAE<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-04<BR>ratio: 0.0382<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-04<BR>ratio: 0.0519<BR>benchmarks:<BR>  FFHQ - Image Generation benchmarking: FID<BR>","<BR>task: Other vision process: Video Prediction<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  CMU Mocap-2 - Video Prediction benchmarking: Test Error<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-05<BR>ratio: 0.0144<BR>benchmarks:<BR>  Jester - Action Recognition benchmarking: Val<BR>","<BR>task: Other vision process: Formation Energy<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  OQMD v1.2 - Formation Energy benchmarking: MAE<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-05<BR>ratio: 0.4235<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>","<BR>task: Other vision process: Depth Completion<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  KITTI Depth Completion - Depth Completion benchmarking: iMAE<BR>  KITTI Depth Completion - Depth Completion benchmarking: iRMSE<BR>","<BR>task: Other vision process: Domain Generalization<BR>date: 2019-05<BR>ratio: 0.2258<BR>benchmarks:<BR>  ImageNet-A - Domain Generalization benchmarking: Top-1 accuracy %<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  AffectNet - Facial Expression Recognition benchmarking: Accuracy (8 emotion)<BR>  FERPlus - Facial Expression Recognition benchmarking: Accuracy<BR>  SFEW - Facial Expression Recognition benchmarking: Accuracy<BR>","<BR>task: Pose tracking: Pose Tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  PoseTrack2017 - Pose Tracking benchmarking: MOTA<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-05<BR>ratio: 0.4063<BR>benchmarks:<BR>  HMDBfull-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  HMDBsmall-to-UCF - Domain Adaptation benchmarking: Accuracy<BR>  Olympic-to-HMDBsmall - Domain Adaptation benchmarking: Accuracy<BR>  SVNH-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>  UCF-to-HMDBfull - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Indian Driving Dataset - Panoptic Segmentation benchmarking: PQ<BR>  KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Other vision process: Horizon Line Estimation<BR>date: 2019-05<BR>ratio: 1.0<BR>benchmarks:<BR>  Horizon Lines in the Wild - Horizon Line Estimation benchmarking: AUC (horizon error)<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-05<BR>ratio: 0.4905<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S2)<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-05<BR>ratio: 0.0225<BR>benchmarks:<BR>  Kinetics-400 - Action Classification benchmarking: Vid acc@1<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  STL-10 - Image Classification benchmarking: Percentage correct<BR>  iNaturalist - Image Classification benchmarking: Top 1 Accuracy<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2174<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-06<BR>ratio: 0.2194<BR>benchmarks:<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>","<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: Total Variation of Information<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Merge<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: VI Split<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-06<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2015 - Brain Tumor Segmentation benchmarking: Dice Score<BR>  BRATS-2017 val - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-06<BR>ratio: 0.0493<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  PASCAL Context - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  CrossTask - Temporal Action Localization benchmarking: Recall<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  OTB-2015 - Visual Object Tracking benchmarking: AUC<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-06<BR>ratio: 0.066<BR>benchmarks:<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-10<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-1<BR>  YouCook2 - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-06<BR>ratio: 0.9629<BR>benchmarks:<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Pose estimation: Head Pose Estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  AFLW2000 - Head Pose Estimation benchmarking: MAE<BR>","<BR>task: Activity recognition: Action Classification<BR>date: 2019-06<BR>ratio: 0.4736<BR>benchmarks:<BR>  Kinetics-600 - Action Classification benchmarking: Top-1 Accuracy<BR>  Kinetics-600 - Action Classification benchmarking: Top-5 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 1 Accuracy<BR>  Moments in Time - Action Classification benchmarking: Top 5 Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-06<BR>ratio: 0.0171<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Object detection: RGB Salient Object Detection<BR>date: 2019-06<BR>ratio: 0.7183<BR>benchmarks:<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: S-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean E-Measure<BR>  DUTS-TE - RGB Salient Object Detection benchmarking: mean F-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: S-Measure<BR>  SOC - RGB Salient Object Detection benchmarking: mean E-Measure<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2019-06<BR>ratio: 0.9021<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>  UCF CC 50 - Crowd Counting benchmarking: MAE<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-06<BR>ratio: 0.1952<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-06<BR>ratio: 0.0318<BR>benchmarks:<BR>  HMDB-51 - Action Recognition benchmarking: Average accuracy of 3 splits<BR>  UCF101 - Action Recognition benchmarking: 3-fold Accuracy<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2019-07<BR>ratio: 0.5126<BR>benchmarks:<BR>  KITTI Cars Easy - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - 3D Object Detection benchmarking: AP<BR>  KITTI Cars Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Cyclist Moderate val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Easy val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Hard val - 3D Object Detection benchmarking: AP<BR>  KITTI Pedestrian Moderate val - 3D Object Detection benchmarking: AP<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>","<BR>task: Other vision process: Monocular Depth Estimation<BR>date: 2019-07<BR>ratio: 0.2932<BR>benchmarks:<BR>  NYU-Depth V2 - Monocular Depth Estimation benchmarking: RMSE<BR>","<BR>task: Object detection: Video Object Detection<BR>date: 2019-07<BR>ratio: 0.1509<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-07<BR>ratio: 0.066<BR>benchmarks:<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.3<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.4<BR>  THUMOS\u201914 - Action Recognition benchmarking: mAP-at-0.5<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-07<BR>ratio: 0.5481<BR>benchmarks:<BR>  GQA test-std - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Activity localization: Temporal Action Proposal Generation<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AR@100<BR>  ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking: AUC (val)<BR>","<BR>task: Image-to-image translation: Fundus to Angiography Generation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking: FID<BR>","<BR>task: Other video process: Video Retrieval<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-10<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-1<BR>  LSMDC - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: text-to-video R-at-5<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-10<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-1<BR>  MSR-VTT - Video Retrieval benchmarking: video-to-text R-at-5<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-10<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-1<BR>  MSR-VTT-1kA - Video Retrieval benchmarking: text-to-video R-at-5<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-07<BR>ratio: 0.2647<BR>benchmarks:<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 14 gestures accuracy<BR>  SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking: 28 gestures accuracy<BR>","<BR>task: Object tracking: Visual Object Tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  VOT2016 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  VOT2017/18 - Visual Object Tracking benchmarking: Expected Average Overlap (EAO)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Seen)<BR>  YouTube-VOS - Visual Object Tracking benchmarking: Jaccard (Unseen)<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-07<BR>ratio: 0.6173<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking: AP<BR>  KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.75<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-08<BR>ratio: 0.3316<BR>benchmarks:<BR>  Fashion-MNIST - Image Classification benchmarking: Percentage error<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-08<BR>ratio: 0.3395<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>  Something-Something V1 - Action Recognition benchmarking: Top 5 Accuracy<BR>  Something-Something V2 - Action Recognition benchmarking: Top-5 Accuracy<BR>","<BR>task: Semantic segmentation: Electron Microscopy Image Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SNEMI3D - Electron Microscopy Image Segmentation benchmarking: AUC<BR>","<BR>task: Other vision process: Metric Learning<BR>date: 2019-08<BR>ratio: 0.3549<BR>benchmarks:<BR>  CARS196 - Metric Learning benchmarking: R-at-1<BR>  CUB-200-2011 - Metric Learning benchmarking: R-at-1<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-08<BR>ratio: 0.1793<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APL<BR>  COCO minival - Object Detection benchmarking: APM<BR>  COCO minival - Object Detection benchmarking: APS<BR>","<BR>task: Other vision process: Crowd Counting<BR>date: 2019-08<BR>ratio: 0.1469<BR>benchmarks:<BR>  ShanghaiTech A - Crowd Counting benchmarking: MAE<BR>  ShanghaiTech B - Crowd Counting benchmarking: MAE<BR>","<BR>task: Facial recognition and modelling: Face Alignment<BR>date: 2019-08<BR>ratio: 0.0324<BR>benchmarks:<BR>  WFLW - Face Alignment benchmarking: AUC-at-0.1 (all)<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-08<BR>ratio: 0.0978<BR>benchmarks:<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Lung Nodule Segmentation<BR>date: 2019-08<BR>ratio: 0.7344<BR>benchmarks:<BR>  LUNA - Lung Nodule Segmentation benchmarking: AUC<BR>  LUNA - Lung Nodule Segmentation benchmarking: F1 score<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking: mIoU<BR>","<BR>task: Object detection: Lane Detection<BR>date: 2019-08<BR>ratio: 0.4583<BR>benchmarks:<BR>  TuSimple - Lane Detection benchmarking: Accuracy<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Accuracy<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Macro-F1<BR>  IEMOCAP - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>","<BR>task: Facial recognition and modelling: Unsupervised Facial Landmark Detection<BR>date: 2019-08<BR>ratio: 0.6596<BR>benchmarks:<BR>  300W - Unsupervised Facial Landmark Detection benchmarking: NME<BR>  AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking: NME<BR>","<BR>task: Semantic segmentation: Scene Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - Scene Segmentation benchmarking: Mean IoU<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking: Actions Top-1 (S1)<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-08<BR>ratio: 0.4622<BR>benchmarks:<BR>  GQA Test2019 - Visual Question Answering benchmarking: Accuracy<BR>  GQA Test2019 - Visual Question Answering benchmarking: Binary<BR>  GQA Test2019 - Visual Question Answering benchmarking: Consistency<BR>  GQA Test2019 - Visual Question Answering benchmarking: Distribution<BR>  GQA Test2019 - Visual Question Answering benchmarking: Open<BR>  GQA Test2019 - Visual Question Answering benchmarking: Plausibility<BR>  GQA Test2019 - Visual Question Answering benchmarking: Validity<BR>  VQA v2 test-dev - Visual Question Answering benchmarking: Accuracy<BR>  VQA v2 test-std - Visual Question Answering benchmarking: overall<BR>  VizWiz 2018 - Visual Question Answering benchmarking: overall<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-08<BR>ratio: 0.2284<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>","<BR>task: Image classification: Document Image Classification<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Noisy Bangla Characters - Document Image Classification benchmarking: Accuracy<BR>  Noisy Bangla Numeral - Document Image Classification benchmarking: Accuracy<BR>  n-MNIST - Document Image Classification benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Brain Tumor Segmentation<BR>date: 2019-08<BR>ratio: 1.0<BR>benchmarks:<BR>  BRATS-2013 - Brain Tumor Segmentation benchmarking: Dice Score<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-08<BR>ratio: 0.3737<BR>benchmarks:<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>  STL-10 - Image Generation benchmarking: FID<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.1<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.2<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.3<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.4<BR>  THUMOS\u201914 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  COCO test-dev - Object Detection benchmarking: AP50<BR>  COCO test-dev - Object Detection benchmarking: AP75<BR>  COCO test-dev - Object Detection benchmarking: APL<BR>  COCO test-dev - Object Detection benchmarking: APM<BR>  COCO test-dev - Object Detection benchmarking: APS<BR>  COCO test-dev - Object Detection benchmarking: box AP<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-09<BR>ratio: 0.071<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Object tracking: Multiple Object Tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  KITTI Tracking test - Multiple Object Tracking benchmarking: MOTA<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CS)<BR>  PKU-MMD - Skeleton Based Action Recognition benchmarking: mAP-at-0.50 (CV)<BR>","<BR>task: Emotion recognition: Emotion Recognition in Conversation<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  MELD - Emotion Recognition in Conversation benchmarking: Weighted-F1<BR>","<BR>task: Other image process: Image Retrieval<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-10<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-1<BR>  Flickr30K 1K test - Image Retrieval benchmarking: R-at-5<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-09<BR>ratio: 0.4198<BR>benchmarks:<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQth<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGB<BR>date: 2019-09<BR>ratio: 0.0137<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGB benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: 3D Part Segmentation<BR>date: 2019-09<BR>ratio: 0.1818<BR>benchmarks:<BR>  ShapeNet-Part - 3D Part Segmentation benchmarking: Instance Average IoU<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-09<BR>ratio: 0.9068<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2019-09<BR>ratio: 0.1111<BR>benchmarks:<BR>  VQA-CP - Visual Question Answering benchmarking: Score<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-09<BR>ratio: 0.5071<BR>benchmarks:<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  USPS-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-09<BR>ratio: 1.0<BR>benchmarks:<BR>  PASCAL VOC 2007 - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Semantic segmentation: Human Part Segmentation<BR>date: 2019-10<BR>ratio: 0.2772<BR>benchmarks:<BR>  CIHP - Human Part Segmentation benchmarking: Mean IoU<BR>  PASCAL-Part - Human Part Segmentation benchmarking: mIoU<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-10<BR>ratio: 0.0466<BR>benchmarks:<BR>  KITTI Cars Easy - Object Detection benchmarking: AP<BR>  KITTI Cars Hard - Object Detection benchmarking: AP<BR>  KITTI Cars Moderate - Object Detection benchmarking: AP<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-10<BR>ratio: 0.0387<BR>benchmarks:<BR>  ICDAR 2015 - Scene Text Detection benchmarking: F-Measure<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Precision<BR>  ICDAR 2015 - Scene Text Detection benchmarking: Recall<BR>  Total-Text - Scene Text Detection benchmarking: F-Measure<BR>  Total-Text - Scene Text Detection benchmarking: Precision<BR>  Total-Text - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other image process: Grayscale Image Denoising<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  BSD200 sigma10 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma30 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma50 - Grayscale Image Denoising benchmarking: PSNR<BR>  BSD200 sigma70 - Grayscale Image Denoising benchmarking: PSNR<BR>","<BR>task: Object detection: Birds Eye View Object Detection<BR>date: 2019-10<BR>ratio: 0.2791<BR>benchmarks:<BR>  KITTI Cars Easy - Birds Eye View Object Detection benchmarking: AP<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-10<BR>ratio: 0.0533<BR>benchmarks:<BR>  PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  COCO test-dev - Pose Estimation benchmarking: AP75<BR>  COCO test-dev - Pose Estimation benchmarking: AP<BR>  COCO test-dev - Pose Estimation benchmarking: APL<BR>  COCO test-dev - Pose Estimation benchmarking: APM<BR>  COCO test-dev - Pose Estimation benchmarking: AR<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-10<BR>ratio: 0.5032<BR>benchmarks:<BR>  LIP val - Semantic Segmentation benchmarking: mIoU<BR>  SkyScapes-Dense - Semantic Segmentation benchmarking: Mean IoU<BR>  SkyScapes-Lane - Semantic Segmentation benchmarking: Mean IoU<BR>","<BR>task: Facial recognition and modelling: Face Verification<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  AgeDB-30 - Face Verification benchmarking: Accuracy<BR>  CFP-FP - Face Verification benchmarking: Accuracy<BR>  Labeled Faces in the Wild - Face Verification benchmarking: Accuracy<BR>","<BR>task: Object recognition: Pedestrian Attribute Recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  PA-100K - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  PETA - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>  RAP - Pedestrian Attribute Recognition benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2019-11<BR>ratio: 0.2369<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Test Score<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  Cityscapes test - Semantic Segmentation benchmarking: Mean IoU (class)<BR>  Cityscapes val - Semantic Segmentation benchmarking: mIoU<BR>  S3DIS - Semantic Segmentation benchmarking: mAcc<BR>  Semantic3D - Semantic Segmentation benchmarking: mIoU<BR>  Semantic3D - Semantic Segmentation benchmarking: oAcc<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.2438<BR>benchmarks:<BR>  COCO minival - Instance Segmentation benchmarking: mask AP<BR>  COCO test-dev - Instance Segmentation benchmarking: AP50<BR>  COCO test-dev - Instance Segmentation benchmarking: AP75<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>  COCO test-dev - Instance Segmentation benchmarking: APM<BR>  COCO test-dev - Instance Segmentation benchmarking: APS<BR>  Cityscapes test - Instance Segmentation benchmarking: Average Precision<BR>","<BR>task: Facial recognition and modelling: Facial Expression Recognition<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Oulu-CASIA - Facial Expression Recognition benchmarking: Accuracy (10-fold)<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking: AP-at-0.7<BR>","<BR>task: Image classification: Satellite Image Classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  SAT-4 - Satellite Image Classification benchmarking: Accuracy<BR>  SAT-6 - Satellite Image Classification benchmarking: Accuracy<BR>","<BR>task: Object detection: Object Detection<BR>date: 2019-11<BR>ratio: 0.0635<BR>benchmarks:<BR>  COCO minival - Object Detection benchmarking: APS<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  CIFAR-10 - Image Generation benchmarking: FID<BR>","<BR>task: Object detection: Weakly Supervised Object Detection<BR>date: 2019-11<BR>ratio: 0.0283<BR>benchmarks:<BR>  PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking: MAP<BR>","<BR>task: Activity localization: Weakly Supervised Action Localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP-at-0.5<BR>  THUMOS 2014 - Weakly Supervised Action Localization benchmarking: mAP@0.1:0.7<BR>","<BR>task: Other vision process: Scene Text Detection<BR>date: 2019-11<BR>ratio: 0.3797<BR>benchmarks:<BR>  MSRA-TD500 - Scene Text Detection benchmarking: F-Measure<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Precision<BR>  MSRA-TD500 - Scene Text Detection benchmarking: Recall<BR>","<BR>task: Other vision process: Domain Adaptation<BR>date: 2019-11<BR>ratio: 0.0848<BR>benchmarks:<BR>  ImageCLEF-DA - Domain Adaptation benchmarking: Accuracy<BR>  MNIST-to-USPS - Domain Adaptation benchmarking: Accuracy<BR>  Office-Caltech - Domain Adaptation benchmarking: Average Accuracy<BR>  Office-Home - Domain Adaptation benchmarking: Accuracy<BR>  SVHN-to-MNIST - Domain Adaptation benchmarking: Accuracy<BR>","<BR>task: Pose estimation: 6D Pose Estimation using RGBD<BR>date: 2019-11<BR>ratio: 0.8357<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD-S<BR>  YCB-Video - 6D Pose Estimation using RGBD benchmarking: Mean ADD<BR>","<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2019-11<BR>ratio: 0.6248<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APL<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APM<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: APS<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Semantic segmentation: Panoptic Segmentation<BR>date: 2019-11<BR>ratio: 0.471<BR>benchmarks:<BR>  COCO test-dev - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes test - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: PQ<BR>  Cityscapes val - Panoptic Segmentation benchmarking: mIoU<BR>  Mapillary val - Panoptic Segmentation benchmarking: PQ<BR>","<BR>task: Pose estimation: 6D Pose Estimation<BR>date: 2019-11<BR>ratio: 0.9839<BR>benchmarks:<BR>  LineMOD - 6D Pose Estimation benchmarking: Accuracy (ADD)<BR>  YCB-Video - 6D Pose Estimation benchmarking: ADDS AUC<BR>","<BR>task: Action localization: Temporal Action Localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.5<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP IOU-at-0.95<BR>  ActivityNet-1.3 - Temporal Action Localization benchmarking: mAP<BR>","<BR>task: Semantic segmentation: Retinal Vessel Segmentation<BR>date: 2019-12<BR>ratio: 0.172<BR>benchmarks:<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: AUC<BR>  CHASE_DB1 - Retinal Vessel Segmentation benchmarking: F1 score<BR>  DRIVE - Retinal Vessel Segmentation benchmarking: AUC<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3DPW - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Activity recognition: Action Recognition<BR>date: 2019-12<BR>ratio: 0.1636<BR>benchmarks:<BR>  Something-Something V1 - Action Recognition benchmarking: Top 1 Accuracy<BR>","<BR>task: Activity recognition: Skeleton Based Action Recognition<BR>date: 2019-12<BR>ratio: 0.04<BR>benchmarks:<BR>  Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking: Accuracy<BR>","<BR>task: Image generation: Image Generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  CIFAR-100 - Image Generation benchmarking: FID<BR>  FFHQ - Image Generation benchmarking: FID<BR>  LSUN Cat 256 x 256 - Image Generation benchmarking: FID<BR>  LSUN Churches 256 x 256 - Image Generation benchmarking: FID<BR>","<BR>task: Image classification: Image Classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  CIFAR-10 - Image Classification benchmarking: Percentage correct<BR>  CIFAR-100 - Image Classification benchmarking: Percentage correct<BR>  ImageNet - Image Classification benchmarking: Top 1 Accuracy<BR>  ImageNet - Image Classification benchmarking: Top 5 Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Accuracy<BR>  ImageNet ReaL - Image Classification benchmarking: Params<BR>  ObjectNet - Image Classification benchmarking: Top 5 Accuracy<BR>  VTAB-1k - Image Classification benchmarking: Top-1 Accuracy<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2019-12<BR>ratio: 0.1413<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>","<BR>task: Other video process: Video Generation<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking: Inception Score<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  UPenn Action - Pose Estimation benchmarking: Mean PCK-at-0.2<BR>","<BR>task: Image classification: Image Classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  MNIST - Image Classification benchmarking: Accuracy<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  SUN-RGBD - 3D Object Detection benchmarking: mAP-at-0.25<BR>","<BR>task: Semantic segmentation: Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.0444<BR>benchmarks:<BR>  COCO test-dev - Instance Segmentation benchmarking: APL<BR>","<BR>task: Other vision process: Unsupervised Domain Adaptation<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Duke to Market - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to Duke - Unsupervised Domain Adaptation benchmarking: rank-5<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: mAP<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-10<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-1<BR>  Market to MSMT - Unsupervised Domain Adaptation benchmarking: rank-5<BR>","<BR>task: Semantic segmentation: Real-time Instance Segmentation<BR>date: 2020-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  MSCOCO - Real-time Instance Segmentation benchmarking: mask AP<BR>","<BR>task: Activity recognition: Egocentric Activity Recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  EGTEA - Egocentric Activity Recognition benchmarking: Average Accuracy<BR>","<BR>task: Other vision process: Visual Question Answering<BR>date: 2020-02<BR>ratio: 0.5266<BR>benchmarks:<BR>  MSRVTT-QA - Visual Question Answering benchmarking: Accuracy<BR>  MSVD-QA - Visual Question Answering benchmarking: Accuracy<BR>","<BR>task: Other vision process: Scene Graph Generation<BR>date: 2020-02<BR>ratio: 0.19<BR>benchmarks:<BR>  Visual Genome - Scene Graph Generation benchmarking: Recall-at-50<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0098<BR>benchmarks:<BR>  MPI-INF-3DHP - 3D Human Pose Estimation benchmarking: AUC<BR>","<BR>task: Pose estimation: Pose Estimation<BR>date: 2020-02<BR>ratio: 0.0152<BR>benchmarks:<BR>  Leeds Sports Poses - Pose Estimation benchmarking: PCK<BR>  MPII Human Pose - Pose Estimation benchmarking: PCKh-0.5<BR>","<BR>task: Action localization: Action Segmentation<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  50 Salads - Action Segmentation benchmarking: Acc<BR>  50 Salads - Action Segmentation benchmarking: Edit<BR>  50 Salads - Action Segmentation benchmarking: F1@10%<BR>  50 Salads - Action Segmentation benchmarking: F1@25%<BR>  50 Salads - Action Segmentation benchmarking: F1@50%<BR>  Breakfast - Action Segmentation benchmarking: Acc<BR>  Breakfast - Action Segmentation benchmarking: Edit<BR>  Breakfast - Action Segmentation benchmarking: F1@10%<BR>  Breakfast - Action Segmentation benchmarking: F1@25%<BR>  Breakfast - Action Segmentation benchmarking: F1@50%<BR>  GTEA - Action Segmentation benchmarking: Acc<BR>  GTEA - Action Segmentation benchmarking: Edit<BR>  GTEA - Action Segmentation benchmarking: F1@10%<BR>  GTEA - Action Segmentation benchmarking: F1@25%<BR>  GTEA - Action Segmentation benchmarking: F1@50%<BR>","<BR>task: Semantic segmentation: 3D Semantic Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.4893<BR>benchmarks:<BR>  ScanNetV2 - 3D Semantic Instance Segmentation benchmarking: mAP-at-0.50<BR>","<BR>task: Semantic segmentation: Lesion Segmentation<BR>date: 2020-03<BR>ratio: 0.5909<BR>benchmarks:<BR>  ISIC 2018 - Lesion Segmentation benchmarking: Dice Score<BR>","<BR>task: Semantic segmenation: 3D Instance Segmentation<BR>date: 2020-03<BR>ratio: 0.8206<BR>benchmarks:<BR>  S3DIS - 3D Instance Segmentation benchmarking: mPrec<BR>  SceneNN - 3D Instance Segmentation benchmarking: mAP-at-0.5<BR>","<BR>task: Other video process: Video Frame Interpolation<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  UCF101 - Video Frame Interpolation benchmarking: PSNR<BR>  Vimeo90k - Video Frame Interpolation benchmarking: PSNR<BR>","<BR>task: Object detection: Video Object Detection<BR>date: 2020-03<BR>ratio: 0.2075<BR>benchmarks:<BR>  ImageNet VID - Video Object Detection benchmarking: MAP<BR>","<BR>task: Object detection: 3D Object Detection<BR>date: 2020-03<BR>ratio: 0.3135<BR>benchmarks:<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.25<BR>  ScanNetV2 - 3D Object Detection benchmarking: mAP-at-0.5<BR>","<BR>task: Pose estimation: 3D Human Pose Estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  Surreal - 3D Human Pose Estimation benchmarking: MPJPE<BR>","<BR>task: Semantic segmentation: Video Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.0204<BR>benchmarks:<BR>  Cityscapes val - Video Semantic Segmentation benchmarking: mIoU<BR>","<BR>task: Semantic segmentation: Semantic Segmentation<BR>date: 2020-04<BR>ratio: 0.4197<BR>benchmarks:<BR>  ADE20K - Semantic Segmentation benchmarking: Validation mIoU<BR>  ADE20K val - Semantic Segmentation benchmarking: mIoU<BR>  NYU Depth v2 - Semantic Segmentation benchmarking: Mean IoU<BR>"],"marker":{"color":[0.5546,0.0189,0.1073,1.0,1.0,0.0397,0.0553,0.021,0.8122,0.4737,0.289,0.1446,0.9752,0.2834,1.0,0.2375,0.2513,0.2406,0.1392,0.0877,0.8858,0.0451,0.3958,0.0134,0.3753,0.2327,0.0741,0.426,0.1007,0.2878,0.8426,0.3991,0.2321,0.1675,0.1142,0.4459,0.2457,0.4321,0.5,0.117,0.4701,0.0854,1.0,0.1888,0.3222,0.0331,0.15,0.1785,0.4339,0.1637,0.0779,0.7477,0.4051,1.0,0.9509,0.5016,1.0,0.0707,0.6524,0.0478,0.056,0.2757,0.0258,1.0,0.3359,0.2119,0.4252,0.5954,0.7322,0.1471,0.4868,0.4832,0.4271,0.6898,0.0092,0.8889,0.3005,0.0418,0.1069,0.2762,0.072,0.2999,0.0741,0.572,0.3146,0.061,0.3235,0.3625,0.0241,0.8371,1.0,0.1233,0.2999,0.6547,1.0,0.0251,0.2523,0.5511,0.1736,0.0915,0.0144,0.3082,0.4115,0.2784,0.0093,0.0142,0.0606,0.0684,0.3326,0.3571,0.0,0.2824,0.2492,0.2441,0.096,0.3399,0.2329,0.2215,0.5084,0.6006,0.674,0.2304,0.6019,0.3137,0.1022,0.5259,0.4898,0.1001,0.5647,0.1125,0.1929,0.0388,0.1288,0.1429,0.9756,0.1328,0.2025,0.6995,0.0692,0.0541,0.9524,0.5645,0.1574,0.3612,0.1565,0.2235,0.0303,0.6528,0.5668,0.4081,0.2484,0.1096,0.21,0.2205,0.1193,0.5877,0.2106,0.0331,0.4378,0.2112,1.0,0.2041,0.1026,0.3846,0.7172,0.1968,0.103,0.1629,0.2194,0.1332,0.8514,0.1767,0.4855,1.0,0.4565,0.1679,0.3172,0.0758,0.4474,0.4983,0.528,0.2366,0.7853,0.4284,1.0,0.8352,0.2561,0.0559,0.0607,0.2867,0.6277,0.2514,0.6175,0.3412,0.1244,0.4844,0.5909,0.4142,0.4898,0.6641,0.1161,0.7104,1.0,0.02,0.6614,0.0095,0.0387,0.4581,0.2874,0.1758,1.0,0.3268,0.6997,0.0363,0.0645,0.2688,0.5913,0.0083,1.0,0.219,0.0624,0.88,0.3456,0.0951,0.4921,0.1108,0.6932,0.5449,0.8514,0.0551,0.1598,0.474,0.3904,0.0103,1.0,1.0,0.0758,0.0809,1.0,0.6429,1.0,0.0909,0.9952,0.4958,0.2127,0.9847,0.4337,0.2492,0.2941,0.7097,0.5297,0.656,0.2656,0.6273,0.435,0.2438,0.619,0.5452,0.6998,0.1491,0.789,0.1976,0.5062,0.6503,0.3521,0.383,0.3953,0.1702,0.6857,0.069,0.2578,0.2407,0.0546,0.0332,1.0,1.0,0.5028,0.5,0.1898,0.7094,0.0238,0.1097,0.5417,0.0083,0.3524,0.613,0.3532,0.0247,0.2406,0.046,0.0583,0.5145,0.4342,0.3197,0.0837,0.3333,0.3059,0.3661,0.6486,0.2233,0.4232,0.1321,0.6229,0.0351,0.628,0.4938,0.2543,0.0865,0.3996,0.6875,0.5545,1.0,0.2155,0.9199,0.3005,0.9695,0.2075,0.1202,0.6418,0.0929,1.0,0.2249,0.1587,0.0083,0.1099,0.2131,0.1771,0.1351,0.387,0.5083,0.1184,0.0203,1.0,1.0,0.3293,0.4588,0.2555,0.1925,0.1608,0.6667,1.0,0.3804,0.5245,0.0647,0.211,1.0,0.0985,0.0067,0.5604,0.5045,0.6147,0.3034,0.1429,0.2694,0.6801,0.028,0.2865,0.6543,1.0,0.714,0.2306,0.6039,0.4536,0.6951,0.145,0.0149,0.1992,0.1822,0.4686,1.0,0.7711,0.7435,0.2077,1.0,0.5515,0.1235,0.1128,0.3129,0.9967,0.7567,0.3698,1.0,1.0,0.297,0.5603,0.2185,0.2047,0.3576,0.5207,0.4007,0.9856,0.1059,0.1448,0.5625,0.1158,0.7291,0.4436,1.0,0.5357,1.0,0.3854,1.0,0.5,0.0631,0.6698,0.6807,0.4543,0.0321,0.0244,0.3547,0.6183,0.1346,0.0378,0.086,0.1442,0.1071,0.2687,0.8995,0.1255,0.0079,0.4091,0.0658,0.2577,0.0225,1.0,0.1139,0.2609,0.2227,0.0087,0.7924,0.0808,0.34,0.2295,0.6415,0.5661,0.4542,0.3352,0.5789,1.0,1.0,0.2788,0.7527,0.6675,0.0755,0.3967,0.3724,0.7361,0.1431,1.0,1.0,0.2526,0.0286,0.5387,0.0423,1.0,0.0684,0.1735,0.0597,0.1767,0.4718,0.027,0.5811,1.0,0.4227,0.2451,1.0,0.549,0.5,0.8571,0.2902,0.3195,0.7779,0.1675,0.6789,0.5095,0.221,0.5107,0.2741,0.3083,0.1343,0.6087,0.0323,1.0,0.4,0.1641,0.3272,0.0894,0.3127,0.1323,0.3141,0.371,0.1086,0.564,0.1212,0.2126,1.0,0.7353,0.0882,0.068,1.0,0.036,0.1,0.0018,0.1324,0.0183,0.4587,0.069,1.0,1.0,0.1426,0.6096,1.0,1.0,0.098,1.0,1.0,0.3109,0.3213,0.1256,0.6399,0.2573,0.2896,0.4466,0.5464,0.9015,0.1348,0.6033,0.2881,0.155,0.3733,0.6718,1.0,0.1081,0.162,1.0,0.6555,0.1943,1.0,0.0663,0.4213,0.1123,0.3059,1.0,0.4321,0.6667,0.6714,0.3147,0.3574,0.339,0.3247,0.7679,0.1341,0.0989,0.7784,0.0382,0.0519,1.0,0.0144,1.0,0.7002,0.4235,1.0,0.2258,0.7067,0.0083,0.4063,0.7854,1.0,0.4905,0.1623,0.0225,0.2305,0.2174,0.2194,1.0,1.0,0.0589,0.0493,0.0985,0.0932,0.2823,0.066,0.6079,0.9629,0.1399,0.4736,0.0171,0.7183,0.9021,0.1952,0.0318,0.5126,0.0989,0.2932,0.1509,0.066,0.5481,0.2415,0.7593,0.4898,1.0,0.2647,0.835,0.6173,0.3796,0.3316,0.3395,1.0,0.3549,0.1793,0.1469,0.0324,0.0978,0.7344,1.0,0.4583,0.3309,0.6596,1.0,1.0,0.4622,0.2284,1.0,1.0,0.1982,0.3737,0.3,0.26,0.1053,0.071,0.0378,0.2029,0.046,0.1412,0.4198,0.0137,0.1818,0.9068,0.1111,0.5071,1.0,0.2772,0.0466,0.0387,0.9715,0.2791,0.0533,0.0205,0.1955,0.5032,0.5483,1.0,0.2369,0.0114,0.2438,1.0,1.0,0.7974,0.0635,0.057,0.0283,0.3356,0.3797,0.0848,0.8357,0.6248,0.471,0.9839,0.0838,0.172,0.0932,0.1636,0.04,0.8135,0.2424,0.1413,0.7122,1.0,0.25,1.0,0.0444,0.4159,0.2233,0.4211,0.5266,0.19,0.0098,0.0152,0.6966,0.4893,0.5909,0.8206,0.8021,0.2075,0.3135,0.4396,0.0204,0.4197],"colorbar":{"len":500,"lenmode":"pixels","thickness":10,"title":{"text":"ratio"}},"colorscale":[[0.0,"rgb(255,255,229)"],[0.125,"rgb(247,252,185)"],[0.25,"rgb(217,240,163)"],[0.375,"rgb(173,221,142)"],[0.5,"rgb(120,198,121)"],[0.625,"rgb(65,171,93)"],[0.75,"rgb(35,132,67)"],[0.875,"rgb(0,104,55)"],[1.0,"rgb(0,69,41)"]],"opacity":0.7,"showscale":true,"size":19,"symbol":"circle","line":{"color":"black","width":1}},"mode":"markers","x":["2012-08","2012-12","2013-02","2013-02","2013-07","2013-11","2013-12","2014-04","2014-06","2014-06","2014-06","2014-09","2014-09","2014-11","2014-12","2014-12","2014-12","2014-12","2014-12","2015-02","2015-02","2015-02","2015-02","2015-02","2015-03","2015-03","2015-03","2015-04","2015-04","2015-04","2015-05","2015-05","2015-05","2015-05","2015-05","2015-06","2015-06","2015-08","2015-09","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-12","2015-12","2015-12","2015-12","2015-12","2015-12","2016-01","2016-01","2016-01","2016-01","2016-02","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-04","2016-04","2016-04","2016-04","2016-04","2016-04","2016-04","2016-05","2016-05","2016-05","2016-05","2016-06","2016-06","2016-06","2016-06","2016-06","2016-06","2016-06","2016-07","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-09","2016-09","2016-09","2016-09","2016-09","2016-10","2016-10","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2017-01","2017-01","2017-02","2017-02","2017-02","2017-02","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-08","2018-08","2018-08","2018-08","2018-08","2018-08","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-09","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-12","2019-12","2019-12","2019-12","2019-12","2019-12","2019-12","2019-12","2020-01","2020-01","2020-01","2020-01","2020-01","2020-01","2020-02","2020-02","2020-02","2020-02","2020-02","2020-03","2020-03","2020-03","2020-03","2020-03","2020-03","2020-03","2020-04","2020-04","2020-04"],"y":["Other image process: Image Clustering","Image classification: Image Classification","Image classification: Image Classification","Activity recognition: Skeleton Based Action Recognition","Facial recognition and modelling: Facial Expression Recognition","Image classification: Image Classification","Image classification: Image Classification","Image classification: Image Classification","Activity recognition: Action Recognition","Facial recognition and modelling: Face Verification","Image classification: Image Classification","Image classification: Image Classification","Other vision process: Domain Adaptation","Semantic segmentation: Semantic Segmentation","Object detection: Pedestrian Detection","Activity recognition: Action Recognition","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Face Verification","Image classification: Image Classification","Semantic segmentation: Semantic Segmentation","Other vision process: Unsupervised Domain Adaptation","Facial recognition and modelling: Face Verification","Other vision process: Domain Adaptation","Image classification: Image Classification","Activity recognition: Action Recognition","Facial recognition and modelling: Face Verification","Semantic segmentation: Semantic Segmentation","Other image process: Image Retrieval","Other vision process: Scene Text Detection","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Multi-tissue Nucleus Segmentation","Other vision process: Domain Adaptation","Other vision process: Curved Text Detection","Activity recognition: Action Recognition","Other vision process: Unsupervised Domain Adaptation","Image classification: Image Classification","Object detection: Object Detection","Facial recognition and modelling: Face Verification","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Face Identification","Image classification: Sequential Image Classification","Facial recognition and modelling: Face Verification","Semantic segmentation: Medical Image Segmentation","Other image process: Image Retrieval","Facial recognition and modelling: Facial Landmark Detection","Pose estimation: Pose Estimation","Other image process: Image Clustering","Semantic segmentation: Semantic Segmentation","Object detection: Weakly Supervised Object Detection","Image classification: Image Classification","Image classification: Image Classification","Other vision process: Crowd Counting","Image classification: Satellite Image Classification","Activity recognition: Action Recognition","Image classification: Retinal OCT Disease Classification","Object detection: Object Detection","Image generation: Image Generation","Activity recognition: Action Recognition","Pose estimation: Pose Estimation","Action localization: Temporal Action Localization","Image classification: Image Classification","Other vision process: Visual Question Answering","Image classification: Image Classification","Activity recognition: Multimodal Activity Recognition","Object detection: Weakly Supervised Object Detection","Facial recognition and modelling: Face Verification","Image classification: Sequential Image Classification","Facial recognition and modelling: Face Detection","Pose estimation: Pose Estimation","Semantic segmentation: Semantic Segmentation","Other image process: Aesthetics Quality Assessment","Other image process: Image Retrieval","Activity recognition: Skeleton Based Action Recognition","Other image process: Image Clustering","Activity recognition: Action Recognition","Other vision process: Object Counting","Other vision process: Scene Text Detection","Semantic segmentation: Semantic Segmentation","Image classification: Image Classification","Other vision process: Visual Question Answering","Other vision process: Domain Adaptation","Other vision process: Visual Question Answering","Semantic segmentation: Semantic Segmentation","Image generation: Conditional Image Generation","Object detection: RGB Salient Object Detection","Activity recognition: Skeleton Based Action Recognition","Object detection: 3D Object Detection","Image generation: Image Generation","Other vision process: Monocular Depth Estimation","Activity recognition: Multimodal Activity Recognition","Other vision process: Horizon Line Estimation","Pose estimation: Keypoint Detection","Object detection: Birds Eye View Object Detection","Other vision process: Domain Adaptation","Object recognition: Pedestrian Attribute Recognition","Image classification: Image Classification","Other vision process: Crowd Counting","Other image process: Grayscale Image Denoising","Other image process: Image Retrieval","Activity recognition: Action Recognition","Object detection: Weakly Supervised Object Detection","Activity recognition: Skeleton Based Action Recognition","Other vision process: Object Counting","Action localization: Temporal Action Localization","Pose estimation: Pose Estimation","Image classification: Image Classification","Image generation: Conditional Image Generation","Image classification: Image Classification","Semantic segmentation: Nuclear Segmentation","Action localization: Action Segmentation","Object detection: RGB Salient Object Detection","Other vision process: Metric Learning","Other video process: Video Generation","Other image process: Image Retrieval","Object tracking: Visual Object Tracking","Activity recognition: Skeleton Based Action Recognition","Semantic segmentation: Instance Segmentation","Object detection: Weakly Supervised Object Detection","Other vision process: Visual Question Answering","Object detection: Birds Eye View Object Detection","Pose estimation: Keypoint Detection","Semantic segmentation: Semantic Segmentation","Pose estimation: Keypoint Detection","Semantic segmentation: Semantic Segmentation","Other video process: Video Retrieval","Pose estimation: Pose Estimation","Semantic segmentation: Video Semantic Segmentation","Other image process: Image Retrieval","Other 3D task: 3D Object Reconstruction","Object detection: Dense Object Detection","Semantic segmentation: 3D Part Segmentation","Object detection: Object Detection","Image generation: Conditional Image Generation","Activity recognition: Action Classification","Other 3D task: 3D Reconstruction","Pose estimation: Keypoint Detection","Pose estimation: Pose Estimation","Image classification: Unsupervised Image Classification","Pose estimation: Pose Estimation","Image generation: Image Generation","Object detection: Pedestrian Detection","Other image process: Image Clustering","Semantic segmentation: Multi-tissue Nucleus Segmentation","Other vision process: Scene Text Detection","Object detection: Object Detection","Pose estimation: Keypoint Detection","Image generation: Conditional Image Generation","Semantic segmentation: Nuclear Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Semantic Segmentation","Image generation: Image Generation","Action localization: Temporal Action Localization","Activity recognition: Action Recognition","Activity detection: Action Detection","Facial recognition and modelling: Face Verification","Activity recognition: Skeleton Based Action Recognition","Other vision process: Scene Text Detection","Pose estimation: Pose Estimation","Other image process: Image Clustering","Semantic segmentation: Semantic Segmentation","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Semantic segmentation: Instance Segmentation","Other vision process: Visual Question Answering","Other vision process: Formation Energy","Other vision process: Domain Adaptation","Other vision process: Monocular Depth Estimation","Object tracking: Visual Object Tracking","Activity recognition: Multimodal Activity Recognition","Object detection: Weakly Supervised Object Detection","Facial recognition and modelling: Face Identification","Other 3D task: 3D Point Cloud Classification","Activity recognition: Skeleton Based Action Recognition","Image classification: Document Image Classification","Other image process: Color Image Denoising","Other image process: Grayscale Image Denoising","Facial recognition and modelling: Face Verification","Activity recognition: Action Recognition","Object detection: RGB Salient Object Detection","Other image process: Aesthetics Quality Assessment","Activity recognition: Action Classification","Other vision process: Domain Adaptation","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Other vision process: Denoising","Other vision process: Visual Question Answering","Pose estimation: 3D Human Pose Estimation","Gesture recognition: Hand Gesture Recognition","Action localization: Temporal Action Localization","Activity recognition: Action Recognition","Pose estimation: Pose Estimation","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Human Interaction Recognition","Image generation: Image Generation","Object tracking: Visual Object Tracking","Other vision process: Metric Learning","Semantic segmentation: 3D Semantic Segmentation","Other 3D task: 3D Point Cloud Classification","Semantic segmentation: 3D Part Segmentation","Other vision process: Formation Energy","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Object detection: Weakly Supervised Object Detection","Semantic segmentation: Semantic Segmentation","Other image process: Image Reconstruction","Object detection: Object Detection","Object detection: Weakly Supervised Object Detection","Pose estimation: Pose Estimation","Object detection: RGB Salient Object Detection","Other video process: Video Generation","Other video process: Video Retrieval","Other vision process: Visual Question Answering","Other vision process: Object Counting","Pose estimation: Hand Pose Estimation","Other vision process: Scene Text Detection","Facial recognition and modelling: Face Alignment","Image classification: Image Classification","Other vision process: Domain Generalization","Facial recognition and modelling: Face Detection","Activity recognition: Skeleton Based Action Recognition","Pose estimation: Pose Estimation","Pose estimation: Hand Pose Estimation","Image classification: Image Classification","Object detection: Object Detection","Facial recognition and modelling: Facial Expression Recognition","Other vision process: Visual Question Answering","Object detection: Weakly Supervised Object Detection","Semantic segmentation: Human Part Segmentation","Other vision process: Scene Text Detection","Object detection: Dense Object Detection","Object detection: RGB Salient Object Detection","Other image process: Image Clustering","Facial recognition and modelling: Face Detection","Other vision process: Formation Energy","Other vision process: Scene Text Detection","Other vision process: Visual Dialog","Image generation: Image Generation","Pose estimation: 3D Human Pose Estimation","Semantic segmentation: Pancreas Segmentation","Image generation: Conditional Image Generation","Image classification: Image Classification","Other image process: Color Image Denoising","Object tracking: Visual Object Tracking","Object detection: Lane Detection","Image classification: Sequential Image Classification","Object detection: RGB Salient Object Detection","Image classification: Image Classification","Semantic segmentation: 3D Semantic Segmentation","Facial recognition and modelling: Face Verification","Other image process: Image Clustering","Semantic segmentation: Semantic Segmentation","Other image process: Grayscale Image Denoising","Other vision process: Domain Generalization","Image generation: Image Generation","Pose estimation: Head Pose Estimation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Face Alignment","Activity recognition: Action Recognition","Object detection: Weakly Supervised Object Detection","Object detection: 3D Object Detection","Object detection: Birds Eye View Object Detection","Semantic segmentation: Retinal Vessel Segmentation","Other 3D task: 3D Shape Classification","Object detection: Object Detection","Semantic segmentation: Skin Cancer Segmentation","Activity recognition: Action Classification","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: Keypoint Detection","Pose estimation: Pose Estimation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: Instance Segmentation","Other vision process: Domain Adaptation","Other vision process: Visual Dialog","Image-to-image translation: Fundus to Angiography Generation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Semantic Segmentation","Pose estimation: Pose Estimation","Pose estimation: Hand Pose Estimation","Activity detection: Action Detection","Image generation: Pose Transfer","Pose tracking: Pose Tracking","Object detection: Birds Eye View Object Detection","Activity recognition: Action Classification","Object detection: 3D Object Detection","Object detection: Lane Detection","Image classification: Image Classification","Object detection: Object Detection","Other vision process: Domain Adaptation","Activity recognition: Action Recognition","Facial recognition and modelling: Face Verification","Activity localization: Weakly Supervised Action Localization","Other image process: Image Retrieval","Other 3D task: 3D Point Cloud Classification","Image classification: Document Image Classification","Activity recognition: Skeleton Based Action Recognition","Other image process: Image Retrieval","Activity recognition: Action Recognition","Activity recognition: Multimodal Activity Recognition","Other vision process: Scene Text Detection","Image classification: Retinal OCT Disease Classification","Facial recognition and modelling: Face Identification","Semantic segmentation: 3D Part Segmentation","Facial recognition and modelling: Face Verification","Object detection: Weakly Supervised Object Detection","Object tracking: Visual Object Tracking","Image classification: Image Classification","Other 3D task: 3D Object Reconstruction","Semantic segmentation: Skin Cancer Segmentation","Semantic segmentation: Semantic Segmentation","Image generation: Conditional Image Generation","Other vision process: Scene Text Detection","Semantic segmentation: Retinal Vessel Segmentation","Image generation: Image Generation","Other image process: Color Image Denoising","Pose tracking: Pose Tracking","Object tracking: Multiple Object Tracking","Image classification: Unsupervised Image Classification","Activity recognition: Skeleton Based Action Recognition","Object detection: 3D Object Detection","Object tracking: Visual Object Tracking","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation using RGBD","Semantic segmentation: Scene Segmentation","Other vision process: Monocular Depth Estimation","Other vision process: Visual Question Answering","Pose estimation: Pose Estimation","Other image process: Image Retrieval","Semantic segmentation: Instance Segmentation","Semantic segmentation: Semantic Segmentation","Other 3D task: 3D Point Cloud Classification","Activity detection: Action Detection","Facial recognition and modelling: Face Alignment","Image classification: Sequential Image Classification","Facial recognition and modelling: Face Verification","Object detection: Weakly Supervised Object Detection","Facial recognition and modelling: Face Identification","Image generation: Image Generation","Facial recognition and modelling: Facial Landmark Detection","Object detection: Object Detection","Pose estimation: Keypoint Detection","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Other vision process: Metric Learning","Semantic segmentation: Pancreas Segmentation","Action localization: Temporal Action Localization","Activity recognition: Skeleton Based Action Recognition","Pose estimation: Pose Estimation","Other 3D task: 3D Shape Classification","Gesture recognition: Hand Gesture Recognition","Other vision process: Scene Text Detection","Object detection: Weakly Supervised Object Detection","Pose estimation: 3D Human Pose Estimation","Other image process: Image Clustering","Other video process: Video Retrieval","Semantic segmentation: Semantic Segmentation","Other image process: Image Retrieval","Facial recognition and modelling: Face Alignment","Pose tracking: Pose Tracking","Pose estimation: Pose Estimation","Image generation: Conditional Image Generation","Activity recognition: Skeleton Based Action Recognition","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Alignment","Semantic segmentation: Human Part Segmentation","Other image process: Grayscale Image Denoising","Other vision process: Monocular Depth Estimation","Object detection: Object Detection","Image classification: Image Classification","Other vision process: Visual Question Answering","Activity recognition: Action Recognition","Object detection: Weakly Supervised Object Detection","Semantic segmentation: Retinal Vessel Segmentation","Object recognition: Traffic Sign Recognition","Other vision process: Monocular Depth Estimation","Activity localization: Temporal Action Proposal Generation","Object tracking: Visual Object Tracking","Object detection: Lane Detection","Activity recognition: Skeleton Based Action Recognition","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Other video process: Video Retrieval","Pose estimation: 3D Human Pose Estimation","Emotion recognition: Emotion Recognition in Conversation","Other vision process: Scene Text Detection","Other vision process: Multivariate Time Series Imputation","Other vision process: Scene Graph Generation","Activity recognition: Action Classification","Action localization: Temporal Action Localization","Action localization: Action Segmentation","Other image process: Grayscale Image Denoising","Object detection: RGB Salient Object Detection","Other vision process: Formation Energy","Object detection: Weakly Supervised Object Detection","Activity recognition: Action Recognition","Other vision process: Scene Text Detection","Semantic segmentation: 3D Semantic Segmentation","Other vision process: Domain Adaptation","Other vision process: Denoising","Image generation: Image Generation","Activity localization: Weakly Supervised Action Localization","Activity recognition: Action Classification","Object detection: Pedestrian Detection","Semantic segmentation: Medical Image Segmentation","Other image process: Image Clustering","Image classification: Hyperspectral Image Classification","Other image process: Color Image Denoising","Image classification: Image Classification","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Other vision process: Depth Completion","Other vision process: Scene Graph Generation","Other 3D task: 3D Reconstruction","Other video process: Video Retrieval","Image generation: Conditional Image Generation","Image generation: Image Generation","Facial recognition and modelling: Face Detection","Semantic segmentation: 3D Semantic Segmentation","Other vision process: Visual Dialog","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Human Part Segmentation","Facial recognition and modelling: Face Verification","Semantic segmentation: Nuclear Segmentation","Facial recognition and modelling: Face Detection","Semantic segmentation: Lesion Segmentation","Other image process: Aesthetics Quality Assessment","Emotion recognition: Emotion Recognition in Conversation","Activity recognition: Action Classification","Image classification: Sequential Image Classification","Semantic segmentation: Retinal Vessel Segmentation","Other video process: Video Frame Interpolation","Other image process: Image Clustering","Activity recognition: Action Recognition","Activity recognition: Action Classification","Image classification: Image Classification","Other vision process: Domain Adaptation","Object detection: Object Detection","Object detection: Video Object Detection","Emotion recognition: Emotion Recognition in Conversation","Semantic segmentation: Human Part Segmentation","Other image process: Image Retrieval","Activity recognition: Egocentric Activity Recognition","Other vision process: Video Prediction","Other 3D task: 3D Reconstruction","Activity recognition: Action Recognition","Other 3D task: 3D Room Layouts From A Single RGB Panorama","Other vision process: Unsupervised Domain Adaptation","Object detection: Weakly Supervised Object Detection","Activity recognition: Group Activity Recognition","Activity recognition: Human Interaction Recognition","Object tracking: Visual Object Tracking","Activity localization: Temporal Action Proposal Generation","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Image generation: Image Generation","Other vision process: Scene Text Detection","Activity recognition: Skeleton Based Action Recognition","Other image process: Image Clustering","Object tracking: Multiple Object Tracking","Pose estimation: Pose Estimation","Other vision process: Depth Completion","Object detection: 3D Object Detection","Pose estimation: Pose Estimation","Other vision process: Monocular Depth Estimation","Other image process: Image Clustering","Other 3D task: 3D Point Cloud Classification","Semantic segmentation: Panoptic Segmentation","Other vision process: Multivariate Time Series Imputation","Object detection: Object Detection","Semantic segmentation: Semantic Segmentation","Other vision process: Unsupervised Domain Adaptation","Pose estimation: 6D Pose Estimation using RGB","Activity recognition: Action Recognition","Activity recognition: Skeleton Based Action Recognition","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Face Identification","Other vision process: Scene Graph Generation","Object tracking: Visual Object Tracking","Gesture recognition: Hand Gesture Recognition","Activity recognition: Egocentric Activity Recognition","Pose estimation: Head Pose Estimation","Semantic segmentation: 3D Semantic Instance Segmentation","Pose estimation: Keypoint Detection","Object detection: Birds Eye View Object Detection","Activity recognition: Action Classification","Image generation: Image Generation","Pose estimation: 6D Pose Estimation","Activity recognition: Multimodal Activity Recognition","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: Keypoint Detection","Pose estimation: Head Pose Estimation","Pose estimation: Pose Estimation","Other vision process: Domain Adaptation","Other image process: Image Clustering","Image classification: Image Classification","Other 3D task: 3D Room Layouts From A Single RGB Panorama","Semantic segmentation: Semantic Segmentation","Gesture recognition: Hand Gesture Recognition","Other 3D task: 3D Object Reconstruction","Semantic segmentation: Panoptic Segmentation","Activity recognition: Action Recognition","Activity recognition: Skeleton Based Action Recognition","Object detection: Object Detection","Pose estimation: Pose Estimation","Other image process: Image Retrieval","Pose estimation: Keypoint Detection","Semantic segmentation: Instance Segmentation","Facial recognition and modelling: Face Alignment","Pose estimation: 6D Pose Estimation using RGB","Pose tracking: Pose Tracking","Other vision process: Visual Question Answering","Other vision process: Depth Completion","Image classification: Hyperspectral Image Classification","Facial recognition and modelling: Facial Expression Recognition","Other vision process: Visual Dialog","Facial recognition and modelling: Facial Landmark Detection","Other vision process: Domain Generalization","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Semantic segmentation: Semantic Segmentation","Image generation: Image Generation","Semantic segmenation: 3D Instance Segmentation","Other vision process: Scene Text Detection","Action localization: Action Segmentation","Facial recognition and modelling: Face Verification","Other image process: Image Retrieval","Object detection: 3D Object Detection","Other image process: Image Reconstruction","Other vision process: Domain Adaptation","Other vision process: Monocular Depth Estimation","Action localization: Temporal Action Localization","Other video process: Video Frame Interpolation","Activity recognition: Group Activity Recognition","Activity recognition: Skeleton Based Action Recognition","Action localization: Temporal Action Localization","Other image process: Image Clustering","Facial recognition and modelling: Face Verification","Other image process: Color Image Denoising","Other 3D task: 3D Point Cloud Classification","Activity recognition: Action Classification","Emotion recognition: Emotion Recognition in Conversation","Semantic segmentation: Real-time Instance Segmentation","Object detection: Dense Object Detection","Image generation: Pose Transfer","Activity recognition: Action Recognition","Other vision process: Visual Dialog","Semantic segmentation: 3D Part Segmentation","Image classification: Image Classification","Activity detection: Action Detection","Semantic segmentation: 3D Semantic Segmentation","Object recognition: Traffic Sign Recognition","Object detection: Weakly Supervised Object Detection","Other vision process: Visual Question Answering","Object detection: 3D Object Detection","Other vision process: Scene Text Detection","Other vision process: Object Counting","Other vision process: Curved Text Detection","Semantic segmentation: Semantic Segmentation","Other vision process: Denoising","Object detection: RGB Salient Object Detection","Facial recognition and modelling: Face Alignment","Image generation: Image Generation","Other vision process: Video Prediction","Activity recognition: Action Recognition","Other vision process: Formation Energy","Activity localization: Weakly Supervised Action Localization","Other vision process: Visual Question Answering","Other vision process: Depth Completion","Other vision process: Domain Generalization","Facial recognition and modelling: Facial Expression Recognition","Pose tracking: Pose Tracking","Other vision process: Domain Adaptation","Semantic segmentation: Panoptic Segmentation","Other vision process: Horizon Line Estimation","Activity recognition: Egocentric Activity Recognition","Image classification: Image Classification","Activity recognition: Action Classification","Image classification: Image Classification","Semantic segmenation: 3D Instance Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Electron Microscopy Image Segmentation","Semantic segmentation: Brain Tumor Segmentation","Activity localization: Weakly Supervised Action Localization","Semantic segmentation: Semantic Segmentation","Action localization: Temporal Action Localization","Facial recognition and modelling: Face Alignment","Object tracking: Visual Object Tracking","Other vision process: Visual Question Answering","Other video process: Video Retrieval","Other vision process: Domain Adaptation","Pose estimation: Head Pose Estimation","Activity recognition: Action Classification","Activity recognition: Skeleton Based Action Recognition","Object detection: RGB Salient Object Detection","Other vision process: Crowd Counting","Object detection: Object Detection","Activity recognition: Action Recognition","Object detection: 3D Object Detection","Image generation: Image Generation","Other vision process: Monocular Depth Estimation","Object detection: Video Object Detection","Activity recognition: Action Recognition","Other vision process: Visual Question Answering","Activity localization: Temporal Action Proposal Generation","Image-to-image translation: Fundus to Angiography Generation","Other video process: Video Retrieval","Semantic segmentation: Retinal Vessel Segmentation","Activity recognition: Skeleton Based Action Recognition","Object tracking: Visual Object Tracking","Object detection: Birds Eye View Object Detection","Action localization: Temporal Action Localization","Image classification: Image Classification","Activity recognition: Action Recognition","Semantic segmentation: Electron Microscopy Image Segmentation","Other vision process: Metric Learning","Object detection: Object Detection","Other vision process: Crowd Counting","Facial recognition and modelling: Face Alignment","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Lung Nodule Segmentation","Other vision process: Domain Adaptation","Object detection: Lane Detection","Emotion recognition: Emotion Recognition in Conversation","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Semantic segmentation: Scene Segmentation","Activity recognition: Egocentric Activity Recognition","Other vision process: Visual Question Answering","Semantic segmentation: Instance Segmentation","Image classification: Document Image Classification","Semantic segmentation: Brain Tumor Segmentation","Activity localization: Weakly Supervised Action Localization","Facial recognition and modelling: Face Verification","Image generation: Image Generation","Action localization: Temporal Action Localization","Object detection: Object Detection","Semantic segmentation: Instance Segmentation","Object tracking: Multiple Object Tracking","Activity recognition: Skeleton Based Action Recognition","Emotion recognition: Emotion Recognition in Conversation","Other image process: Image Retrieval","Semantic segmentation: Panoptic Segmentation","Pose estimation: 6D Pose Estimation using RGB","Semantic segmentation: 3D Part Segmentation","Pose estimation: 3D Human Pose Estimation","Other vision process: Visual Question Answering","Other vision process: Domain Adaptation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: Human Part Segmentation","Object detection: Object Detection","Other vision process: Scene Text Detection","Other image process: Grayscale Image Denoising","Object detection: Birds Eye View Object Detection","Object detection: Weakly Supervised Object Detection","Pose estimation: Pose Estimation","Image classification: Image Classification","Semantic segmentation: Semantic Segmentation","Facial recognition and modelling: Face Verification","Object recognition: Pedestrian Attribute Recognition","Semantic segmentation: Semantic Segmentation","Activity recognition: Skeleton Based Action Recognition","Semantic segmentation: Instance Segmentation","Facial recognition and modelling: Facial Expression Recognition","Other vision process: Unsupervised Domain Adaptation","Image classification: Satellite Image Classification","Object detection: Object Detection","Image generation: Image Generation","Object detection: Weakly Supervised Object Detection","Activity localization: Weakly Supervised Action Localization","Other vision process: Scene Text Detection","Other vision process: Domain Adaptation","Pose estimation: 6D Pose Estimation using RGBD","Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Panoptic Segmentation","Pose estimation: 6D Pose Estimation","Action localization: Temporal Action Localization","Semantic segmentation: Retinal Vessel Segmentation","Pose estimation: 3D Human Pose Estimation","Activity recognition: Action Recognition","Activity recognition: Skeleton Based Action Recognition","Image generation: Image Generation","Image classification: Image Classification","Semantic segmenation: 3D Instance Segmentation","Other video process: Video Generation","Pose estimation: Pose Estimation","Image classification: Image Classification","Object detection: 3D Object Detection","Semantic segmentation: Instance Segmentation","Other vision process: Unsupervised Domain Adaptation","Semantic segmentation: Real-time Instance Segmentation","Activity recognition: Egocentric Activity Recognition","Other vision process: Visual Question Answering","Other vision process: Scene Graph Generation","Pose estimation: 3D Human Pose Estimation","Pose estimation: Pose Estimation","Action localization: Action Segmentation","Semantic segmentation: 3D Semantic Instance Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmenation: 3D Instance Segmentation","Other video process: Video Frame Interpolation","Object detection: Video Object Detection","Object detection: 3D Object Detection","Pose estimation: 3D Human Pose Estimation","Semantic segmentation: Video Semantic Segmentation","Semantic segmentation: Semantic Segmentation"],"type":"scatter","line":{"color":"black","width":0}}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"},"showgrid":true,"gridcolor":"lightBlue","tickmode":"auto"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{},"categoryorder":"array","categoryarray":["Semantic segmentation: Skin Cancer Segmentation","Semantic segmentation: Real-time Instance Segmentation","Semantic segmentation: Panoptic Segmentation","Semantic segmentation: Pancreas Segmentation","Semantic segmentation: Nuclear Segmentation","Semantic segmentation: Multi-tissue Nucleus Segmentation","Semantic segmentation: Medical Image Segmentation","Semantic segmentation: Lung Nodule Segmentation","Semantic segmentation: Retinal Vessel Segmentation","Semantic segmentation: Scene Segmentation","Semantic segmentation: Lesion Segmentation","Semantic segmentation: Semantic Segmentation","Semantic segmentation: 3D Part Segmentation","Semantic segmentation: 3D Semantic Instance Segmentation","Semantic segmentation: 3D Semantic Segmentation","Semantic segmentation: Brain Tumor Segmentation","Semantic segmentation: Human Part Segmentation","Semantic segmentation: Electron Microscopy Image Segmentation","Semantic segmentation: Instance Segmentation","Semantic segmentation: Video Semantic Segmentation","Semantic segmenation: 3D Instance Segmentation","Pose tracking: Pose Tracking","Pose estimation: 6D Pose Estimation using RGBD","Pose estimation: 6D Pose Estimation using RGB","Pose estimation: 6D Pose Estimation","Pose estimation: 3D Human Pose Estimation","Pose estimation: Hand Pose Estimation","Pose estimation: Keypoint Detection","Pose estimation: Head Pose Estimation","Pose estimation: Pose Estimation","Pose estimation: Weakly-supervised 3D Human Pose Estimation","Other vision process: Denoising","Other vision process: Object Counting","Other vision process: Horizon Line Estimation","Other vision process: Unsupervised Domain Adaptation","Other vision process: Visual Dialog","Other vision process: Video Prediction","Other vision process: Metric Learning","Other vision process: Monocular Depth Estimation","Other vision process: Crowd Counting","Other vision process: Curved Text Detection","Other vision process: Multivariate Time Series Imputation","Other vision process: Depth Completion","Other vision process: Scene Graph Generation","Other vision process: Domain Generalization","Other vision process: Scene Text Detection","Other vision process: Visual Question Answering","Other vision process: Domain Adaptation","Other vision process: Formation Energy","Other video process: Video Retrieval","Other video process: Video Frame Interpolation","Other video process: Video Generation","Other image process: Image Reconstruction","Other image process: Image Retrieval","Other image process: Grayscale Image Denoising","Other image process: Aesthetics Quality Assessment","Other image process: Image Clustering","Other image process: Color Image Denoising","Other 3D task: 3D Shape Classification","Other 3D task: 3D Room Layouts From A Single RGB Panorama","Other 3D task: 3D Reconstruction","Other 3D task: 3D Object Reconstruction","Other 3D task: 3D Point Cloud Classification","Object tracking: Multiple Object Tracking","Object tracking: Visual Object Tracking","Object recognition: Pedestrian Attribute Recognition","Object recognition: Traffic Sign Recognition","Object detection: Dense Object Detection","Object detection: Object Detection","Object detection: Pedestrian Detection","Object detection: Weakly Supervised Object Detection","Object detection: Birds Eye View Object Detection","Object detection: Lane Detection","Object detection: 3D Object Detection","Object detection: Video Object Detection","Object detection: RGB Salient Object Detection","Image-to-image translation: Fundus to Angiography Generation","Image generation: Pose Transfer","Image generation: Conditional Image Generation","Image generation: Image Generation","Image classification: Document Image Classification","Image classification: Sequential Image Classification","Image classification: Satellite Image Classification","Image classification: Retinal OCT Disease Classification","Image classification: Hyperspectral Image Classification","Image classification: Unsupervised Image Classification","Image classification: Image Classification","Gesture recognition: Hand Gesture Recognition","Facial recognition and modelling: Unsupervised Facial Landmark Detection","Facial recognition and modelling: Face Identification","Facial recognition and modelling: Face Alignment","Facial recognition and modelling: Face Detection","Facial recognition and modelling: Facial Expression Recognition","Facial recognition and modelling: Face Verification","Facial recognition and modelling: Facial Landmark Detection","Emotion recognition: Emotion Recognition in Conversation","Activity recognition: Skeleton Based Action Recognition","Activity recognition: Multimodal Activity Recognition","Activity recognition: Egocentric Activity Recognition","Activity recognition: Human Interaction Recognition","Activity recognition: Group Activity Recognition","Activity recognition: Action Recognition","Activity recognition: Action Classification","Activity localization: Temporal Action Proposal Generation","Activity localization: Weakly Supervised Action Localization","Activity detection: Action Detection","Action localization: Action Segmentation","Action localization: Temporal Action Localization"],"showgrid":true,"gridcolor":"lightBlue","side":"left"},"legend":{"title":{"text":"task"},"tracegroupgap":0},"margin":{"t":60},"title":{"text":"Vision process","y":0.995},"font":{"size":21},"showlegend":false,"plot_bgcolor":"white","height":4023.0000000000005,"width":1500},                        {"responsive": true}                    )                };                            </script>        </div>
</body>
</html>