<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>                        <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-2.8.3.min.js"></script>                <div id="e99151f3-dcd5-469c-8ee5-bbfcf1935702" class="plotly-graph-div" style="height:1050.0px; width:1500px;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("e99151f3-dcd5-469c-8ee5-bbfcf1935702")) {                    Plotly.newPlot(                        "e99151f3-dcd5-469c-8ee5-bbfcf1935702",                        [{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Action localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Action localization","orientation":"v","showlegend":true,"x":["2016-01","2016-09","2016-11","2017-03","2017-05","2018-04","2018-06","2019-03","2019-04","2019-06","2019-07","2019-09","2019-11","2020-03"],"xaxis":"x","y":["Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity detection","orientation":"v","showlegend":true,"x":["2017-03","2017-12","2018-03","2019-04"],"xaxis":"x","y":["Activity detection","Activity detection","Activity detection","Activity detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity localization","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity localization","orientation":"v","showlegend":true,"x":["2017-12","2018-06","2018-07","2018-11","2019-05","2019-06","2019-07","2019-08","2019-11"],"xaxis":"x","y":["Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Activity recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Activity recognition","orientation":"v","showlegend":true,"x":["2013-02","2014-06","2014-12","2015-03","2015-05","2015-12","2016-01","2016-03","2016-04","2016-06","2016-08","2016-09","2016-11","2016-12","2017-03","2017-04","2017-05","2017-06","2017-08","2017-11","2017-12","2018-01","2018-02","2018-04","2018-05","2018-06","2018-07","2018-10","2018-11","2018-12","2019-01","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-11","2019-12","2020-02"],"xaxis":"x","y":["Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Emotion recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Emotion recognition","orientation":"v","showlegend":true,"x":["2018-06","2018-10","2018-11","2019-04","2019-08","2019-09"],"xaxis":"x","y":["Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition","Emotion recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Facial recognition and modelling","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Facial recognition and modelling","orientation":"v","showlegend":true,"x":["2013-07","2014-06","2014-12","2015-02","2015-03","2015-08","2015-11","2016-03","2017-03","2017-04","2017-05","2017-06","2017-07","2017-08","2017-09","2017-10","2017-11","2017-12","2018-01","2018-03","2018-04","2018-05","2018-06","2018-08","2018-09","2018-10","2018-12","2019-02","2019-03","2019-04","2019-05","2019-06","2019-08","2019-10","2019-11"],"xaxis":"x","y":["Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Gesture recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Gesture recognition","orientation":"v","showlegend":true,"x":["2017-05","2018-04","2018-12","2019-01"],"xaxis":"x","y":["Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image classification","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image classification","orientation":"v","showlegend":true,"x":["2012-12","2013-02","2013-11","2013-12","2014-04","2014-06","2014-09","2014-12","2015-02","2015-06","2015-11","2015-12","2016-02","2016-03","2016-05","2016-08","2016-10","2016-11","2017-02","2017-04","2017-07","2017-08","2017-09","2017-10","2017-12","2018-01","2018-02","2018-03","2018-05","2018-07","2018-10","2018-11","2019-01","2019-02","2019-04","2019-05","2019-06","2019-08","2019-10","2019-11","2019-12","2020-01"],"xaxis":"x","y":["Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image generation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image generation","orientation":"v","showlegend":true,"x":["2016-01","2016-06","2016-10","2016-12","2017-02","2017-03","2017-06","2017-09","2017-10","2017-12","2018-02","2018-03","2018-05","2018-07","2018-09","2018-11","2018-12","2019-03","2019-04","2019-07","2019-08","2019-11","2019-12"],"xaxis":"x","y":["Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Image-to-image translation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Image-to-image translation","orientation":"v","showlegend":true,"x":["2017-11","2019-07"],"xaxis":"x","y":["Image-to-image translation","Image-to-image translation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object detection","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object detection","orientation":"v","showlegend":true,"x":["2014-12","2015-06","2015-11","2015-12","2016-03","2016-06","2016-08","2016-09","2016-11","2016-12","2017-02","2017-03","2017-04","2017-06","2017-07","2017-08","2017-10","2017-11","2017-12","2018-02","2018-03","2018-04","2018-05","2018-06","2018-07","2018-11","2018-12","2019-01","2019-03","2019-04","2019-06","2019-07","2019-08","2019-09","2019-10","2019-11","2020-01","2020-03"],"xaxis":"x","y":["Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object recognition","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object recognition","orientation":"v","showlegend":true,"x":["2016-08","2018-06","2019-04","2019-10"],"xaxis":"x","y":["Object recognition","Object recognition","Object recognition","Object recognition"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Object tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Object tracking","orientation":"v","showlegend":true,"x":["2016-11","2017-04","2017-06","2017-10","2018-02","2018-03","2018-06","2018-11","2018-12","2019-06","2019-07","2019-09"],"xaxis":"x","y":["Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other 3D task","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other 3D task","orientation":"v","showlegend":true,"x":["2016-12","2017-04","2017-06","2017-11","2018-01","2018-02","2018-03","2018-04","2018-08","2018-11","2018-12","2019-01","2019-04"],"xaxis":"x","y":["Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other image process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other image process","orientation":"v","showlegend":true,"x":["2012-08","2015-04","2015-11","2016-04","2016-08","2016-11","2016-12","2017-03","2017-04","2017-06","2017-09","2017-10","2017-12","2018-01","2018-02","2018-03","2018-04","2018-05","2018-06","2018-07","2018-10","2018-11","2018-12","2019-01","2019-02","2019-03","2019-04","2019-09","2019-10"],"xaxis":"x","y":["Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other video process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other video process","orientation":"v","showlegend":true,"x":["2016-11","2016-12","2017-07","2018-04","2018-06","2018-08","2018-10","2019-04","2019-06","2019-07","2019-12","2020-03"],"xaxis":"x","y":["Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Other vision process","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Other vision process","orientation":"v","showlegend":true,"x":["2014-09","2015-02","2015-04","2015-05","2015-12","2016-03","2016-04","2016-05","2016-06","2016-07","2016-08","2016-09","2016-11","2017-03","2017-04","2017-05","2017-06","2017-07","2017-08","2017-09","2017-10","2017-11","2017-12","2018-01","2018-02","2018-03","2018-04","2018-05","2018-06","2018-07","2018-08","2018-09","2018-11","2018-12","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-10","2019-11","2020-01","2020-02"],"xaxis":"x","y":["Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose estimation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose estimation","orientation":"v","showlegend":true,"x":["2015-11","2016-01","2016-03","2016-08","2016-09","2016-11","2016-12","2017-01","2017-02","2017-03","2017-04","2017-05","2017-07","2017-08","2017-09","2017-10","2017-11","2017-12","2018-03","2018-04","2018-05","2018-06","2018-11","2018-12","2019-01","2019-02","2019-03","2019-06","2019-09","2019-10","2019-11","2019-12","2020-01","2020-02","2020-04"],"xaxis":"x","y":["Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Pose tracking","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Pose tracking","orientation":"v","showlegend":true,"x":["2017-12","2018-02","2018-04","2019-02","2019-05"],"xaxis":"x","y":["Pose tracking","Pose tracking","Pose tracking","Pose tracking","Pose tracking"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmenation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmenation","orientation":"v","showlegend":true,"x":["2019-03","2019-06","2019-12","2020-03"],"xaxis":"x","y":["Semantic segmenation","Semantic segmenation","Semantic segmenation","Semantic segmenation"],"yaxis":"y","type":"scatter"},{"hovertemplate":"task=%{y}<br>date=%{x}<extra></extra>","legendgroup":"Semantic segmentation","line":{"color":"black","dash":"solid","width":0},"marker":{"symbol":"circle","line":{"color":"black","width":1}},"mode":"lines","name":"Semantic segmentation","orientation":"v","showlegend":true,"x":["2014-11","2014-12","2015-02","2015-03","2015-04","2015-05","2015-09","2015-11","2016-03","2016-05","2016-06","2016-11","2016-12","2017-03","2017-04","2017-06","2017-08","2017-09","2017-10","2017-11","2017-12","2018-01","2018-02","2018-03","2018-04","2018-05","2018-06","2018-07","2018-08","2018-09","2018-10","2018-11","2018-12","2019-01","2019-02","2019-03","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2019-10","2019-11","2019-12","2020-01","2020-03","2020-04"],"xaxis":"x","y":["Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation"],"yaxis":"y","type":"scatter"},{"hovertemplate":["<BR>task: Action localization<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>","<BR>task: Action localization<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>","<BR>task: Action localization<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: MEXaction2 - Temporal Action Localization benchmarking - mAP<BR>","<BR>task: Action localization<BR>date: 2016-02<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>","<BR>task: Action localization<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>","<BR>task: Action localization<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>","<BR>task: Action localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>","<BR>task: Action localization<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Video-mAP 0.5<BR>","<BR>task: Action localization<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>","<BR>task: Action localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>","<BR>task: Action localization<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>","<BR>task: Action localization<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.2 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>","<BR>task: Activity detection<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>","<BR>task: Activity detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>","<BR>task: Activity detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>","<BR>task: Activity detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - mAP<BR>","<BR>task: Activity localization<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Activity localization<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>","<BR>task: Activity localization<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>","<BR>task: Activity localization<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@1000<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@200<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@500<BR>  Temporal Action Proposal Generation: THUMOS' 14 - Temporal Action Proposal Generation benchmarking - AR@50<BR>","<BR>task: Activity localization<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Activity localization<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - Mean mAP<BR>","<BR>task: Activity localization<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.5<BR>  Weakly Supervised Action Localization: THUMOS\u201914 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Activity recognition<BR>date: 2012-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2012-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: VIRAT Ground 2.0 - Action Recognition benchmarking - Average Accuracy<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>","<BR>task: Activity recognition<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>","<BR>task: Activity recognition<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>","<BR>task: Activity recognition<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>","<BR>task: Activity recognition<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Volleyball - Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS\u201914 - Action Classification benchmarking - mAP<BR>","<BR>task: Activity recognition<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>","<BR>task: Activity recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV1<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CV2<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>","<BR>task: Activity recognition<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ActionNet-VE - Action Recognition benchmarking - F-measure (%)<BR>","<BR>task: Activity recognition<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>","<BR>task: Activity recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top-1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: UTD-MHAD - Multimodal Activity Recognition benchmarking - Accuracy (CS)<BR>","<BR>task: Activity recognition<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>","<BR>task: Activity recognition<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - GFlops<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - Params (M)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Action Recognition: Diving-48 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: UTD-MHAD - Action Recognition benchmarking - Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>","<BR>task: Activity recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - Speed  (FPS)<BR>","<BR>task: Activity recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top 5 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Object Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-1 Accuracy<BR>  Action Classification: YouCook2 - Action Classification benchmarking - Verb Top-5 Accuracy<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: miniSports - Action Recognition benchmarking - Video hit-at-5<BR>","<BR>task: Activity recognition<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: MiniKinetics - Action Classification benchmarking - Top-1 Accuracy<BR>  Skeleton Based Action Recognition: MSR Action3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UPenn Action - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - No. parameters<BR>","<BR>task: Activity recognition<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Action Classification: THUMOS'14 - Action Classification benchmarking - mAP<BR>","<BR>task: Activity recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: Nurse Care Activity Recognition Challenge - Multimodal Activity Recognition benchmarking - Train F-measure<BR>","<BR>task: Activity recognition<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  Action Recognition: EPIC-KITCHENS-55 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: EgoGesture - Action Recognition benchmarking - Top-5 Accuracy<BR>","<BR>task: Emotion recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Arousal)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>","<BR>task: Emotion recognition<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Emotion recognition<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Emotion Recognition in Conversation: DailyDialog - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>  Emotion Recognition in Conversation: EmoryNLP - Emotion Recognition in Conversation benchmarking - Weighted Macro-F1<BR>","<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-04<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000 - Face Alignment benchmarking - Error rate<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-08<BR>Anchor.<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: Oulu-CASIA - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - FR-at-0.1(%, all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: MMI - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW2000-3D - Face Alignment benchmarking - Mean NME<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Facial recognition and modelling<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>","<BR>task: Facial recognition and modelling<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - CVGTCE<BR>  Face Alignment: 3DFAW - Face Alignment benchmarking - GTE<BR>","<BR>task: Facial recognition and modelling<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Cohn-Kanade - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-Full - Face Alignment benchmarking - Mean NME<BR>  Face Alignment: LS3D-W Balanced - Face Alignment benchmarking - AUC0.07<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Failure private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Alignment: 300W - Face Alignment benchmarking - Mean Error Rate private<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Identification: IJB-B - Face Identification benchmarking - Accuracy<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - Mean Error Rate<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW-LFPA - Face Alignment benchmarking - Mean NME<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.1<BR>  Face Verification: IJB-B - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: COFW - Face Alignment benchmarking - Mean Error Rate<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Facial Landmark Detection: AFLW-Front - Facial Landmark Detection benchmarking - Mean NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: AFLW - Face Alignment benchmarking - Mean NME<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: Real-World Affective Faces - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: IBUG - Face Alignment benchmarking - Mean Error Rate<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: FERG - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: IIIT-D Viewed Sketch - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Facial Expression Recognition: RAF-DB - Facial Expression Recognition benchmarking - Overall Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MOS<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - MS-SSIM<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - PSNR<BR>  Face Alignment: CelebA Aligned - Face Alignment benchmarking - SSIM<BR>","<BR>task: Gesture recognition<BR>date: 2013-03<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: BUAA - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: MGB - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SmartWatch - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLean test - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>  Hand Gesture Recognition: Jester val - Hand Gesture Recognition benchmarking - Top 5 Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: Northwestern University - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Gesture recognition<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Hand Gesture Recognition: DHG-14 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: DHG-28 - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 - Hand Gesture Recognition benchmarking - 28 gestures accuracy<BR>  Hand Gesture Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Hand Gesture Recognition benchmarking - 14 gestures accuracy<BR>","<BR>task: Image classification<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>","<BR>task: Image classification<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2013-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>","<BR>task: Image classification<BR>date: 2013-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Image classification<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-09<BR>Anchor.<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - # of clusters (k)<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>","<BR>task: Image classification<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>","<BR>task: Image classification<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: iNaturalist 2018 - Image Classification benchmarking - Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>","<BR>task: Image classification<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Balanced - Image Classification benchmarking - Accuracy<BR>  Image Classification: MultiMNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: smallNORB - Image Classification benchmarking - Classification Error<BR>","<BR>task: Image classification<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Clothing1M - Image Classification benchmarking - Accuracy<BR>  Image Classification: Food-101N - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: MNIST - Unsupervised Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Image Classification: CIFAR-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: CIFAR-20 - Unsupervised Image Classification benchmarking - Accuracy<BR>  Unsupervised Image Classification: STL-10 - Unsupervised Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: CINIC-10 - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Error<BR>","<BR>task: Image classification<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Hyperspectral Image Classification: Salinas Scene - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: EMNIST-Letters - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>","<BR>task: Image classification<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Classification: Flowers-102 - Image Classification benchmarking - Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top-1 Accuracy<BR>","<BR>task: Image generation<BR>date: 2014-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - bits/dimension<BR>","<BR>task: Image generation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>","<BR>task: Image generation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>","<BR>task: Image generation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>","<BR>task: Image generation<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>","<BR>task: Image generation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>","<BR>task: Image generation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom 64 x 64 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Image generation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: CelebA-HQ 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - LPIPS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - Retrieval Top10 Recall<BR>","<BR>task: Image generation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>","<BR>task: Image generation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 128x128 - Image Generation benchmarking - IS<BR>  Image Generation: ImageNet 256x256 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - bits/dimension<BR>","<BR>task: Image generation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Bedroom - Image Generation benchmarking - FID-50k<BR>","<BR>task: Image generation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 64x64 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - DS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - PCKh<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - SSIM<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-IS<BR>  Pose Transfer: Market-1501 - Pose Transfer benchmarking - mask-SSIM<BR>","<BR>task: Image generation<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: Fashion-MNIST - Image Generation benchmarking - FID<BR>  Image Generation: MNIST - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: CelebA 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: Stacked MNIST - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: ADE-Indoor - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-25K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: Cityscapes-5K 256x512 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - FID<BR>","<BR>task: Image generation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Image Generation: LSUN Car 512 x 384 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Horse 256 x 256 - Image Generation benchmarking - FID<BR>","<BR>task: Image-to-image translation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>","<BR>task: Image-to-image translation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - Kernel Inception Distance<BR>","<BR>task: Object detection<BR>date: 2014-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>","<BR>task: Object detection<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>","<BR>task: Object detection<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>","<BR>task: Object detection<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>","<BR>task: Object detection<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>","<BR>task: Object detection<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>","<BR>task: Object detection<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: PeopleArt - Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: ISTD - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>","<BR>task: Object detection<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Large MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>","<BR>task: Object detection<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - runtime (ms)<BR>","<BR>task: Object detection<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>","<BR>task: Object detection<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>","<BR>task: Object detection<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: NYU Depth v2 - 3D Object Detection benchmarking - MAP<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclist Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrian Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Heavy MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>","<BR>task: Object detection<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: CULane - Lane Detection benchmarking - F1 score<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>","<BR>task: Object detection<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Clipart1k - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: Comic2k - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>","<BR>task: Object detection<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Weakly Supervised Object Detection: IconArt - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PeopleArt - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object detection<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>","<BR>task: Object detection<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>","<BR>task: Object detection<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.5<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-test - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - F-measure<BR>","<BR>task: Object detection<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-F - 3D Object Detection benchmarking - ARs<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP50<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP75<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - AR<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARI<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARm<BR>  3D Object Detection: nuScenes-FB - 3D Object Detection benchmarking - ARs<BR>","<BR>task: Object detection<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cyclists Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Hard - Birds Eye View Object Detection benchmarking - AP<BR>","<BR>task: Object detection<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - mAP<BR>  Lane Detection: BDD100K - Lane Detection benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: BDD100K - Object Detection benchmarking - mAP-at-0.5<BR>  Object Detection: India Driving Dataset - Object Detection benchmarking - mAP-at-0.5<BR>","<BR>task: Object detection<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Object Detection: COCO 2017 - Object Detection benchmarking - Mean mAP<BR>","<BR>task: Object recognition<BR>date: 2012-02<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: GTSRB - Traffic Sign Recognition benchmarking - Accuracy<BR>","<BR>task: Object recognition<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>","<BR>task: Object recognition<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>","<BR>task: Object recognition<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>","<BR>task: Object recognition<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>","<BR>task: Object tracking<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>","<BR>task: Object tracking<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>","<BR>task: Object tracking<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Object tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Unseen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - O (Average of Measures)<BR>","<BR>task: Object tracking<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Object tracking<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>","<BR>task: Object tracking<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Average Overlap<BR>  Visual Object Tracking: GOT-10k - Visual Object Tracking benchmarking - Success Rate 0.5<BR>  Visual Object Tracking: LaSOT - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Object tracking<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Object tracking<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: VOT2019 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Object tracking<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - Precision<BR>","<BR>task: Other 3D task<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>","<BR>task: Other 3D task<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>  3D Reconstruction: Data3D\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>","<BR>task: Other 3D task<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>","<BR>task: Other 3D task<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Mean Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Other 3D task<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>","<BR>task: Other image process<BR>date: 2012-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2012-08<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>","<BR>task: Other image process<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>","<BR>task: Other image process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other image process<BR>date: 2015-04<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>","<BR>task: Other image process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Other image process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>","<BR>task: Other image process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: UMist - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>","<BR>task: Other image process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - SSIM<BR>","<BR>task: Other image process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Other image process<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other image process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: BSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma5 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: FRGC - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other image process<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: Kodak25 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma50 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: McMaster sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma75 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma35 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Clip300 sigma60 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Other image process<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: ARL Polarimetric Thermal Face Dataset - Image Clustering benchmarking - Accuracy<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other image process<BR>date: 2018-05<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Other image process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - SSIM<BR>  Grayscale Image Denoising: Urban100 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other image process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: LetterA-J - Image Clustering benchmarking - NMI<BR>","<BR>task: Other image process<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: street2shop - topwear - Image Retrieval benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>  Image Retrieval: NUS-WIDE - Image Retrieval benchmarking - MAP<BR>","<BR>task: Other image process<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - MMD<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - HP<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - MMD<BR>","<BR>task: Other image process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Grayscale Image Denoising: Set12 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: DeepFashion - Image Retrieval benchmarking - Recall-at-20<BR>","<BR>task: Other image process<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Clothes - Image Reconstruction benchmarking - LPIPS<BR>","<BR>task: Other video process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Other video process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>","<BR>task: Other video process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Other video process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - Interpolation Error<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>","<BR>task: Other video process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Mean Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text Median Rank<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>","<BR>task: Other video process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Other video process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - SSIM<BR>  Video Frame Interpolation: X4K1000FPS - Video Frame Interpolation benchmarking - tOF<BR>  Video Generation: TrailerFaces - Video Generation benchmarking - FID<BR>","<BR>task: Other video process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: BAIR Robot Pushing - Video Generation benchmarking - FVD score<BR>  Video Generation: Kinetics-600 12 frames, 128x128 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 12 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - FID<BR>  Video Generation: Kinetics-600 48 frames, 64x64 - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: ActivityNet - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: DiDeMo - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Mean Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-50<BR>  Video Retrieval: MSVD - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Other video process<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, 64x64, Unconditional - Video Generation benchmarking - Inception Score<BR>","<BR>task: Other video process<BR>date: 2020-03<BR>Anchor.<BR>benchmarks:<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Middlebury - Video Frame Interpolation benchmarking - SSIM<BR>","<BR>task: Other vision process<BR>date: 2010-11<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Beijing Air Quality - Multivariate Time Series Imputation benchmarking - MAE (PM2.5)<BR>  Multivariate Time Series Imputation: KDD CUP Challenge 2018 - Multivariate Time Series Imputation benchmarking - MSE (10% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - MAE (10% of data as GT)<BR>  Multivariate Time Series Imputation: UCI localization data - Multivariate Time Series Imputation benchmarking - MAE (10% missing)<BR>","<BR>task: Other vision process<BR>date: 2012-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2014-06<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2014-09<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>","<BR>task: Other vision process<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other vision process<BR>date: 2015-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>","<BR>task: Other vision process<BR>date: 2015-06<BR>Anchor.<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>","<BR>task: Other vision process<BR>date: 2015-08<BR>Anchor.<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>","<BR>task: Other vision process<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Domain Generalization: ImageNet-R - Domain Generalization benchmarking - Top-1 Error Rate<BR>","<BR>task: Other vision process<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other vision process<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - Mean Rank<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Other vision process<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Other vision process<BR>date: 2016-07<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>","<BR>task: Other vision process<BR>date: 2016-08<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Classification Accuracy<BR>  Domain Adaptation: Synth Objects-to-LINEMOD - Domain Adaptation benchmarking - Mean Angle Error<BR>","<BR>task: Other vision process<BR>date: 2016-09<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split unsupervised - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>","<BR>task: Other vision process<BR>date: 2016-10<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>","<BR>task: Other vision process<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Other vision process<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>","<BR>task: Other vision process<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other vision process<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - NDCG (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>","<BR>task: Other vision process<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MSE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>","<BR>task: Other vision process<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>","<BR>task: Other vision process<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>","<BR>task: Other vision process<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>","<BR>task: Other vision process<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>","<BR>task: Other vision process<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^\u22123)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Player Distance<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^\u22123)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other vision process<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>","<BR>task: Other vision process<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - RMSE<BR>  Monocular Depth Estimation: Make3D - Monocular Depth Estimation benchmarking - Sq Rel<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>","<BR>task: Other vision process<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech-10 - Domain Adaptation benchmarking - Accuracy (%)<BR>","<BR>task: Other vision process<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: CLEVR - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Other vision process<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: 100 sleep nights of 8 caregivers - Visual Question Answering benchmarking - 14 gestures accuracy<BR>  Visual Question Answering: HowmanyQA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TallyQA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - TIoU<BR>","<BR>task: Other vision process<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: SIM10K to BDD100K - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>","<BR>task: Other vision process<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: TDIUC - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - H-Mean<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Other vision process<BR>date: 2019-05<BR>Anchor.<BR>benchmarks:<BR>  Depth Completion: VOID - Depth Completion benchmarking - MAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - RMSE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iMAE<BR>  Depth Completion: VOID - Depth Completion benchmarking - iRMSE<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>  Video Prediction: CMU Mocap-1 - Video Prediction benchmarking - Test Error<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>","<BR>task: Other vision process<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Scene Text Detection: IC19-ReCTs - Scene Text Detection benchmarking - F-Measure<BR>","<BR>task: Other vision process<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - ATV<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - AUC<BR>  Horizon Line Estimation: KITTI Horizon - Horizon Line Estimation benchmarking - MSE<BR>  Visual Question Answering: GQA test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - number<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - other<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - unanswerable<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - yes/no<BR>","<BR>task: Other vision process<BR>date: 2020-02<BR>Anchor.<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - mean Recall @20<BR>","<BR>task: Pose estimation<BR>date: 2010-03<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: HumanEva-I - 3D Human Pose Estimation benchmarking - Mean Reconstruction Error (mm)<BR>","<BR>task: Pose estimation<BR>date: 2013-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Human3.6M - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>","<BR>task: Pose estimation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>","<BR>task: Pose estimation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Pose estimation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with other data)<BR>","<BR>task: Pose estimation<BR>date: 2016-01<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>","<BR>task: Pose estimation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>","<BR>task: Pose estimation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>","<BR>task: Pose estimation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: BJUT-3D - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: Pointing'04 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>","<BR>task: Pose estimation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - FPS<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: UAV-Human - Pose Estimation benchmarking - mAP<BR>","<BR>task: Pose estimation<BR>date: 2017-01<BR>Anchor.<BR>benchmarks:<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>","<BR>task: Pose estimation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: MSRA Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>","<BR>task: Pose estimation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>","<BR>task: Pose estimation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - MPJPE (CS)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>","<BR>task: Pose estimation<BR>date: 2017-05<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>","<BR>task: Pose estimation<BR>date: 2017-08<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>","<BR>task: Pose estimation<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Head Pose Estimation: BIWI - Head Pose Estimation benchmarking - MAE (trained with BIWI data)<BR>","<BR>task: Pose estimation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean IoU<BR>  6D Pose Estimation using RGB: OCCLUSION - 6D Pose Estimation using RGB benchmarking - MAP<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean IoU<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - IoU-3D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-2D<BR>  6D Pose Estimation using RGBD: Tejani - 6D Pose Estimation using RGBD benchmarking - VSS-3D<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>","<BR>task: Pose estimation<BR>date: 2017-12<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - PA-MPJPE<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - acceleration error<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>","<BR>task: Pose estimation<BR>date: 2018-02<BR>Anchor.<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>","<BR>task: Pose estimation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADI<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADI<BR>","<BR>task: Pose estimation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: CHALL H80K - 3D Human Pose Estimation benchmarking - MPJPE<BR>","<BR>task: Pose estimation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: Occlusion LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean AUC<BR>","<BR>task: Pose estimation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: CAMERA25 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 10cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 10, 5cm<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-25<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 3DIou-at-50<BR>  6D Pose Estimation using RGBD: REAL275 - 6D Pose Estimation using RGBD benchmarking - mAP 5, 5cm<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP<BR>  Pose Estimation: COCO minival - Pose Estimation benchmarking - AP<BR>","<BR>task: Pose estimation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Mean Recall<BR>  6D Pose Estimation using RGBD: T-LESS - 6D Pose Estimation using RGBD benchmarking - Mean Recall<BR>","<BR>task: Pose estimation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJAE<BR>  3D Human Pose Estimation: 3D Poses in the Wild Challenge - 3D Human Pose Estimation benchmarking - MPJPE<BR>","<BR>task: Pose estimation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation using RGB: T-LESS - 6D Pose Estimation using RGB benchmarking - Recall (VSD)<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - FPS<BR>  Hand Pose Estimation: K2HPD - Hand Pose Estimation benchmarking - PDJ@5mm<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - FPS<BR>","<BR>task: Pose estimation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPVPE<BR>","<BR>task: Pose estimation<BR>date: 2019-10<BR>Anchor.<BR>benchmarks:<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - 5\u00b05 cm<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - IOU25<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Rerr<BR>  6D Pose Estimation: NOCS-REAL275 - 6D Pose Estimation benchmarking - Terr<BR>","<BR>task: Pose estimation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2019 - Hand Pose Estimation benchmarking - Average 3D Error<BR>","<BR>task: Pose estimation<BR>date: 2020-04<BR>Anchor.<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - PCK3D<BR>","<BR>task: Pose tracking<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: Multi-Person PoseTrack - Pose Tracking benchmarking - MOTP<BR>","<BR>task: Pose tracking<BR>date: 2017-10<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Pose tracking<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2018 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Semantic segmenation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mAcc<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmenation<BR>date: 2019-02<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>  3D Instance Segmentation: ScanNet - 3D Instance Segmentation benchmarking - mAP<BR>","<BR>task: Semantic segmenation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>","<BR>task: Semantic segmenation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mCov<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mWCov<BR>","<BR>task: Semantic segmentation<BR>date: 2014-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2014-11<BR>Anchor.<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Dice<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2014-12<BR>Anchor.<BR>benchmarks:<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>","<BR>task: Semantic segmentation<BR>date: 2015-05<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2013 leaderboard - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Medical Image Segmentation: CVC-ClinicDB - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - Warping Error<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - Average MAE<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - DSC<BR>  Semantic Segmentation: Kvasir-Instrument - Semantic Segmentation benchmarking - mIoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>","<BR>task: Semantic segmentation<BR>date: 2015-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2015-12<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-03<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISLES-2015 - Lesion Segmentation benchmarking - Dice Score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-04<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>","<BR>task: Semantic segmentation<BR>date: 2016-05<BR>Anchor.<BR>benchmarks:<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-06<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-11<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2011 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2016-12<BR>Anchor.<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-02<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Lesion Segmentation: ISIC 2017 - Lesion Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-04<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>","<BR>task: Semantic segmentation<BR>date: 2017-06<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-07<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2017-09<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2014 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>","<BR>task: Semantic segmentation<BR>date: 2017-11<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV1 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.25<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: NYU Depth v2 - Instance Segmentation benchmarking - mAP-at-0.5<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>","<BR>task: Semantic segmentation<BR>date: 2018-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQst<BR>","<BR>task: Semantic segmentation<BR>date: 2018-03<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: iSAID - Instance Segmentation benchmarking - Average Precision<BR>","<BR>task: Semantic segmentation<BR>date: 2018-04<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: iSEG 2017 Challenge - Medical Image Segmentation benchmarking - Dice Score<BR>","<BR>task: Semantic segmentation<BR>date: 2018-06<BR>Anchor.<BR>benchmarks:<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: HRF - Retinal Vessel Segmentation benchmarking - F1 score<BR>","<BR>task: Semantic segmentation<BR>date: 2018-07<BR>Anchor.<BR>benchmarks:<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - Dice<BR>  Medical Image Segmentation: 2018 Data Science Bowl - Medical Image Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-08<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SUN-RGBD - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SYNTHIA-CVPR\u201916 - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>","<BR>task: Semantic segmentation<BR>date: 2018-10<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: BUS 2017 Dataset B - Lesion Segmentation benchmarking - Dice Score<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>","<BR>task: Semantic segmentation<BR>date: 2018-11<BR>Anchor.<BR>benchmarks:<BR>  Human Part Segmentation: MHP v2.0 - Human Part Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-12<BR>Anchor.<BR>benchmarks:<BR>  Semantic Segmentation: KITTI Semantic Segmentation - Semantic Segmentation benchmarking - Mean IoU (class)<BR>","<BR>task: Semantic segmentation<BR>date: 2019-01<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: COCO panoptic - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - Pixel Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2019-03<BR>Anchor.<BR>benchmarks:<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VInfo<BR>  Medical Image Segmentation: ISBI 2012 EM Segmentation - Medical Image Segmentation benchmarking - VRand<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2019-04<BR>Anchor.<BR>benchmarks:<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - AVD<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Precision<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - Recall<BR>  Lung Nodule Segmentation: NIH - Lung Nodule Segmentation benchmarking - VS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: ParisLille3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-06<BR>Anchor.<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - MSD<BR>  Brain Tumor Segmentation: BRATS 2018 - Brain Tumor Segmentation benchmarking - VS<BR>  Brain Tumor Segmentation: BRATS 2018 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: CHAOS MRI Dataset - Medical Image Segmentation benchmarking - VS<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - MSD<BR>  Medical Image Segmentation: HSVM - Medical Image Segmentation benchmarking - VS<BR>","<BR>task: Semantic segmentation<BR>date: 2019-07<BR>Anchor.<BR>benchmarks:<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mAcc<BR>  3D Semantic Segmentation: S3DIS - 3D Semantic Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - mIoU<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - Accuracy<BR>  Lung Nodule Segmentation: Montgomery County - Lung Nodule Segmentation benchmarking - mIoU<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-08<BR>Anchor.<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - F1-Score<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - Dice<BR>  Lung Nodule Segmentation: LIDC-IDRI - Lung Nodule Segmentation benchmarking - IoU<BR>  Lung Nodule Segmentation: Lung Nodule  - Lung Nodule Segmentation benchmarking - Dice Score<BR>  Medical Image Segmentation: DRIVE - Medical Image Segmentation benchmarking - F1 score<BR>","<BR>task: Semantic segmentation<BR>date: 2019-09<BR>Anchor.<BR>benchmarks:<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-11<BR>Anchor.<BR>benchmarks:<BR>  Skin Cancer Segmentation: PH2 - Skin Cancer Segmentation benchmarking - IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-12<BR>Anchor.<BR>benchmarks:<BR>  Instance Segmentation: LVIS v1.0 - Instance Segmentation benchmarking - mask AP<BR>  Medical Image Segmentation: Cell - Medical Image Segmentation benchmarking - IoU<BR>  Medical Image Segmentation: EM - Medical Image Segmentation benchmarking - IoU<BR>  Semantic Segmentation: BDD - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Category mIoU<BR>  Semantic Segmentation: GTAV-to-Cityscapes Labels - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2020-01<BR>Anchor.<BR>benchmarks:<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - Frame (fps)<BR>"],"marker":{"line":{"width":1,"color":"black"},"size":21,"symbol":42},"mode":"markers","x":["2015-06","2015-11","2016-01","2016-02","2016-04","2016-09","2017-03","2017-05","2018-04","2018-06","2019-03","2020-01","2015-07","2016-12","2017-03","2019-04","2017-03","2017-07","2017-12","2018-06","2018-07","2019-08","2019-11","2012-07","2012-10","2012-12","2013-06","2014-03","2014-06","2014-11","2015-11","2015-12","2016-04","2016-06","2016-08","2016-11","2016-12","2017-03","2017-04","2017-05","2017-06","2017-08","2017-10","2017-11","2017-12","2018-01","2018-02","2018-06","2018-07","2018-11","2018-12","2019-01","2019-04","2019-05","2019-06","2019-07","2019-08","2019-09","2020-04","2017-07","2019-03","2019-09","2013-07","2014-04","2014-06","2014-08","2014-12","2015-03","2015-05","2015-06","2015-07","2015-09","2015-11","2016-04","2016-07","2016-09","2016-10","2017-01","2017-03","2017-05","2017-06","2017-08","2017-09","2017-10","2018-02","2018-03","2018-04","2018-05","2018-08","2018-12","2019-02","2019-03","2019-05","2019-07","2019-08","2013-03","2014-06","2017-01","2017-05","2017-07","2017-11","2018-04","2019-01","2019-07","2012-02","2012-12","2013-01","2013-06","2013-12","2015-02","2015-04","2015-09","2015-11","2015-12","2016-03","2016-12","2017-07","2017-08","2017-10","2017-11","2018-02","2018-03","2018-05","2018-06","2018-07","2018-10","2019-01","2019-02","2019-04","2019-06","2019-10","2019-12","2014-10","2015-11","2016-01","2016-06","2016-10","2017-03","2017-05","2017-06","2017-09","2017-10","2017-12","2018-02","2018-07","2018-09","2018-11","2018-12","2019-03","2019-04","2019-05","2019-10","2019-11","2019-12","2016-11","2017-11","2014-03","2014-06","2014-07","2014-11","2015-04","2015-05","2015-06","2015-11","2015-12","2016-03","2016-08","2016-09","2016-10","2016-11","2016-12","2017-02","2017-03","2017-07","2017-08","2017-11","2017-12","2018-03","2018-06","2018-10","2018-12","2019-03","2019-04","2019-05","2019-07","2019-08","2019-09","2019-12","2012-02","2015-12","2017-09","2018-06","2019-04","2015-04","2015-12","2016-06","2016-11","2017-04","2017-06","2018-11","2019-01","2019-07","2019-09","2016-03","2016-04","2016-10","2016-12","2018-03","2012-03","2012-08","2013-12","2014-12","2015-04","2015-08","2015-11","2015-12","2016-04","2016-06","2016-08","2016-11","2017-03","2017-04","2017-06","2017-10","2018-04","2018-05","2018-10","2018-11","2018-12","2019-01","2019-02","2019-03","2019-08","2019-10","2015-06","2016-09","2016-10","2017-08","2018-06","2018-08","2019-04","2019-07","2019-12","2020-03","2010-11","2012-12","2014-06","2014-09","2014-11","2014-12","2015-02","2015-05","2015-06","2015-08","2015-11","2015-12","2016-04","2016-05","2016-06","2016-07","2016-08","2016-09","2016-10","2016-12","2017-02","2017-03","2017-04","2017-05","2017-06","2017-07","2017-08","2017-09","2017-11","2017-12","2018-01","2018-03","2018-06","2018-07","2018-08","2018-10","2018-11","2018-12","2019-02","2019-04","2019-05","2019-06","2019-07","2019-08","2020-02","2010-03","2013-12","2014-07","2014-11","2015-11","2016-01","2016-03","2016-05","2016-11","2016-12","2017-01","2017-02","2017-03","2017-04","2017-05","2017-08","2017-10","2017-11","2017-12","2018-02","2018-03","2018-09","2018-12","2019-01","2019-02","2019-07","2019-08","2019-09","2019-10","2020-01","2020-04","2016-11","2017-10","2018-04","2018-01","2019-02","2019-04","2019-12","2014-07","2014-11","2014-12","2015-05","2015-11","2015-12","2016-03","2016-04","2016-05","2016-06","2016-11","2016-12","2017-02","2017-03","2017-04","2017-06","2017-07","2017-09","2017-11","2018-01","2018-03","2018-04","2018-06","2018-07","2018-08","2018-09","2018-10","2018-11","2018-12","2019-01","2019-03","2019-04","2019-06","2019-07","2019-08","2019-09","2019-11","2019-12","2020-01"],"y":["Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Action localization","Activity detection","Activity detection","Activity detection","Activity detection","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity localization","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Activity recognition","Emotion recognition","Emotion recognition","Emotion recognition","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Facial recognition and modelling","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Gesture recognition","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image classification","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image generation","Image-to-image translation","Image-to-image translation","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object detection","Object recognition","Object recognition","Object recognition","Object recognition","Object recognition","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Object tracking","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other 3D task","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other image process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other video process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Other vision process","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose estimation","Pose tracking","Pose tracking","Pose tracking","Semantic segmenation","Semantic segmenation","Semantic segmenation","Semantic segmenation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation","Semantic segmentation"],"type":"scatter","line":{"color":"black","width":0}},{"hovertemplate":["<BR>task: Other image process<BR>date: 2012-08<BR>ratio: 0.5546<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>","<BR>task: Image classification<BR>date: 2012-12<BR>ratio: 0.0189<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Image classification<BR>date: 2013-02<BR>ratio: 0.1073<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>","<BR>task: Activity recognition<BR>date: 2013-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Skeleton Based Action Recognition: CAD-120 - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2013-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: FER2013 - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2013-11<BR>ratio: 0.0397<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2013-12<BR>ratio: 0.0553<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>","<BR>task: Image classification<BR>date: 2014-04<BR>ratio: 0.021<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>","<BR>task: Image classification<BR>date: 2014-06<BR>ratio: 0.289<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>","<BR>task: Activity recognition<BR>date: 2014-06<BR>ratio: 0.8122<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-06<BR>ratio: 0.4737<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2014-09<BR>ratio: 0.1446<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2014-09<BR>ratio: 0.9752<BR>benchmarks:<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-Olympic - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2014-11<BR>ratio: 0.2834<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Activity recognition<BR>date: 2014-12<BR>ratio: 0.2375<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>","<BR>task: Semantic segmentation<BR>date: 2014-12<BR>ratio: 0.2513<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2014-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2014-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Detection: Caltech - Pedestrian Detection benchmarking - Reasonable Miss Rate<BR>","<BR>task: Image classification<BR>date: 2014-12<BR>ratio: 0.1392<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Semantic segmentation<BR>date: 2015-02<BR>ratio: 0.0877<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-02<BR>ratio: 0.0451<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2015-02<BR>ratio: 0.6408<BR>benchmarks:<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-02<BR>ratio: 0.0134<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2015-03<BR>ratio: 0.3753<BR>benchmarks:<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2015-03<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-03<BR>ratio: 0.2327<BR>benchmarks:<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2015-04<BR>ratio: 0.2878<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Other vision process<BR>date: 2015-04<BR>ratio: 0.1007<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other image process<BR>date: 2015-04<BR>ratio: 0.426<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other vision process<BR>date: 2015-05<BR>ratio: 0.24846666666666664<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Unsupervised Domain Adaptation: Office-Home - Unsupervised Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2015-05<BR>ratio: 0.8426<BR>benchmarks:<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>","<BR>task: Activity recognition<BR>date: 2015-05<BR>ratio: 0.1675<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>","<BR>task: Image classification<BR>date: 2015-06<BR>ratio: 0.4459<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Object detection<BR>date: 2015-06<BR>ratio: 0.2457<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-08<BR>ratio: 0.4321<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Semantic segmentation<BR>date: 2015-09<BR>ratio: 0.5<BR>benchmarks:<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2015-11<BR>ratio: 0.17486666666666664<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Other image process<BR>date: 2015-11<BR>ratio: 0.1694<BR>benchmarks:<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>","<BR>task: Pose estimation<BR>date: 2015-11<BR>ratio: 0.0331<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Image classification<BR>date: 2015-11<BR>ratio: 0.3169<BR>benchmarks:<BR>  Image Classification: SVHN - Image Classification benchmarking - Percentage error<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Object detection<BR>date: 2015-11<BR>ratio: 0.4339<BR>benchmarks:<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Semantic segmentation<BR>date: 2015-11<BR>ratio: 0.58925<BR>benchmarks:<BR>  Medical Image Segmentation: RITE - Medical Image Segmentation benchmarking - Jaccard Index<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Activity recognition<BR>date: 2015-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Recognition: ActivityNet - Action Recognition benchmarking - mAP<BR>","<BR>task: Other vision process<BR>date: 2015-12<BR>ratio: 0.7477<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>","<BR>task: Object detection<BR>date: 2015-12<BR>ratio: 0.5016<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2012 - Object Detection benchmarking - MAP<BR>","<BR>task: Image classification<BR>date: 2015-12<BR>ratio: 0.47796666666666665<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2016-01<BR>ratio: 0.6524<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Image generation<BR>date: 2016-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: Binarized MNIST - Image Generation benchmarking - nats<BR>","<BR>task: Action localization<BR>date: 2016-01<BR>ratio: 0.0478<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>","<BR>task: Activity recognition<BR>date: 2016-01<BR>ratio: 0.0707<BR>benchmarks:<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>","<BR>task: Image classification<BR>date: 2016-02<BR>ratio: 0.056<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2016-03<BR>ratio: 0.40365<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Image classification<BR>date: 2016-03<BR>ratio: 0.2255<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2016-03<BR>ratio: 0.1471<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Other vision process<BR>date: 2016-03<BR>ratio: 0.2757<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2016-03<BR>ratio: 0.3359<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Pose estimation<BR>date: 2016-03<BR>ratio: 0.7322<BR>benchmarks:<BR>  Pose Estimation: FLIC Elbows - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: FLIC Wrists - Pose Estimation benchmarking - PCK-at-0.2<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Activity recognition<BR>date: 2016-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Multimodal Activity Recognition: MSR Daily Activity3D dataset - Multimodal Activity Recognition benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2016-04<BR>ratio: 0.5947<BR>benchmarks:<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - m-reIRMSE<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: COCO count-test - Object Counting benchmarking - mRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-reIRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - m-relRMSE<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE-nz<BR>  Object Counting: Pascal VOC 2007 count-test - Object Counting benchmarking - mRMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other image process<BR>date: 2016-04<BR>ratio: 0.5532666666666667<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - NMI<BR>  Image Clustering: Coil-20 - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - NMI<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>","<BR>task: Activity recognition<BR>date: 2016-04<BR>ratio: 0.21814999999999998<BR>benchmarks:<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>","<BR>task: Image classification<BR>date: 2016-05<BR>ratio: 0.1069<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other vision process<BR>date: 2016-05<BR>ratio: 0.1741<BR>benchmarks:<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2016-05<BR>ratio: 0.0418<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Other vision process<BR>date: 2016-06<BR>ratio: 0.2999<BR>benchmarks:<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Image generation<BR>date: 2016-06<BR>ratio: 0.46724999999999994<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>","<BR>task: Semantic segmentation<BR>date: 2016-06<BR>ratio: 0.0741<BR>benchmarks:<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Activity recognition<BR>date: 2016-06<BR>ratio: 0.061<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2016-06<BR>ratio: 0.31905<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>","<BR>task: Other vision process<BR>date: 2016-07<BR>ratio: 0.0241<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>","<BR>task: Object detection<BR>date: 2016-08<BR>ratio: 0.2999<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>","<BR>task: Object recognition<BR>date: 2016-08<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Backpack<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Gender<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - Hat<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - LCS<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCC<BR>  Pedestrian Attribute Recognition: UAV-Human - Pedestrian Attribute Recognition benchmarking - UCS<BR>","<BR>task: Image classification<BR>date: 2016-08<BR>ratio: 0.0251<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other vision process<BR>date: 2016-08<BR>ratio: 0.6356666666666667<BR>benchmarks:<BR>  Crowd Counting: UCF-QNRF - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: MNIST-to-MNIST-M - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Digits-to-SVHN - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Synth Signs-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>  Horizon Line Estimation: Eurasian Cities Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Horizon Line Estimation: York Urban Dataset - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>","<BR>task: Other image process<BR>date: 2016-08<BR>ratio: 0.36235<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Pose estimation<BR>date: 2016-08<BR>ratio: 0.1233<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>","<BR>task: Activity recognition<BR>date: 2016-08<BR>ratio: 0.4643<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2016-09<BR>ratio: 0.0144<BR>benchmarks:<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Pose estimation<BR>date: 2016-09<BR>ratio: 0.0093<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>","<BR>task: Activity recognition<BR>date: 2016-09<BR>ratio: 0.3082<BR>benchmarks:<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>","<BR>task: Action localization<BR>date: 2016-09<BR>ratio: 0.2784<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>","<BR>task: Other vision process<BR>date: 2016-09<BR>ratio: 0.4115<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>","<BR>task: Image classification<BR>date: 2016-10<BR>ratio: 0.0142<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Image generation<BR>date: 2016-10<BR>ratio: 0.0606<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>","<BR>task: Semantic segmentation<BR>date: 2016-11<BR>ratio: 0.26530000000000004<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Global Accuracy<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Activity recognition<BR>date: 2016-11<BR>ratio: 0.3399<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>","<BR>task: Other vision process<BR>date: 2016-11<BR>ratio: 0.3954<BR>benchmarks:<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: Visual7W - Visual Question Answering benchmarking - Percentage correct<BR>","<BR>task: Pose estimation<BR>date: 2016-11<BR>ratio: 0.674<BR>benchmarks:<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>","<BR>task: Other video process<BR>date: 2016-11<BR>ratio: 0.2492<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>","<BR>task: Other image process<BR>date: 2016-11<BR>ratio: 0.2441<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>","<BR>task: Object detection<BR>date: 2016-11<BR>ratio: 0.27403333333333335<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Action localization<BR>date: 2016-11<BR>ratio: 0.3571<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>","<BR>task: Object tracking<BR>date: 2016-11<BR>ratio: 0.096<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Image classification<BR>date: 2016-11<BR>ratio: 0.0684<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Image generation<BR>date: 2016-12<BR>ratio: 0.1288<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>","<BR>task: Other image process<BR>date: 2016-12<BR>ratio: 0.1001<BR>benchmarks:<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>","<BR>task: Other 3D task<BR>date: 2016-12<BR>ratio: 0.77015<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Reconstruction: Data3D\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>","<BR>task: Activity recognition<BR>date: 2016-12<BR>ratio: 0.1429<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>","<BR>task: Other video process<BR>date: 2016-12<BR>ratio: 0.1022<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>","<BR>task: Object detection<BR>date: 2016-12<BR>ratio: 0.07565<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>","<BR>task: Pose estimation<BR>date: 2016-12<BR>ratio: 0.5639000000000001<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: MPII Multi-Person - Keypoint Detection benchmarking - mAP-at-0.5<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>","<BR>task: Semantic segmentation<BR>date: 2016-12<BR>ratio: 0.33213333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Pose estimation<BR>date: 2017-01<BR>ratio: 0.16765000000000002<BR>benchmarks:<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>","<BR>task: Image classification<BR>date: 2017-02<BR>ratio: 0.6995<BR>benchmarks:<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>","<BR>task: Object detection<BR>date: 2017-02<BR>ratio: 0.9524<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Medium MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Small MR^-2<BR>","<BR>task: Image generation<BR>date: 2017-02<BR>ratio: 0.0541<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Pose estimation<BR>date: 2017-02<BR>ratio: 0.0692<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Object detection<BR>date: 2017-03<BR>ratio: 0.1565<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>","<BR>task: Action localization<BR>date: 2017-03<BR>ratio: 0.1096<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2017-03<BR>ratio: 0.44627500000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Multi-tissue Nucleus Segmentation: Kumar - Multi-tissue Nucleus Segmentation benchmarking - Hausdorff Distance (mm)<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Activity recognition<BR>date: 2017-03<BR>ratio: 0.39885<BR>benchmarks:<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SBU - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-03<BR>ratio: 0.1193<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Other vision process<BR>date: 2017-03<BR>ratio: 0.3612<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other image process<BR>date: 2017-03<BR>ratio: 0.5645<BR>benchmarks:<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: coil-100 - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2017-03<BR>ratio: 0.2235<BR>benchmarks:<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>","<BR>task: Image generation<BR>date: 2017-03<BR>ratio: 0.13935<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>","<BR>task: Activity detection<BR>date: 2017-03<BR>ratio: 0.2205<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>","<BR>task: Semantic segmentation<BR>date: 2017-04<BR>ratio: 0.20765<BR>benchmarks:<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Pose estimation<BR>date: 2017-04<BR>ratio: 0.51655<BR>benchmarks:<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>","<BR>task: Object detection<BR>date: 2017-04<BR>ratio: 0.1476<BR>benchmarks:<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Image classification<BR>date: 2017-04<BR>ratio: 0.4855<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-04<BR>ratio: 0.2189333333333333<BR>benchmarks:<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.1<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.2<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Multimodal Activity Recognition: EV-Action - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>","<BR>task: Other image process<BR>date: 2017-04<BR>ratio: 0.5854250000000001<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Color Image Denoising: BSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: BSD68 sigma25 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: FRGC - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Clustering: YouTube Faces DB - Image Clustering benchmarking - Accuracy<BR>","<BR>task: Object tracking<BR>date: 2017-04<BR>ratio: 0.103<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Other 3D task<BR>date: 2017-04<BR>ratio: 0.8514<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: Sydney Urban Objects - 3D Point Cloud Classification benchmarking - F1<BR>","<BR>task: Other vision process<BR>date: 2017-04<BR>ratio: 0.32236<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Visual Question Answering: VQA v1 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v1 test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-04<BR>ratio: 0.15055000000000002<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-05<BR>ratio: 0.2366<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Gesture recognition<BR>date: 2017-05<BR>ratio: 0.8352<BR>benchmarks:<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2017-05<BR>ratio: 0.2803<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>","<BR>task: Action localization<BR>date: 2017-05<BR>ratio: 0.2561<BR>benchmarks:<BR>  Temporal Action Localization: J-HMDB-21 - Temporal Action Localization benchmarking - Frame-mAP<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>  Temporal Action Localization: UCF101-24 - Temporal Action Localization benchmarking - Frame-mAP<BR>","<BR>task: Other vision process<BR>date: 2017-05<BR>ratio: 0.5805666666666667<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Pose estimation<BR>date: 2017-05<BR>ratio: 0.53035<BR>benchmarks:<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CA)<BR>  3D Human Pose Estimation: Geometric Pose Affordance  - 3D Human Pose Estimation benchmarking - PCK3D (CS)<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>","<BR>task: Other image process<BR>date: 2017-06<BR>ratio: 0.7104<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>","<BR>task: Image generation<BR>date: 2017-06<BR>ratio: 0.2514<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>","<BR>task: Object tracking<BR>date: 2017-06<BR>ratio: 0.6175<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - F-Measure (Seen)<BR>","<BR>task: Other 3D task<BR>date: 2017-06<BR>ratio: 0.4844<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2017-06<BR>ratio: 0.2771333333333333<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>","<BR>task: Other vision process<BR>date: 2017-06<BR>ratio: 0.37770000000000004<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>","<BR>task: Object detection<BR>date: 2017-06<BR>ratio: 0.6641<BR>benchmarks:<BR>  Weakly Supervised Object Detection: COCO - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-06<BR>ratio: 0.4898<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Activity recognition<BR>date: 2017-06<BR>ratio: 0.6277<BR>benchmarks:<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2017-07<BR>ratio: 0.2633333333333333<BR>benchmarks:<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-07<BR>ratio: 0.6997<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Alignment: WFLW - Face Alignment benchmarking - ME (%, all)<BR>","<BR>task: Pose estimation<BR>date: 2017-07<BR>ratio: 0.8307<BR>benchmarks:<BR>  Hand Pose Estimation: NYU Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>","<BR>task: Image classification<BR>date: 2017-07<BR>ratio: 0.0363<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>","<BR>task: Other video process<BR>date: 2017-07<BR>ratio: 0.2484<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video Median Rank<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Object detection<BR>date: 2017-07<BR>ratio: 0.3431666666666667<BR>benchmarks:<BR>  Object Detection: Visual Genome - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Image classification<BR>date: 2017-08<BR>ratio: 0.219<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-08<BR>ratio: 0.5744<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>","<BR>task: Object detection<BR>date: 2017-08<BR>ratio: 0.3489<BR>benchmarks:<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - Average MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Other vision process<BR>date: 2017-08<BR>ratio: 0.17363333333333333<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2017-08<BR>ratio: 0.50415<BR>benchmarks:<BR>  Hand Pose Estimation: ICVL Hands - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2017-08<BR>ratio: 0.4921<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>","<BR>task: Activity recognition<BR>date: 2017-08<BR>ratio: 0.5913<BR>benchmarks:<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV II)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>","<BR>task: Other vision process<BR>date: 2017-09<BR>ratio: 0.3414<BR>benchmarks:<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Image generation<BR>date: 2017-09<BR>ratio: 0.043050000000000005<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Pose estimation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Human Pose Estimation: Total Capture - 3D Human Pose Estimation benchmarking - Average MPJPE (mm)<BR>","<BR>task: Other image process<BR>date: 2017-09<BR>ratio: 0.8514<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>","<BR>task: Semantic segmentation<BR>date: 2017-09<BR>ratio: 1.0<BR>benchmarks:<BR>  Pancreas Segmentation: TCIA Pancreas-CT Dataset - Pancreas Segmentation benchmarking - Dice Score<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-09<BR>ratio: 0.0551<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>","<BR>task: Image classification<BR>date: 2017-09<BR>ratio: 0.0809<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other image process<BR>date: 2017-10<BR>ratio: 0.5759333333333333<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma35 - Color Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>","<BR>task: Image generation<BR>date: 2017-10<BR>ratio: 0.5297<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>","<BR>task: Object detection<BR>date: 2017-10<BR>ratio: 0.9976<BR>benchmarks:<BR>  Lane Detection: Caltech Lanes Cordova - Lane Detection benchmarking - F1<BR>  Lane Detection: Caltech Lanes Washington - Lane Detection benchmarking - F1<BR>  RGB Salient Object Detection: SBU - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>  RGB Salient Object Detection: UCF - RGB Salient Object Detection benchmarking - Balanced Error Rate<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-10<BR>ratio: 0.9847<BR>benchmarks:<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Image classification<BR>date: 2017-10<BR>ratio: 0.29335<BR>benchmarks:<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Other vision process<BR>date: 2017-10<BR>ratio: 0.7097<BR>benchmarks:<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>","<BR>task: Object tracking<BR>date: 2017-10<BR>ratio: 0.6429<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Semantic segmentation<BR>date: 2017-10<BR>ratio: 0.23095<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Pose estimation<BR>date: 2017-10<BR>ratio: 0.656<BR>benchmarks:<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>","<BR>task: Object detection<BR>date: 2017-11<BR>ratio: 0.5154000000000001<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  Birds Eye View Object Detection: KITTI Cars Easy val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard val - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate val - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP50<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: COCO test-dev - Weakly Supervised Object Detection benchmarking - AP50<BR>","<BR>task: Pose estimation<BR>date: 2017-11<BR>ratio: 0.3768<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: ITOP front-view - Pose Estimation benchmarking - Mean mAP<BR>  Pose Estimation: ITOP top-view - Pose Estimation benchmarking - Mean mAP<BR>","<BR>task: Image-to-image translation<BR>date: 2017-11<BR>ratio: 0.2407<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>","<BR>task: Semantic segmentation<BR>date: 2017-11<BR>ratio: 0.40068333333333334<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ShapeNet - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>","<BR>task: Other vision process<BR>date: 2017-11<BR>ratio: 0.1634<BR>benchmarks:<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Activity recognition<BR>date: 2017-11<BR>ratio: 0.44705<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Toyota Smarthome dataset - Action Classification benchmarking - CS<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Clip Hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>","<BR>task: Other 3D task<BR>date: 2017-11<BR>ratio: 0.789<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-11<BR>ratio: 0.435<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>","<BR>task: Activity detection<BR>date: 2017-12<BR>ratio: 0.5028<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>","<BR>task: Image generation<BR>date: 2017-12<BR>ratio: 0.5<BR>benchmarks:<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - IS<BR>","<BR>task: Other vision process<BR>date: 2017-12<BR>ratio: 0.613<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SYNSIG-to-GTSRB - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Activity localization<BR>date: 2017-12<BR>ratio: 0.2406<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Other image process<BR>date: 2017-12<BR>ratio: 0.046<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>","<BR>task: Facial recognition and modelling<BR>date: 2017-12<BR>ratio: 0.0247<BR>benchmarks:<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Pose estimation<BR>date: 2017-12<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Pose Estimation: HANDS 2017 - Hand Pose Estimation benchmarking - Average 3D Error<BR>  Pose Estimation: J-HMDB - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>","<BR>task: Object detection<BR>date: 2017-12<BR>ratio: 0.4283<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>","<BR>task: Activity recognition<BR>date: 2017-12<BR>ratio: 0.1885<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>","<BR>task: Pose tracking<BR>date: 2017-12<BR>ratio: 0.1898<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Image classification<BR>date: 2017-12<BR>ratio: 0.0083<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2017-12<BR>ratio: 0.0439<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>","<BR>task: Semantic segmentation<BR>date: 2018-01<BR>ratio: 0.2233<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-01<BR>ratio: 0.5359<BR>benchmarks:<BR>  Face Identification: MegaFace - Face Identification benchmarking - Accuracy<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>  Face Verification: MegaFace - Face Verification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2018-01<BR>ratio: 0.3059<BR>benchmarks:<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>","<BR>task: Other 3D task<BR>date: 2018-01<BR>ratio: 0.0583<BR>benchmarks:<BR>  3D Point Cloud Classification: ScanObjectNN - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-01<BR>ratio: 0.44029999999999997<BR>benchmarks:<BR>  Document Image Classification: RVL-CDIP - Document Image Classification benchmarking - Accuracy<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Acc<BR>  Retinal OCT Disease Classification: OCT2017 - Retinal OCT Disease Classification benchmarking - Sensitivity<BR>  Retinal OCT Disease Classification: Srinivasan2014 - Retinal OCT Disease Classification benchmarking - Acc<BR>","<BR>task: Other image process<BR>date: 2018-01<BR>ratio: 0.3197<BR>benchmarks:<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Activity recognition<BR>date: 2018-01<BR>ratio: 0.28373333333333334<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-1 (%)<BR>  Multimodal Activity Recognition: Moments in Time Dataset - Multimodal Activity Recognition benchmarking - Top-5 (%)<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>","<BR>task: Other vision process<BR>date: 2018-02<BR>ratio: 0.3996<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>","<BR>task: Other image process<BR>date: 2018-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma50 - Color Image Denoising benchmarking - PSNR<BR>","<BR>task: Pose tracking<BR>date: 2018-02<BR>ratio: 0.2155<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Object tracking<BR>date: 2018-02<BR>ratio: 0.7714000000000001<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: OTB-2013 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>  Visual Object Tracking: OTB-50 - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Other 3D task<BR>date: 2018-02<BR>ratio: 0.628<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>","<BR>task: Semantic segmentation<BR>date: 2018-02<BR>ratio: 0.4785333333333333<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: STARE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: PASCAL VOC 2012 test - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - AUC<BR>  Skin Cancer Segmentation: Kaggle Skin Lesion Segmentation - Skin Cancer Segmentation benchmarking - F1 score<BR>","<BR>task: Activity recognition<BR>date: 2018-02<BR>ratio: 0.9695<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Florence 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: J-HMBD Early Action - Skeleton Based Action Recognition benchmarking - 10%<BR>","<BR>task: Image generation<BR>date: 2018-02<BR>ratio: 0.3205<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 32x32 - Image Generation benchmarking - bpd<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Image classification<BR>date: 2018-02<BR>ratio: 0.1678<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Unsupervised Image Classification: SVHN - Unsupervised Image Classification benchmarking - Acc<BR>","<BR>task: Object detection<BR>date: 2018-02<BR>ratio: 0.1698<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Semantic segmentation<BR>date: 2018-03<BR>ratio: 0.46340000000000003<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Scene Segmentation: ScanNet - Scene Segmentation benchmarking - Average Accuracy<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Other vision process<BR>date: 2018-03<BR>ratio: 0.1918<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2018-03<BR>ratio: 0.1099<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>","<BR>task: Object detection<BR>date: 2018-03<BR>ratio: 0.62775<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  Object Detection: iSAID - Object Detection benchmarking - Average Precision<BR>  Weakly Supervised Object Detection: Watercolor2k - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-03<BR>ratio: 0.49684999999999996<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Face Identification: IJB-A - Face Identification benchmarking - Accuracy<BR>  Face Verification: YouTube Faces DB - Face Verification benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Object tracking<BR>date: 2018-03<BR>ratio: 0.1202<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Image generation<BR>date: 2018-03<BR>ratio: 0.3293<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>","<BR>task: Image classification<BR>date: 2018-03<BR>ratio: 0.1184<BR>benchmarks:<BR>  Sequential Image Classification: Sequential MNIST - Sequential Image Classification benchmarking - Permuted Accuracy<BR>","<BR>task: Other 3D task<BR>date: 2018-03<BR>ratio: 0.1351<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2018-03<BR>ratio: 0.24766666666666667<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Activity detection<BR>date: 2018-03<BR>ratio: 0.387<BR>benchmarks:<BR>  Action Detection: Charades - Action Detection benchmarking - mAP<BR>  Action Detection: Multi-THUMOS - Action Detection benchmarking - mAP<BR>","<BR>task: Other video process<BR>date: 2018-04<BR>ratio: 0.6147<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Other vision process<BR>date: 2018-04<BR>ratio: 0.3826<BR>benchmarks:<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: COCO-Text - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other 3D task<BR>date: 2018-04<BR>ratio: 0.211<BR>benchmarks:<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-16<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-1<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-2<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-32<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-4<BR>  3D Shape Classification: Pix3D - 3D Shape Classification benchmarking - R-at-8<BR>","<BR>task: Other image process<BR>date: 2018-04<BR>ratio: 0.3237<BR>benchmarks:<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Extended Yale-B - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Pose estimation<BR>date: 2018-04<BR>ratio: 0.27253333333333335<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>","<BR>task: Object detection<BR>date: 2018-04<BR>ratio: 0.0067<BR>benchmarks:<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Semantic segmentation<BR>date: 2018-04<BR>ratio: 0.6517<BR>benchmarks:<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Dice Score<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Precision<BR>  Pancreas Segmentation: CT-150 - Pancreas Segmentation benchmarking - Recall<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL VOC 2012 val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Gesture recognition<BR>date: 2018-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Hand Gesture Recognition: ChaLearn val - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: Jester test - Hand Gesture Recognition benchmarking - Top 1 Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-04<BR>ratio: 0.21509999999999999<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - Fullset (public)<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Activity recognition<BR>date: 2018-04<BR>ratio: 0.5245<BR>benchmarks:<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UWA3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Pose tracking<BR>date: 2018-04<BR>ratio: 0.6801<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Action localization<BR>date: 2018-04<BR>ratio: 0.3804<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.6<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.7<BR>","<BR>task: Other image process<BR>date: 2018-05<BR>ratio: 0.6039<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Image generation<BR>date: 2018-05<BR>ratio: 0.2865<BR>benchmarks:<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>","<BR>task: Pose estimation<BR>date: 2018-05<BR>ratio: 0.028<BR>benchmarks:<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>","<BR>task: Other vision process<BR>date: 2018-05<BR>ratio: 0.23425<BR>benchmarks:<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-05<BR>ratio: 0.857<BR>benchmarks:<BR>  Face Alignment: 300W - Face Alignment benchmarking - AUC0.08 private<BR>  Facial Expression Recognition: Static Facial Expressions in the Wild - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2018-05<BR>ratio: 0.2306<BR>benchmarks:<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>","<BR>task: Activity recognition<BR>date: 2018-05<BR>ratio: 0.6543<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: UAV-Human - Skeleton Based Action Recognition benchmarking - Average Accuracy<BR>","<BR>task: Image classification<BR>date: 2018-05<BR>ratio: 0.145<BR>benchmarks:<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2018-05<BR>ratio: 0.6951<BR>benchmarks:<BR>  Object Detection: PASCAL VOC 2007 - Object Detection benchmarking - MAP<BR>","<BR>task: Activity recognition<BR>date: 2018-06<BR>ratio: 0.3492333333333333<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: J-HMDB - Skeleton Based Action Recognition benchmarking - Accuracy (RGB+pose)<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: UT-Kinect - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2018-06<BR>ratio: 0.5132666666666666<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - F1 score<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object recognition<BR>date: 2018-06<BR>ratio: 1.0<BR>benchmarks:<BR>  Traffic Sign Recognition: Bosch Small Traffic Lights - Traffic Sign Recognition benchmarking - MAP<BR>  Traffic Sign Recognition: Tsinghua-Tencent 100K - Traffic Sign Recognition benchmarking - MAP<BR>","<BR>task: Activity localization<BR>date: 2018-06<BR>ratio: 0.7435<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (test)<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>","<BR>task: Pose estimation<BR>date: 2018-06<BR>ratio: 0.9967<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - 3DPCK<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - MJPE<BR>","<BR>task: Semantic segmentation<BR>date: 2018-06<BR>ratio: 0.29605000000000004<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Other image process<BR>date: 2018-06<BR>ratio: 0.2047<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD68 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Set12 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma15 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma25 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: Urban100 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Object tracking<BR>date: 2018-06<BR>ratio: 0.2077<BR>benchmarks:<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-06<BR>ratio: 0.1128<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: MAFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Emotion recognition<BR>date: 2018-06<BR>ratio: 0.7567<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Expectancy)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Power)<BR>  Emotion Recognition in Conversation: SEMAINE - Emotion Recognition in Conversation benchmarking - MAE (Valence)<BR>","<BR>task: Other vision process<BR>date: 2018-06<BR>ratio: 0.7323200000000001<BR>benchmarks:<BR>  Formation Energy: Materials Project - Formation Energy benchmarking - MAE<BR>  Formation Energy: QM9 - Formation Energy benchmarking - MAE<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - Abs Rel<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - RMSE log<BR>  Monocular Depth Estimation: Mid-Air Dataset - Monocular Depth Estimation benchmarking - SQ Rel<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: MuJoCo - Multivariate Time Series Imputation benchmarking - MSE (10^2, 50% missing)<BR>  Multivariate Time Series Imputation: PhysioNet Challenge 2012 - Multivariate Time Series Imputation benchmarking - mse (10^-3)<BR>  Scene Graph Generation: VRD - Scene Graph Generation benchmarking - Recall-at-50<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Action localization<BR>date: 2018-06<BR>ratio: 0.3894<BR>benchmarks:<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>","<BR>task: Other video process<BR>date: 2018-06<BR>ratio: 0.3129<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>","<BR>task: Image classification<BR>date: 2018-07<BR>ratio: 0.53155<BR>benchmarks:<BR>  Hyperspectral Image Classification: Pavia University - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other image process<BR>date: 2018-07<BR>ratio: 0.4427<BR>benchmarks:<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - PSNR (sRGB)<BR>  Color Image Denoising: Darmstadt Noise Dataset - Color Image Denoising benchmarking - SSIM (sRGB)<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - ARI<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - NMI<BR>","<BR>task: Other vision process<BR>date: 2018-07<BR>ratio: 0.2614<BR>benchmarks:<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Image generation<BR>date: 2018-07<BR>ratio: 0.7291<BR>benchmarks:<BR>  Image Generation: CAT 256x256 - Image Generation benchmarking - FID<BR>  Image Generation: ImageNet 64x64 - Image Generation benchmarking - Bits per dim<BR>","<BR>task: Semantic segmentation<BR>date: 2018-07<BR>ratio: 0.5724<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - S-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - max E-Measure<BR>  Medical Image Segmentation: Kvasir-SEG - Medical Image Segmentation benchmarking - mean Dice<BR>","<BR>task: Object detection<BR>date: 2018-07<BR>ratio: 0.46819999999999995<BR>benchmarks:<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Bare MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Partial MR^-2<BR>  Pedestrian Detection: CityPersons - Pedestrian Detection benchmarking - Reasonable MR^-2<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: ImageNet - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Activity localization<BR>date: 2018-07<BR>ratio: 0.4436<BR>benchmarks:<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Activity recognition<BR>date: 2018-07<BR>ratio: 0.9928<BR>benchmarks:<BR>  Action Classification: ActivityNet-1.2 - Action Classification benchmarking - mAP<BR>  Action Classification: THUMOS\u201914 - Action Classification benchmarking - mAP<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>","<BR>task: Other 3D task<BR>date: 2018-08<BR>ratio: 0.0244<BR>benchmarks:<BR>  3D Reconstruction: Data3D\u2212R2N2 - 3D Reconstruction benchmarking - 3DIoU<BR>","<BR>task: Semantic segmentation<BR>date: 2018-08<BR>ratio: 0.6698<BR>benchmarks:<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Freiburg Forest - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ScanNetV2 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-08<BR>ratio: 0.6807<BR>benchmarks:<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Other video process<BR>date: 2018-08<BR>ratio: 0.3547<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>","<BR>task: Other vision process<BR>date: 2018-08<BR>ratio: 0.2432<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - Runtime [ms]<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>","<BR>task: Image generation<BR>date: 2018-09<BR>ratio: 0.37644999999999995<BR>benchmarks:<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: CIFAR-10 - Conditional Image Generation benchmarking - Inception score<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - FID<BR>  Conditional Image Generation: ImageNet 128x128 - Conditional Image Generation benchmarking - Inception score<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-09<BR>ratio: 0.46865<BR>benchmarks:<BR>  Face Detection: Annotated Faces in the Wild - Face Detection benchmarking - AP<BR>  Face Detection: PASCAL Face - Face Detection benchmarking - AP<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.01<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Other vision process<BR>date: 2018-09<BR>ratio: 0.1442<BR>benchmarks:<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>","<BR>task: Semantic segmentation<BR>date: 2018-09<BR>ratio: 0.14682499999999998<BR>benchmarks:<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Dice<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - F1-score<BR>  Nuclear Segmentation: Cell17 - Nuclear Segmentation benchmarking - Hausdorff<BR>  Semantic Segmentation: COCO-Stuff test - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-10<BR>ratio: 0.0079<BR>benchmarks:<BR>  Face Detection: FDDB - Face Detection benchmarking - AP<BR>","<BR>task: Other video process<BR>date: 2018-10<BR>ratio: 0.2609<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>","<BR>task: Image classification<BR>date: 2018-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Sequential Image Classification: Sequential CIFAR-10 - Sequential Image Classification benchmarking - Unpermuted Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2018-10<BR>ratio: 0.2615<BR>benchmarks:<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>","<BR>task: Activity recognition<BR>date: 2018-10<BR>ratio: 0.0156<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>","<BR>task: Other image process<BR>date: 2018-10<BR>ratio: 0.14425<BR>benchmarks:<BR>  Aesthetics Quality Assessment: AVA - Aesthetics Quality Assessment benchmarking - Accuracy<BR>  Image Clustering: CMU-PIE - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>","<BR>task: Emotion recognition<BR>date: 2018-10<BR>ratio: 0.2577<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Object detection<BR>date: 2018-11<BR>ratio: 0.3155<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - AP75<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - box AP<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Image classification<BR>date: 2018-11<BR>ratio: 0.0808<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other vision process<BR>date: 2018-11<BR>ratio: 0.4657<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - MAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MAE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - MSE<BR>  Video Prediction: Human3.6M - Video Prediction benchmarking - SSIM<BR>","<BR>task: Object tracking<BR>date: 2018-11<BR>ratio: 0.3892<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>","<BR>task: Activity localization<BR>date: 2018-11<BR>ratio: 0.1431<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>","<BR>task: Other image process<BR>date: 2018-11<BR>ratio: 0.43694999999999995<BR>benchmarks:<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CUB Birds - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Cars - Image Clustering benchmarking - NMI<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Stanford Dogs - Image Clustering benchmarking - NMI<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Oxf105k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Oxf5k - Image Retrieval benchmarking - MAP<BR>  Image Retrieval: Par106k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: Par6k - Image Retrieval benchmarking - mAP<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Other 3D task<BR>date: 2018-11<BR>ratio: 0.87635<BR>benchmarks:<BR>  3D Reconstruction: Scan2CAD - 3D Reconstruction benchmarking - Average Accuracy<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Realtor360 - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>","<BR>task: Emotion recognition<BR>date: 2018-11<BR>ratio: 0.5661<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Activity recognition<BR>date: 2018-11<BR>ratio: 0.40796666666666664<BR>benchmarks:<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-1 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: BIT - Human Interaction Recognition benchmarking - Accuracy<BR>  Human Interaction Recognition: UT - Human Interaction Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Image generation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - FID<BR>  Image Generation: CUB 128 x 128 - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Cars - Image Generation benchmarking - Inception score<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - FID<BR>  Image Generation: Stanford Dogs - Image Generation benchmarking - Inception score<BR>","<BR>task: Pose estimation<BR>date: 2018-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: DensePose-COCO - Pose Estimation benchmarking - AP<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Frames Per View<BR>","<BR>task: Semantic segmentation<BR>date: 2018-11<BR>ratio: 0.4542<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>","<BR>task: Activity recognition<BR>date: 2018-12<BR>ratio: 0.500225<BR>benchmarks:<BR>  Action Classification: Charades - Action Classification benchmarking - MAP<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Recognition: AVA v2.1 - Action Recognition benchmarking - mAP (Val)<BR>  Action Recognition: AVA v2.2 - Action Recognition benchmarking - mAP<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2018-12<BR>ratio: 0.30485<BR>benchmarks:<BR>  Face Identification: Trillion Pairs Dataset - Face Identification benchmarking - Accuracy<BR>  Face Verification: Trillion Pairs Dataset - Face Verification benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2018-12<BR>ratio: 0.27595000000000003<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>  6D Pose Estimation using RGB: YCB-Video - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  Head Pose Estimation: AFLW - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Test AP<BR>  Keypoint Detection: COCO - Keypoint Detection benchmarking - Validation AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>","<BR>task: Other vision process<BR>date: 2018-12<BR>ratio: 0.73865<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - OOB Rate (10^\u22123)<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Difference<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Path Length<BR>  Multivariate Time Series Imputation: Basketball Players Movement - Multivariate Time Series Imputation benchmarking - Step Change (10^\u22123)<BR>  Multivariate Time Series Imputation: PEMS-SF - Multivariate Time Series Imputation benchmarking - L2 Loss (10^-4)<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Unsupervised Domain Adaptation: Cityscapes to Foggy Cityscapes - Unsupervised Domain Adaptation benchmarking - mAP-at-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2018-12<BR>ratio: 0.4456333333333333<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Semantic Segmentation: CamVid - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>","<BR>task: Gesture recognition<BR>date: 2018-12<BR>ratio: 0.6789<BR>benchmarks:<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: NVGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: VIVA Hand Gestures Dataset - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2018-12<BR>ratio: 0.30150000000000005<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrians Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>","<BR>task: Other image process<BR>date: 2018-12<BR>ratio: 0.4718<BR>benchmarks:<BR>  Image Clustering: Fashion-MNIST - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-full - Image Clustering benchmarking - NMI<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>","<BR>task: Image generation<BR>date: 2018-12<BR>ratio: 0.6087<BR>benchmarks:<BR>  Image Generation: CelebA 256x256 - Image Generation benchmarking - bpd<BR>  Image Generation: CelebA-HQ 1024x1024 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Bedroom 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - Inception score<BR>","<BR>task: Other 3D task<BR>date: 2018-12<BR>ratio: 0.027<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Object tracking<BR>date: 2018-12<BR>ratio: 0.1675<BR>benchmarks:<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Accuracy<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Normalized Precision<BR>  Visual Object Tracking: TrackingNet - Visual Object Tracking benchmarking - Precision<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>","<BR>task: Semantic segmentation<BR>date: 2019-01<BR>ratio: 0.16060000000000002<BR>benchmarks:<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQst<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Other image process<BR>date: 2019-01<BR>ratio: 0.1323<BR>benchmarks:<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: MNIST-test - Image Clustering benchmarking - NMI<BR>  Image Clustering: USPS - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: USPS - Image Clustering benchmarking - NMI<BR>","<BR>task: Other vision process<BR>date: 2019-01<BR>ratio: 0.3127<BR>benchmarks:<BR>  Domain Adaptation: Office-31 - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: VisDA2017 - Domain Adaptation benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2019-01<BR>ratio: 0.20259999999999997<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AP<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR75<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARL<BR>  Keypoint Detection: COCO test-challenge - Keypoint Detection benchmarking - ARM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR50<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - ARM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP50<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Object detection<BR>date: 2019-01<BR>ratio: 0.0882<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>","<BR>task: Gesture recognition<BR>date: 2019-01<BR>ratio: 0.564<BR>benchmarks:<BR>  Hand Gesture Recognition: Cambridge - Hand Gesture Recognition benchmarking - Accuracy<BR>  Hand Gesture Recognition: EgoGesture - Hand Gesture Recognition benchmarking - Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-01<BR>ratio: 0.9117666666666667<BR>benchmarks:<BR>  Action Recognition: ICVL-4 - Action Recognition benchmarking - Accuracy<BR>  Action Recognition: IRD - Action Recognition benchmarking - Accuracy<BR>  Multimodal Activity Recognition: LboroHAR - Multimodal Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>","<BR>task: Other 3D task<BR>date: 2019-01<BR>ratio: 0.24609999999999999<BR>benchmarks:<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - 3DIoU<BR>  3D Object Reconstruction: Data3D\u2212R2N2 - 3D Object Reconstruction benchmarking - Avg F1<BR>  3D Room Layouts From A Single RGB Panorama: PanoContext - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>  3D Room Layouts From A Single RGB Panorama: Stanford 2D-3D - 3D Room Layouts From A Single RGB Panorama benchmarking - 3DIoU<BR>","<BR>task: Image classification<BR>date: 2019-01<BR>ratio: 0.3141<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: Kuzushiji-MNIST - Image Classification benchmarking - Accuracy<BR>","<BR>task: Image classification<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Hyperspectral Image Classification: Indian Pines - Hyperspectral Image Classification benchmarking - Overall Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2019-02<BR>ratio: 0.0788<BR>benchmarks:<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Mean ADD<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AP75<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APL<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - APM<BR>  Keypoint Detection: COCO test-dev - Keypoint Detection benchmarking - AR<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2019-02<BR>ratio: 0.1<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>","<BR>task: Other image process<BR>date: 2019-02<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Retrieval: INRIA Holidays - Image Retrieval benchmarking - Mean mAP<BR>","<BR>task: Other vision process<BR>date: 2019-02<BR>ratio: 0.22343333333333334<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - RMSE<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Pose tracking<BR>date: 2019-02<BR>ratio: 0.0183<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - mAP<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-02<BR>ratio: 0.5371333333333334<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (7 emotion)<BR>  Facial Expression Recognition: JAFFE - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Landmark Detection: 300W - Facial Landmark Detection benchmarking - NME<BR>  Facial Landmark Detection: AFLW-Full - Facial Landmark Detection benchmarking - Mean NME<BR>","<BR>task: Image generation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Image Generation: CelebA-HQ 128x128 - Image Generation benchmarking - FID<BR>","<BR>task: Pose estimation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  Weakly-supervised 3D Human Pose Estimation: Human3.6M - Weakly-supervised 3D Human Pose Estimation benchmarking - Number of Views<BR>","<BR>task: Other image process<BR>date: 2019-03<BR>ratio: 0.46475<BR>benchmarks:<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Handbags - Image Reconstruction benchmarking - LPIPS<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - FID<BR>  Image Reconstruction: Edge-to-Shoes - Image Reconstruction benchmarking - LPIPS<BR>  Image Retrieval: CARS196 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: CUB-200-2011 - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: In-Shop - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: SOP - Image Retrieval benchmarking - R-at-1<BR>","<BR>task: Semantic segmentation<BR>date: 2019-03<BR>ratio: 0.098<BR>benchmarks:<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Semantic segmenation<BR>date: 2019-03<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mRec<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-03<BR>ratio: 0.1256<BR>benchmarks:<BR>  Face Verification: BUAA-VisNir - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: CASIA NIR-VIS 2.0 - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: Oulu-CASIA NIR-VIS - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Action localization<BR>date: 2019-03<BR>ratio: 0.6113999999999999<BR>benchmarks:<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>","<BR>task: Other vision process<BR>date: 2019-03<BR>ratio: 0.5759749999999999<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-C - Domain Generalization benchmarking - mean Corruption Error (mCE)<BR>  Monocular Depth Estimation: KITTI Eigen split - Monocular Depth Estimation benchmarking - absolute relative error<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2017 MLT - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Object detection<BR>date: 2019-03<BR>ratio: 0.2573<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclists Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: nuScenes - 3D Object Detection benchmarking - NDS<BR>","<BR>task: Image generation<BR>date: 2019-04<BR>ratio: 0.52595<BR>benchmarks:<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Pose Transfer: Deep-Fashion - Pose Transfer benchmarking - SSIM<BR>","<BR>task: Activity detection<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.1<BR>  Action Detection: UCF101-24 - Action Detection benchmarking - Video-mAP 0.2<BR>","<BR>task: Action localization<BR>date: 2019-04<BR>ratio: 0.155<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2019-04<BR>ratio: 0.3335<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Class Average IoU<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  3D Semantic Segmentation: SemanticKITTI - 3D Semantic Segmentation benchmarking - mIoU<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP50<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - AP75<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - oAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: S3DIS Area5 - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: ScanNet - Semantic Segmentation benchmarking - 3DIoU<BR>","<BR>task: Emotion recognition<BR>date: 2019-04<BR>ratio: 1.0<BR>benchmarks:<BR>  Emotion Recognition in Conversation: EC - Emotion Recognition in Conversation benchmarking - Micro-F1<BR>","<BR>task: Other 3D task<BR>date: 2019-04<BR>ratio: 0.1081<BR>benchmarks:<BR>  3D Point Cloud Classification: ModelNet40 - 3D Point Cloud Classification benchmarking - Overall Accuracy<BR>","<BR>task: Object detection<BR>date: 2019-04<BR>ratio: 0.500375<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD val - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP75<BR>  Dense Object Detection: SKU-110K - Dense Object Detection benchmarking - AP<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUT-OMRON - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: ECSSD - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: HKU-IS - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - F-measure<BR>  RGB Salient Object Detection: PASCAL-S - RGB Salient Object Detection benchmarking - MAE<BR>  RGB Salient Object Detection: SOD - RGB Salient Object Detection benchmarking - MAE<BR>  Weakly Supervised Object Detection: Charades - Weakly Supervised Object Detection benchmarking - MAP<BR>  Weakly Supervised Object Detection: HICO-DET - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Activity recognition<BR>date: 2019-04<BR>ratio: 0.279925<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@5<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-1<BR>  Action Recognition: Sports-1M - Action Recognition benchmarking - Video hit-at-5<BR>  Group Activity Recognition: Collective Activity - Group Activity Recognition benchmarking - Accuracy<BR>  Group Activity Recognition: Volleyball - Group Activity Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.1<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.2<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.3<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.4<BR>  Skeleton Based Action Recognition: JHMDB Pose Tracking - Skeleton Based Action Recognition benchmarking - PCK-at-0.5<BR>  Skeleton Based Action Recognition: N-UCLA - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: SYSU 3D - Skeleton Based Action Recognition benchmarking - Accuracy<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (AV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CS)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV I)<BR>  Skeleton Based Action Recognition: Varying-view RGB-D Action-Skeleton - Skeleton Based Action Recognition benchmarking - Accuracy (CV II)<BR>","<BR>task: Image classification<BR>date: 2019-04<BR>ratio: 0.3059<BR>benchmarks:<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Other image process<BR>date: 2019-04<BR>ratio: 0.68665<BR>benchmarks:<BR>  Color Image Denoising: CBSD68 sigma15 - Color Image Denoising benchmarking - PSNR<BR>  Color Image Denoising: CBSD68 sigma75 - Color Image Denoising benchmarking - PSNR<BR>  Image Clustering: CIFAR-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: CIFAR-100 - Image Clustering benchmarking - NMI<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: ImageNet-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Imagenet-dog-15 - Image Clustering benchmarking - NMI<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: STL-10 - Image Clustering benchmarking - NMI<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - Accuracy<BR>  Image Clustering: Tiny-ImageNet - Image Clustering benchmarking - NMI<BR>","<BR>task: Object recognition<BR>date: 2019-04<BR>ratio: 0.6667<BR>benchmarks:<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP at-0.5:0.95<BR>  Traffic Sign Recognition: DFG traffic-sign dataset - Traffic Sign Recognition benchmarking - mAP-at-0.50<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-04<BR>ratio: 0.355<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: IJB-A - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.001<BR>  Face Verification: IJB-C - Face Verification benchmarking - TAR at FAR=0.01<BR>","<BR>task: Other vision process<BR>date: 2019-04<BR>ratio: 0.37775000000000003<BR>benchmarks:<BR>  Curved Text Detection: SCUT-CTW1500 - Curved Text Detection benchmarking - F-Measure<BR>  Denoising: Darmstadt Noise Dataset - Denoising benchmarking - PSNR<BR>  Object Counting: CARPK - Object Counting benchmarking - MAE<BR>  Object Counting: CARPK - Object Counting benchmarking - RMSE<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2013 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: SCUT-CTW1500 - Scene Text Detection benchmarking - H-Mean<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - MRR<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: VisDial v0.9 val - Visual Dialog benchmarking - R-at-5<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - MRR (x 100)<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - Mean<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-10<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-1<BR>  Visual Dialog: Visual Dialog v1.0 test-std - Visual Dialog benchmarking - R-at-5<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other video process<BR>date: 2019-04<BR>ratio: 0.1348<BR>benchmarks:<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>","<BR>task: Semantic segmentation<BR>date: 2019-05<BR>ratio: 0.7854<BR>benchmarks:<BR>  Panoptic Segmentation: Indian Driving Dataset - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: KITTI Panoptic Segmentation - Panoptic Segmentation benchmarking - PQ<BR>","<BR>task: Pose tracking<BR>date: 2019-05<BR>ratio: 0.0083<BR>benchmarks:<BR>  Pose Tracking: PoseTrack2017 - Pose Tracking benchmarking - MOTA<BR>","<BR>task: Activity localization<BR>date: 2019-05<BR>ratio: 0.7002<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>","<BR>task: Image classification<BR>date: 2019-05<BR>ratio: 0.1623<BR>benchmarks:<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-05<BR>ratio: 0.7067<BR>benchmarks:<BR>  Facial Expression Recognition: AffectNet - Facial Expression Recognition benchmarking - Accuracy (8 emotion)<BR>  Facial Expression Recognition: FERPlus - Facial Expression Recognition benchmarking - Accuracy<BR>  Facial Expression Recognition: SFEW - Facial Expression Recognition benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-05<BR>ratio: 0.7222285714285714<BR>benchmarks:<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iMAE<BR>  Depth Completion: KITTI Depth Completion - Depth Completion benchmarking - iRMSE<BR>  Domain Adaptation: HMDBfull-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: HMDBsmall-to-UCF - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Olympic-to-HMDBsmall - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVNH-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: UCF-to-HMDBfull - Domain Adaptation benchmarking - Accuracy<BR>  Domain Generalization: ImageNet-A - Domain Generalization benchmarking - Top-1 accuracy %<BR>  Formation Energy: OQMD v1.2 - Formation Energy benchmarking - MAE<BR>  Horizon Line Estimation: Horizon Lines in the Wild - Horizon Line Estimation benchmarking - AUC (horizon error)<BR>  Video Prediction: CMU Mocap-2 - Video Prediction benchmarking - Test Error<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Activity recognition<BR>date: 2019-05<BR>ratio: 0.17579999999999998<BR>benchmarks:<BR>  Action Classification: Kinetics-400 - Action Classification benchmarking - Vid acc@1<BR>  Action Recognition: Jester - Action Recognition benchmarking - Val<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S2)<BR>","<BR>task: Object detection<BR>date: 2019-06<BR>ratio: 0.45675000000000004<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean E-Measure<BR>  RGB Salient Object Detection: DUTS-TE - RGB Salient Object Detection benchmarking - mean F-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - S-Measure<BR>  RGB Salient Object Detection: SOC - RGB Salient Object Detection benchmarking - mean E-Measure<BR>","<BR>task: Object tracking<BR>date: 2019-06<BR>ratio: 0.2823<BR>benchmarks:<BR>  Visual Object Tracking: OTB-2015 - Visual Object Tracking benchmarking - AUC<BR>","<BR>task: Pose estimation<BR>date: 2019-06<BR>ratio: 0.1399<BR>benchmarks:<BR>  Head Pose Estimation: AFLW2000 - Head Pose Estimation benchmarking - MAE<BR>","<BR>task: Semantic segmenation<BR>date: 2019-06<BR>ratio: 0.2174<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>","<BR>task: Action localization<BR>date: 2019-06<BR>ratio: 0.0985<BR>benchmarks:<BR>  Temporal Action Localization: CrossTask - Temporal Action Localization benchmarking - Recall<BR>","<BR>task: Other video process<BR>date: 2019-06<BR>ratio: 0.6079<BR>benchmarks:<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: YouCook2 - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Semantic segmentation<BR>date: 2019-06<BR>ratio: 0.5671750000000001<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2015 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Brain Tumor Segmentation: BRATS-2017 val - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - Total Variation of Information<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Merge<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - VI Split<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: PASCAL Context - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Activity localization<BR>date: 2019-06<BR>ratio: 0.0589<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-06<BR>ratio: 0.0932<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>","<BR>task: Image classification<BR>date: 2019-06<BR>ratio: 0.2305<BR>benchmarks:<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: STL-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: iNaturalist - Image Classification benchmarking - Top 1 Accuracy<BR>","<BR>task: Activity recognition<BR>date: 2019-06<BR>ratio: 0.1741666666666667<BR>benchmarks:<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-1 Accuracy<BR>  Action Classification: Kinetics-600 - Action Classification benchmarking - Top-5 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 1 Accuracy<BR>  Action Classification: Moments in Time - Action Classification benchmarking - Top 5 Accuracy<BR>  Action Recognition: HMDB-51 - Action Recognition benchmarking - Average accuracy of 3 splits<BR>  Action Recognition: UCF101 - Action Recognition benchmarking - 3-fold Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-06<BR>ratio: 0.6436666666666667<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: UCF CC 50 - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>","<BR>task: Action localization<BR>date: 2019-07<BR>ratio: 0.3796<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.75<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>","<BR>task: Semantic segmentation<BR>date: 2019-07<BR>ratio: 1.0<BR>benchmarks:<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - Accuracy<BR>","<BR>task: Object detection<BR>date: 2019-07<BR>ratio: 0.42693333333333333<BR>benchmarks:<BR>  3D Object Detection: KITTI Cars Easy - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cars Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Cyclist Moderate val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Easy val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Hard val - 3D Object Detection benchmarking - AP<BR>  3D Object Detection: KITTI Pedestrian Moderate val - 3D Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Hard - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cars Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Cyclists Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Birds Eye View Object Detection: KITTI Pedestrians Moderate - Birds Eye View Object Detection benchmarking - AP<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>","<BR>task: Other video process<BR>date: 2019-07<BR>ratio: 0.4898<BR>benchmarks:<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: LSMDC - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - text-to-video R-at-5<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-10<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-1<BR>  Video Retrieval: MSR-VTT - Video Retrieval benchmarking - video-to-text R-at-5<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-10<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-1<BR>  Video Retrieval: MSR-VTT-1kA - Video Retrieval benchmarking - text-to-video R-at-5<BR>","<BR>task: Image-to-image translation<BR>date: 2019-07<BR>ratio: 0.7593<BR>benchmarks:<BR>  Fundus to Angiography Generation: Fundus Fluorescein Angiogram Photographs & Colour Fundus Images of Diabetic Patients - Fundus to Angiography Generation benchmarking - FID<BR>","<BR>task: Activity recognition<BR>date: 2019-07<BR>ratio: 0.16535<BR>benchmarks:<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.3<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.4<BR>  Action Recognition: THUMOS\u201914 - Action Recognition benchmarking - mAP-at-0.5<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 14 gestures accuracy<BR>  Skeleton Based Action Recognition: SHREC 2017 track on 3D Hand Gesture Recognition - Skeleton Based Action Recognition benchmarking - 28 gestures accuracy<BR>","<BR>task: Activity localization<BR>date: 2019-07<BR>ratio: 0.2415<BR>benchmarks:<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AR@100<BR>  Temporal Action Proposal Generation: ActivityNet-1.3 - Temporal Action Proposal Generation benchmarking - AUC (val)<BR>","<BR>task: Object tracking<BR>date: 2019-07<BR>ratio: 0.835<BR>benchmarks:<BR>  Visual Object Tracking: VOT2016 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: VOT2017/18 - Visual Object Tracking benchmarking - Expected Average Overlap (EAO)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Seen)<BR>  Visual Object Tracking: YouTube-VOS - Visual Object Tracking benchmarking - Jaccard (Unseen)<BR>","<BR>task: Image generation<BR>date: 2019-07<BR>ratio: 0.0989<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>","<BR>task: Other vision process<BR>date: 2019-07<BR>ratio: 0.42065<BR>benchmarks:<BR>  Monocular Depth Estimation: NYU-Depth V2 - Monocular Depth Estimation benchmarking - RMSE<BR>  Visual Question Answering: GQA test-std - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-08<BR>ratio: 0.491<BR>benchmarks:<BR>  Crowd Counting: ShanghaiTech A - Crowd Counting benchmarking - MAE<BR>  Crowd Counting: ShanghaiTech B - Crowd Counting benchmarking - MAE<BR>  Domain Adaptation: SYNTHIA-to-Cityscapes - Domain Adaptation benchmarking - mIoU<BR>  Metric Learning: CARS196 - Metric Learning benchmarking - R-at-1<BR>  Metric Learning: CUB-200-2011 - Metric Learning benchmarking - R-at-1<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Binary<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Consistency<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Distribution<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Open<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Plausibility<BR>  Visual Question Answering: GQA Test2019 - Visual Question Answering benchmarking - Validity<BR>  Visual Question Answering: VQA v2 test-dev - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: VQA v2 test-std - Visual Question Answering benchmarking - overall<BR>  Visual Question Answering: VizWiz 2018 - Visual Question Answering benchmarking - overall<BR>","<BR>task: Image classification<BR>date: 2019-08<BR>ratio: 0.6658<BR>benchmarks:<BR>  Document Image Classification: Noisy Bangla Characters - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: Noisy Bangla Numeral - Document Image Classification benchmarking - Accuracy<BR>  Document Image Classification: n-MNIST - Document Image Classification benchmarking - Accuracy<BR>  Image Classification: Fashion-MNIST - Image Classification benchmarking - Percentage error<BR>","<BR>task: Object detection<BR>date: 2019-08<BR>ratio: 0.3188<BR>benchmarks:<BR>  Lane Detection: TuSimple - Lane Detection benchmarking - Accuracy<BR>  Object Detection: COCO minival - Object Detection benchmarking - APL<BR>  Object Detection: COCO minival - Object Detection benchmarking - APM<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>","<BR>task: Image generation<BR>date: 2019-08<BR>ratio: 0.3<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>  Image Generation: STL-10 - Image Generation benchmarking - FID<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-08<BR>ratio: 0.3552333333333333<BR>benchmarks:<BR>  Face Alignment: WFLW - Face Alignment benchmarking - AUC-at-0.1 (all)<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Unsupervised Facial Landmark Detection: 300W - Unsupervised Facial Landmark Detection benchmarking - NME<BR>  Unsupervised Facial Landmark Detection: AFLW-MTFL - Unsupervised Facial Landmark Detection benchmarking - NME<BR>","<BR>task: Semantic segmentation<BR>date: 2019-08<BR>ratio: 0.6767666666666666<BR>benchmarks:<BR>  Brain Tumor Segmentation: BRATS-2013 - Brain Tumor Segmentation benchmarking - Dice Score<BR>  Electron Microscopy Image Segmentation: SNEMI3D - Electron Microscopy Image Segmentation benchmarking - AUC<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - AUC<BR>  Lung Nodule Segmentation: LUNA - Lung Nodule Segmentation benchmarking - F1 score<BR>  Scene Segmentation: SUN-RGBD - Scene Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>","<BR>task: Emotion recognition<BR>date: 2019-08<BR>ratio: 0.3309<BR>benchmarks:<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Accuracy<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Macro-F1<BR>  Emotion Recognition in Conversation: IEMOCAP - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Activity recognition<BR>date: 2019-08<BR>ratio: 0.6697500000000001<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 5 Accuracy<BR>  Action Recognition: Something-Something V2 - Action Recognition benchmarking - Top-5 Accuracy<BR>  Egocentric Activity Recognition: EPIC-KITCHENS-55 - Egocentric Activity Recognition benchmarking - Actions Top-1 (S1)<BR>","<BR>task: Activity localization<BR>date: 2019-08<BR>ratio: 0.1982<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>","<BR>task: Semantic segmentation<BR>date: 2019-09<BR>ratio: 0.41814999999999997<BR>benchmarks:<BR>  3D Part Segmentation: ShapeNet-Part - 3D Part Segmentation benchmarking - Instance Average IoU<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - mask AP<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQth<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Semantic Segmentation: PASCAL VOC 2007 - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Object detection<BR>date: 2019-09<BR>ratio: 0.1053<BR>benchmarks:<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP50<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - AP75<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APL<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APM<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - APS<BR>  Object Detection: COCO test-dev - Object Detection benchmarking - box AP<BR>","<BR>task: Activity recognition<BR>date: 2019-09<BR>ratio: 0.2029<BR>benchmarks:<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CS)<BR>  Skeleton Based Action Recognition: PKU-MMD - Skeleton Based Action Recognition benchmarking - mAP-at-0.50 (CV)<BR>","<BR>task: Object tracking<BR>date: 2019-09<BR>ratio: 0.0378<BR>benchmarks:<BR>  Multiple Object Tracking: KITTI Tracking test - Multiple Object Tracking benchmarking - MOTA<BR>","<BR>task: Emotion recognition<BR>date: 2019-09<BR>ratio: 0.046<BR>benchmarks:<BR>  Emotion Recognition in Conversation: MELD - Emotion Recognition in Conversation benchmarking - Weighted-F1<BR>","<BR>task: Pose estimation<BR>date: 2019-09<BR>ratio: 0.46025000000000005<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>  6D Pose Estimation using RGB: LineMOD - 6D Pose Estimation using RGB benchmarking - Accuracy<BR>","<BR>task: Other image process<BR>date: 2019-09<BR>ratio: 0.1412<BR>benchmarks:<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-10<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-1<BR>  Image Retrieval: Flickr30K 1K test - Image Retrieval benchmarking - R-at-5<BR>","<BR>task: Action localization<BR>date: 2019-09<BR>ratio: 0.26<BR>benchmarks:<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.1<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.2<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.3<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.4<BR>  Temporal Action Localization: THUMOS\u201914 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>","<BR>task: Other vision process<BR>date: 2019-09<BR>ratio: 0.3091<BR>benchmarks:<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: USPS-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Visual Question Answering: VQA-CP - Visual Question Answering benchmarking - Score<BR>","<BR>task: Object detection<BR>date: 2019-10<BR>ratio: 0.12633333333333333<BR>benchmarks:<BR>  Birds Eye View Object Detection: KITTI Cars Easy - Birds Eye View Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Easy - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Hard - Object Detection benchmarking - AP<BR>  Object Detection: KITTI Cars Moderate - Object Detection benchmarking - AP<BR>  Weakly Supervised Object Detection: PASCAL VOC 2012 test - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Object recognition<BR>date: 2019-10<BR>ratio: 1.0<BR>benchmarks:<BR>  Pedestrian Attribute Recognition: PA-100K - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: PETA - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>  Pedestrian Attribute Recognition: RAP - Pedestrian Attribute Recognition benchmarking - Accuracy<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-10<BR>ratio: 0.5483<BR>benchmarks:<BR>  Face Verification: AgeDB-30 - Face Verification benchmarking - Accuracy<BR>  Face Verification: CFP-FP - Face Verification benchmarking - Accuracy<BR>  Face Verification: Labeled Faces in the Wild - Face Verification benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2019-10<BR>ratio: 0.0205<BR>benchmarks:<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP75<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AP<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APL<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - APM<BR>  Pose Estimation: COCO test-dev - Pose Estimation benchmarking - AR<BR>","<BR>task: Image classification<BR>date: 2019-10<BR>ratio: 0.1955<BR>benchmarks:<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>","<BR>task: Other vision process<BR>date: 2019-10<BR>ratio: 0.0387<BR>benchmarks:<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: ICDAR 2015 - Scene Text Detection benchmarking - Recall<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: Total-Text - Scene Text Detection benchmarking - Recall<BR>","<BR>task: Other image process<BR>date: 2019-10<BR>ratio: 0.9715<BR>benchmarks:<BR>  Grayscale Image Denoising: BSD200 sigma10 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma30 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma50 - Grayscale Image Denoising benchmarking - PSNR<BR>  Grayscale Image Denoising: BSD200 sigma70 - Grayscale Image Denoising benchmarking - PSNR<BR>","<BR>task: Semantic segmentation<BR>date: 2019-10<BR>ratio: 0.3902<BR>benchmarks:<BR>  Human Part Segmentation: CIHP - Human Part Segmentation benchmarking - Mean IoU<BR>  Human Part Segmentation: PASCAL-Part - Human Part Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: LIP val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: SkyScapes-Dense - Semantic Segmentation benchmarking - Mean IoU<BR>  Semantic Segmentation: SkyScapes-Lane - Semantic Segmentation benchmarking - Mean IoU<BR>","<BR>task: Semantic segmentation<BR>date: 2019-11<BR>ratio: 0.39412500000000006<BR>benchmarks:<BR>  Instance Segmentation: COCO minival - Instance Segmentation benchmarking - mask AP<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP50<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - AP75<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APM<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APS<BR>  Instance Segmentation: Cityscapes test - Instance Segmentation benchmarking - Average Precision<BR>  Panoptic Segmentation: COCO test-dev - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes test - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - PQ<BR>  Panoptic Segmentation: Cityscapes val - Panoptic Segmentation benchmarking - mIoU<BR>  Panoptic Segmentation: Mapillary val - Panoptic Segmentation benchmarking - PQ<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APM<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - APS<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Test Score<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: Cityscapes test - Semantic Segmentation benchmarking - Mean IoU (class)<BR>  Semantic Segmentation: Cityscapes val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: S3DIS - Semantic Segmentation benchmarking - mAcc<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: Semantic3D - Semantic Segmentation benchmarking - oAcc<BR>","<BR>task: Facial recognition and modelling<BR>date: 2019-11<BR>ratio: 1.0<BR>benchmarks:<BR>  Facial Expression Recognition: Oulu-CASIA - Facial Expression Recognition benchmarking - Accuracy (10-fold)<BR>","<BR>task: Activity recognition<BR>date: 2019-11<BR>ratio: 0.0114<BR>benchmarks:<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Activity localization<BR>date: 2019-11<BR>ratio: 0.3356<BR>benchmarks:<BR>  Weakly Supervised Action Localization: ActivityNet-1.2 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: ActivityNet-1.3 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP-at-0.5<BR>  Weakly Supervised Action Localization: THUMOS 2014 - Weakly Supervised Action Localization benchmarking - mAP@0.1:0.7<BR>","<BR>task: Image generation<BR>date: 2019-11<BR>ratio: 0.057<BR>benchmarks:<BR>  Image Generation: CIFAR-10 - Image Generation benchmarking - FID<BR>","<BR>task: Image classification<BR>date: 2019-11<BR>ratio: 0.7974<BR>benchmarks:<BR>  Satellite Image Classification: SAT-4 - Satellite Image Classification benchmarking - Accuracy<BR>  Satellite Image Classification: SAT-6 - Satellite Image Classification benchmarking - Accuracy<BR>","<BR>task: Action localization<BR>date: 2019-11<BR>ratio: 0.0838<BR>benchmarks:<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.5<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP IOU-at-0.95<BR>  Temporal Action Localization: ActivityNet-1.3 - Temporal Action Localization benchmarking - mAP<BR>","<BR>task: Other vision process<BR>date: 2019-11<BR>ratio: 0.48816666666666664<BR>benchmarks:<BR>  Domain Adaptation: ImageCLEF-DA - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: MNIST-to-USPS - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: Office-Caltech - Domain Adaptation benchmarking - Average Accuracy<BR>  Domain Adaptation: Office-Home - Domain Adaptation benchmarking - Accuracy<BR>  Domain Adaptation: SVHN-to-MNIST - Domain Adaptation benchmarking - Accuracy<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - F-Measure<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Precision<BR>  Scene Text Detection: MSRA-TD500 - Scene Text Detection benchmarking - Recall<BR>  Unsupervised Domain Adaptation: PreSIL to KITTI - Unsupervised Domain Adaptation benchmarking - AP-at-0.7<BR>","<BR>task: Object detection<BR>date: 2019-11<BR>ratio: 0.045899999999999996<BR>benchmarks:<BR>  Object Detection: COCO minival - Object Detection benchmarking - APS<BR>  Weakly Supervised Object Detection: PASCAL VOC 2007 - Weakly Supervised Object Detection benchmarking - MAP<BR>","<BR>task: Pose estimation<BR>date: 2019-11<BR>ratio: 0.9097999999999999<BR>benchmarks:<BR>  6D Pose Estimation using RGBD: LineMOD - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD-S<BR>  6D Pose Estimation using RGBD: YCB-Video - 6D Pose Estimation using RGBD benchmarking - Mean ADD<BR>  6D Pose Estimation: LineMOD - 6D Pose Estimation benchmarking - Accuracy (ADD)<BR>  6D Pose Estimation: YCB-Video - 6D Pose Estimation benchmarking - ADDS AUC<BR>","<BR>task: Semantic segmenation<BR>date: 2019-12<BR>ratio: 0.1413<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>","<BR>task: Other video process<BR>date: 2019-12<BR>ratio: 0.7122<BR>benchmarks:<BR>  Video Generation: UCF-101 16 frames, Unconditional, Single GPU - Video Generation benchmarking - Inception Score<BR>","<BR>task: Image generation<BR>date: 2019-12<BR>ratio: 0.8135<BR>benchmarks:<BR>  Image Generation: CIFAR-100 - Image Generation benchmarking - FID<BR>  Image Generation: FFHQ - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Cat 256 x 256 - Image Generation benchmarking - FID<BR>  Image Generation: LSUN Churches 256 x 256 - Image Generation benchmarking - FID<BR>","<BR>task: Semantic segmentation<BR>date: 2019-12<BR>ratio: 0.172<BR>benchmarks:<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - AUC<BR>  Retinal Vessel Segmentation: CHASE_DB1 - Retinal Vessel Segmentation benchmarking - F1 score<BR>  Retinal Vessel Segmentation: DRIVE - Retinal Vessel Segmentation benchmarking - AUC<BR>","<BR>task: Image classification<BR>date: 2019-12<BR>ratio: 0.2424<BR>benchmarks:<BR>  Image Classification: CIFAR-10 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: CIFAR-100 - Image Classification benchmarking - Percentage correct<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 1 Accuracy<BR>  Image Classification: ImageNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Accuracy<BR>  Image Classification: ImageNet ReaL - Image Classification benchmarking - Params<BR>  Image Classification: ObjectNet - Image Classification benchmarking - Top 5 Accuracy<BR>  Image Classification: VTAB-1k - Image Classification benchmarking - Top-1 Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2019-12<BR>ratio: 0.0932<BR>benchmarks:<BR>  3D Human Pose Estimation: 3DPW - 3D Human Pose Estimation benchmarking - MPJPE<BR>","<BR>task: Activity recognition<BR>date: 2019-12<BR>ratio: 0.1018<BR>benchmarks:<BR>  Action Recognition: Something-Something V1 - Action Recognition benchmarking - Top 1 Accuracy<BR>  Skeleton Based Action Recognition: Kinetics-Skeleton dataset - Skeleton Based Action Recognition benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2020-01<BR>ratio: 0.13385<BR>benchmarks:<BR>  Instance Segmentation: COCO test-dev - Instance Segmentation benchmarking - APL<BR>  Real-time Instance Segmentation: MSCOCO - Real-time Instance Segmentation benchmarking - mask AP<BR>","<BR>task: Pose estimation<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  Pose Estimation: UPenn Action - Pose Estimation benchmarking - Mean PCK-at-0.2<BR>","<BR>task: Object detection<BR>date: 2020-01<BR>ratio: 1.0<BR>benchmarks:<BR>  3D Object Detection: SUN-RGBD - 3D Object Detection benchmarking - mAP-at-0.25<BR>","<BR>task: Other vision process<BR>date: 2020-01<BR>ratio: 0.4159<BR>benchmarks:<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Duke to Market - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to Duke - Unsupervised Domain Adaptation benchmarking - rank-5<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - mAP<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-10<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-1<BR>  Unsupervised Domain Adaptation: Market to MSMT - Unsupervised Domain Adaptation benchmarking - rank-5<BR>","<BR>task: Image classification<BR>date: 2020-01<BR>ratio: 0.25<BR>benchmarks:<BR>  Image Classification: MNIST - Image Classification benchmarking - Accuracy<BR>","<BR>task: Pose estimation<BR>date: 2020-02<BR>ratio: 0.0125<BR>benchmarks:<BR>  3D Human Pose Estimation: MPI-INF-3DHP - 3D Human Pose Estimation benchmarking - AUC<BR>  Pose Estimation: Leeds Sports Poses - Pose Estimation benchmarking - PCK<BR>  Pose Estimation: MPII Human Pose - Pose Estimation benchmarking - PCKh-0.5<BR>","<BR>task: Activity recognition<BR>date: 2020-02<BR>ratio: 0.4211<BR>benchmarks:<BR>  Egocentric Activity Recognition: EGTEA - Egocentric Activity Recognition benchmarking - Average Accuracy<BR>","<BR>task: Other vision process<BR>date: 2020-02<BR>ratio: 0.35829999999999995<BR>benchmarks:<BR>  Scene Graph Generation: Visual Genome - Scene Graph Generation benchmarking - Recall-at-50<BR>  Visual Question Answering: MSRVTT-QA - Visual Question Answering benchmarking - Accuracy<BR>  Visual Question Answering: MSVD-QA - Visual Question Answering benchmarking - Accuracy<BR>","<BR>task: Semantic segmentation<BR>date: 2020-03<BR>ratio: 0.5401<BR>benchmarks:<BR>  3D Semantic Instance Segmentation: ScanNetV2 - 3D Semantic Instance Segmentation benchmarking - mAP-at-0.50<BR>  Lesion Segmentation: ISIC 2018 - Lesion Segmentation benchmarking - Dice Score<BR>","<BR>task: Semantic segmenation<BR>date: 2020-03<BR>ratio: 0.8206<BR>benchmarks:<BR>  3D Instance Segmentation: S3DIS - 3D Instance Segmentation benchmarking - mPrec<BR>  3D Instance Segmentation: SceneNN - 3D Instance Segmentation benchmarking - mAP-at-0.5<BR>","<BR>task: Object detection<BR>date: 2020-03<BR>ratio: 0.2605<BR>benchmarks:<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.25<BR>  3D Object Detection: ScanNetV2 - 3D Object Detection benchmarking - mAP-at-0.5<BR>  Video Object Detection: ImageNet VID - Video Object Detection benchmarking - MAP<BR>","<BR>task: Other video process<BR>date: 2020-03<BR>ratio: 0.8021<BR>benchmarks:<BR>  Video Frame Interpolation: UCF101 - Video Frame Interpolation benchmarking - PSNR<BR>  Video Frame Interpolation: Vimeo90k - Video Frame Interpolation benchmarking - PSNR<BR>","<BR>task: Action localization<BR>date: 2020-03<BR>ratio: 0.6966<BR>benchmarks:<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: 50 Salads - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: Breakfast - Action Segmentation benchmarking - F1@50%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Acc<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - Edit<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@10%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@25%<BR>  Action Segmentation: GTEA - Action Segmentation benchmarking - F1@50%<BR>","<BR>task: Pose estimation<BR>date: 2020-04<BR>ratio: 0.4396<BR>benchmarks:<BR>  3D Human Pose Estimation: Surreal - 3D Human Pose Estimation benchmarking - MPJPE<BR>","<BR>task: Semantic segmentation<BR>date: 2020-04<BR>ratio: 0.22005000000000002<BR>benchmarks:<BR>  Semantic Segmentation: ADE20K - Semantic Segmentation benchmarking - Validation mIoU<BR>  Semantic Segmentation: ADE20K val - Semantic Segmentation benchmarking - mIoU<BR>  Semantic Segmentation: NYU Depth v2 - Semantic Segmentation benchmarking - Mean IoU<BR>  Video Semantic Segmentation: Cityscapes val - Video Semantic Segmentation benchmarking - mIoU<BR>"],"marker":{"color":[0.5546,0.0189,0.1073,1.0,1.0,0.0397,0.0553,0.021,0.289,0.8122,0.4737,0.1446,0.9752,0.2834,0.2375,0.2513,0.2406,1.0,0.1392,0.0877,0.0451,0.6408,0.0134,0.3753,0.0741,0.2327,0.2878,0.1007,0.426,0.24846666666666664,0.8426,0.1675,0.4459,0.2457,0.4321,0.5,0.17486666666666664,0.1694,0.0331,0.3169,0.4339,0.58925,1.0,0.7477,0.5016,0.47796666666666665,0.6524,1.0,0.0478,0.0707,0.056,0.40365,0.2255,0.1471,0.2757,0.3359,0.7322,1.0,0.5947,0.5532666666666667,0.21814999999999998,0.1069,0.1741,0.0418,0.2999,0.46724999999999994,0.0741,0.061,0.31905,0.0241,0.2999,1.0,0.0251,0.6356666666666667,0.36235,0.1233,0.4643,0.0144,0.0093,0.3082,0.2784,0.4115,0.0142,0.0606,0.26530000000000004,0.3399,0.3954,0.674,0.2492,0.2441,0.27403333333333335,0.3571,0.096,0.0684,0.1288,0.1001,0.77015,0.1429,0.1022,0.07565,0.5639000000000001,0.33213333333333334,0.16765000000000002,0.6995,0.9524,0.0541,0.0692,0.1565,0.1096,0.44627500000000003,0.39885,0.1193,0.3612,0.5645,0.2235,0.13935,0.2205,0.20765,0.51655,0.1476,0.4855,0.2189333333333333,0.5854250000000001,0.103,0.8514,0.32236,0.15055000000000002,0.2366,0.8352,0.2803,0.2561,0.5805666666666667,0.53035,0.7104,0.2514,0.6175,0.4844,0.2771333333333333,0.37770000000000004,0.6641,0.4898,0.6277,0.2633333333333333,0.6997,0.8307,0.0363,0.2484,0.3431666666666667,0.219,0.5744,0.3489,0.17363333333333333,0.50415,0.4921,0.5913,0.3414,0.043050000000000005,1.0,0.8514,1.0,0.0551,0.0809,0.5759333333333333,0.5297,0.9976,0.9847,0.29335,0.7097,0.6429,0.23095,0.656,0.5154000000000001,0.3768,0.2407,0.40068333333333334,0.1634,0.44705,0.789,0.435,0.5028,0.5,0.613,0.2406,0.046,0.0247,1.0,0.4283,0.1885,0.1898,0.0083,0.0439,0.2233,0.5359,0.3059,0.0583,0.44029999999999997,0.3197,0.28373333333333334,0.3996,1.0,0.2155,0.7714000000000001,0.628,0.4785333333333333,0.9695,0.3205,0.1678,0.1698,0.46340000000000003,0.1918,0.1099,0.62775,0.49684999999999996,0.1202,0.3293,0.1184,0.1351,0.24766666666666667,0.387,0.6147,0.3826,0.211,0.3237,0.27253333333333335,0.0067,0.6517,1.0,0.21509999999999999,0.5245,0.6801,0.3804,0.6039,0.2865,0.028,0.23425,0.857,0.2306,0.6543,0.145,0.6951,0.3492333333333333,0.5132666666666666,1.0,0.7435,0.9967,0.29605000000000004,0.2047,0.2077,0.1128,0.7567,0.7323200000000001,0.3894,0.3129,0.53155,0.4427,0.2614,0.7291,0.5724,0.46819999999999995,0.4436,0.9928,0.0244,0.6698,0.6807,0.3547,0.2432,0.37644999999999995,0.46865,0.1442,0.14682499999999998,0.0079,0.2609,1.0,0.2615,0.0156,0.14425,0.2577,0.3155,0.0808,0.4657,0.3892,0.1431,0.43694999999999995,0.87635,0.5661,0.40796666666666664,1.0,1.0,0.4542,0.500225,0.30485,0.27595000000000003,0.73865,0.4456333333333333,0.6789,0.30150000000000005,0.4718,0.6087,0.027,0.1675,0.16060000000000002,0.1323,0.3127,0.20259999999999997,0.0882,0.564,0.9117666666666667,0.24609999999999999,0.3141,1.0,0.0788,0.1,1.0,0.22343333333333334,0.0183,0.5371333333333334,1.0,1.0,0.46475,0.098,1.0,0.1256,0.6113999999999999,0.5759749999999999,0.2573,0.52595,1.0,0.155,0.3335,1.0,0.1081,0.500375,0.279925,0.3059,0.68665,0.6667,0.355,0.37775000000000003,0.1348,0.7854,0.0083,0.7002,0.1623,0.7067,0.7222285714285714,0.17579999999999998,0.45675000000000004,0.2823,0.1399,0.2174,0.0985,0.6079,0.5671750000000001,0.0589,0.0932,0.2305,0.1741666666666667,0.6436666666666667,0.3796,1.0,0.42693333333333333,0.4898,0.7593,0.16535,0.2415,0.835,0.0989,0.42065,0.491,0.6658,0.3188,0.3,0.3552333333333333,0.6767666666666666,0.3309,0.6697500000000001,0.1982,0.41814999999999997,0.1053,0.2029,0.0378,0.046,0.46025000000000005,0.1412,0.26,0.3091,0.12633333333333333,1.0,0.5483,0.0205,0.1955,0.0387,0.9715,0.3902,0.39412500000000006,1.0,0.0114,0.3356,0.057,0.7974,0.0838,0.48816666666666664,0.045899999999999996,0.9097999999999999,0.1413,0.7122,0.8135,0.172,0.2424,0.0932,0.1018,0.13385,1.0,1.0,0.4159,0.25,0.0125,0.4211,0.35829999999999995,0.5401,0.8206,0.2605,0.8021,0.6966,0.4396,0.22005000000000002],"colorbar":{"len":500,"lenmode":"pixels","thickness":10,"title":{"text":"ratio"}},"colorscale":[[0.0,"rgb(255,255,229)"],[0.125,"rgb(247,252,185)"],[0.25,"rgb(217,240,163)"],[0.375,"rgb(173,221,142)"],[0.5,"rgb(120,198,121)"],[0.625,"rgb(65,171,93)"],[0.75,"rgb(35,132,67)"],[0.875,"rgb(0,104,55)"],[1.0,"rgb(0,69,41)"]],"opacity":0.7,"showscale":true,"size":20,"symbol":"circle","line":{"color":"black","width":1}},"mode":"markers","x":["2012-08","2012-12","2013-02","2013-02","2013-07","2013-11","2013-12","2014-04","2014-06","2014-06","2014-06","2014-09","2014-09","2014-11","2014-12","2014-12","2014-12","2014-12","2014-12","2015-02","2015-02","2015-02","2015-02","2015-03","2015-03","2015-03","2015-04","2015-04","2015-04","2015-05","2015-05","2015-05","2015-06","2015-06","2015-08","2015-09","2015-11","2015-11","2015-11","2015-11","2015-11","2015-11","2015-12","2015-12","2015-12","2015-12","2016-01","2016-01","2016-01","2016-01","2016-02","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-03","2016-04","2016-04","2016-04","2016-05","2016-05","2016-05","2016-06","2016-06","2016-06","2016-06","2016-06","2016-07","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-08","2016-09","2016-09","2016-09","2016-09","2016-09","2016-10","2016-10","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-11","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2016-12","2017-01","2017-02","2017-02","2017-02","2017-02","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-03","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-04","2017-05","2017-05","2017-05","2017-05","2017-05","2017-05","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-06","2017-07","2017-07","2017-07","2017-07","2017-07","2017-07","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-08","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-09","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-10","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-11","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2017-12","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-01","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-02","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-03","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-04","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-05","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-06","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-07","2018-08","2018-08","2018-08","2018-08","2018-08","2018-09","2018-09","2018-09","2018-09","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-10","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-11","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2018-12","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-01","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-02","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-03","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-04","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-05","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-06","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-07","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-08","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-09","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-10","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-11","2019-12","2019-12","2019-12","2019-12","2019-12","2019-12","2019-12","2020-01","2020-01","2020-01","2020-01","2020-01","2020-02","2020-02","2020-02","2020-03","2020-03","2020-03","2020-03","2020-03","2020-04","2020-04"],"y":["Other image process","Image classification","Image classification","Activity recognition","Facial recognition and modelling","Image classification","Image classification","Image classification","Image classification","Activity recognition","Facial recognition and modelling","Image classification","Other vision process","Semantic segmentation","Activity recognition","Semantic segmentation","Facial recognition and modelling","Object detection","Image classification","Semantic segmentation","Facial recognition and modelling","Other vision process","Image classification","Activity recognition","Semantic segmentation","Facial recognition and modelling","Semantic segmentation","Other vision process","Other image process","Other vision process","Semantic segmentation","Activity recognition","Image classification","Object detection","Facial recognition and modelling","Semantic segmentation","Facial recognition and modelling","Other image process","Pose estimation","Image classification","Object detection","Semantic segmentation","Activity recognition","Other vision process","Object detection","Image classification","Pose estimation","Image generation","Action localization","Activity recognition","Image classification","Facial recognition and modelling","Image classification","Semantic segmentation","Other vision process","Object detection","Pose estimation","Activity recognition","Other vision process","Other image process","Activity recognition","Image classification","Other vision process","Semantic segmentation","Other vision process","Image generation","Semantic segmentation","Activity recognition","Object detection","Other vision process","Object detection","Object recognition","Image classification","Other vision process","Other image process","Pose estimation","Activity recognition","Object detection","Pose estimation","Activity recognition","Action localization","Other vision process","Image classification","Image generation","Semantic segmentation","Activity recognition","Other vision process","Pose estimation","Other video process","Other image process","Object detection","Action localization","Object tracking","Image classification","Image generation","Other image process","Other 3D task","Activity recognition","Other video process","Object detection","Pose estimation","Semantic segmentation","Pose estimation","Image classification","Object detection","Image generation","Pose estimation","Object detection","Action localization","Semantic segmentation","Activity recognition","Facial recognition and modelling","Other vision process","Other image process","Pose estimation","Image generation","Activity detection","Semantic segmentation","Pose estimation","Object detection","Image classification","Activity recognition","Other image process","Object tracking","Other 3D task","Other vision process","Facial recognition and modelling","Facial recognition and modelling","Gesture recognition","Activity recognition","Action localization","Other vision process","Pose estimation","Other image process","Image generation","Object tracking","Other 3D task","Semantic segmentation","Other vision process","Object detection","Facial recognition and modelling","Activity recognition","Other vision process","Facial recognition and modelling","Pose estimation","Image classification","Other video process","Object detection","Image classification","Facial recognition and modelling","Object detection","Other vision process","Pose estimation","Semantic segmentation","Activity recognition","Other vision process","Image generation","Pose estimation","Other image process","Semantic segmentation","Facial recognition and modelling","Image classification","Other image process","Image generation","Object detection","Facial recognition and modelling","Image classification","Other vision process","Object tracking","Semantic segmentation","Pose estimation","Object detection","Pose estimation","Image-to-image translation","Semantic segmentation","Other vision process","Activity recognition","Other 3D task","Facial recognition and modelling","Activity detection","Image generation","Other vision process","Activity localization","Other image process","Facial recognition and modelling","Pose estimation","Object detection","Activity recognition","Pose tracking","Image classification","Semantic segmentation","Semantic segmentation","Facial recognition and modelling","Other vision process","Other 3D task","Image classification","Other image process","Activity recognition","Other vision process","Other image process","Pose tracking","Object tracking","Other 3D task","Semantic segmentation","Activity recognition","Image generation","Image classification","Object detection","Semantic segmentation","Other vision process","Other image process","Object detection","Facial recognition and modelling","Object tracking","Image generation","Image classification","Other 3D task","Pose estimation","Activity detection","Other video process","Other vision process","Other 3D task","Other image process","Pose estimation","Object detection","Semantic segmentation","Gesture recognition","Facial recognition and modelling","Activity recognition","Pose tracking","Action localization","Other image process","Image generation","Pose estimation","Other vision process","Facial recognition and modelling","Semantic segmentation","Activity recognition","Image classification","Object detection","Activity recognition","Object detection","Object recognition","Activity localization","Pose estimation","Semantic segmentation","Other image process","Object tracking","Facial recognition and modelling","Emotion recognition","Other vision process","Action localization","Other video process","Image classification","Other image process","Other vision process","Image generation","Semantic segmentation","Object detection","Activity localization","Activity recognition","Other 3D task","Semantic segmentation","Facial recognition and modelling","Other video process","Other vision process","Image generation","Facial recognition and modelling","Other vision process","Semantic segmentation","Facial recognition and modelling","Other video process","Image classification","Semantic segmentation","Activity recognition","Other image process","Emotion recognition","Object detection","Image classification","Other vision process","Object tracking","Activity localization","Other image process","Other 3D task","Emotion recognition","Activity recognition","Image generation","Pose estimation","Semantic segmentation","Activity recognition","Facial recognition and modelling","Pose estimation","Other vision process","Semantic segmentation","Gesture recognition","Object detection","Other image process","Image generation","Other 3D task","Object tracking","Semantic segmentation","Other image process","Other vision process","Pose estimation","Object detection","Gesture recognition","Activity recognition","Other 3D task","Image classification","Image classification","Pose estimation","Semantic segmentation","Other image process","Other vision process","Pose tracking","Facial recognition and modelling","Image generation","Pose estimation","Other image process","Semantic segmentation","Semantic segmenation","Facial recognition and modelling","Action localization","Other vision process","Object detection","Image generation","Activity detection","Action localization","Semantic segmentation","Emotion recognition","Other 3D task","Object detection","Activity recognition","Image classification","Other image process","Object recognition","Facial recognition and modelling","Other vision process","Other video process","Semantic segmentation","Pose tracking","Activity localization","Image classification","Facial recognition and modelling","Other vision process","Activity recognition","Object detection","Object tracking","Pose estimation","Semantic segmenation","Action localization","Other video process","Semantic segmentation","Activity localization","Facial recognition and modelling","Image classification","Activity recognition","Other vision process","Action localization","Semantic segmentation","Object detection","Other video process","Image-to-image translation","Activity recognition","Activity localization","Object tracking","Image generation","Other vision process","Other vision process","Image classification","Object detection","Image generation","Facial recognition and modelling","Semantic segmentation","Emotion recognition","Activity recognition","Activity localization","Semantic segmentation","Object detection","Activity recognition","Object tracking","Emotion recognition","Pose estimation","Other image process","Action localization","Other vision process","Object detection","Object recognition","Facial recognition and modelling","Pose estimation","Image classification","Other vision process","Other image process","Semantic segmentation","Semantic segmentation","Facial recognition and modelling","Activity recognition","Activity localization","Image generation","Image classification","Action localization","Other vision process","Object detection","Pose estimation","Semantic segmenation","Other video process","Image generation","Semantic segmentation","Image classification","Pose estimation","Activity recognition","Semantic segmentation","Pose estimation","Object detection","Other vision process","Image classification","Pose estimation","Activity recognition","Other vision process","Semantic segmentation","Semantic segmenation","Object detection","Other video process","Action localization","Pose estimation","Semantic segmentation"],"type":"scatter","line":{"color":"black","width":0}}],                        {"template":{"data":{"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"geo":{"bgcolor":"white","lakecolor":"white","landcolor":"#E5ECF6","showlakes":true,"showland":true,"subunitcolor":"white"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"light"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","gridwidth":2,"linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white"}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"ternary":{"aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"bgcolor":"#E5ECF6","caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"title":{"x":0.05},"xaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","zerolinewidth":2}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Year"},"showgrid":true,"gridcolor":"lightBlue","tickmode":"auto"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{},"categoryorder":"array","categoryarray":["Semantic segmentation","Semantic segmenation","Pose tracking","Pose estimation","Other vision process","Other video process","Other image process","Other 3D task","Object tracking","Object recognition","Object detection","Image-to-image translation","Image generation","Image classification","Gesture recognition","Facial recognition and modelling","Emotion recognition","Activity recognition","Activity localization","Activity detection","Action localization"],"showgrid":true,"gridcolor":"lightBlue","side":"left"},"legend":{"title":{"text":"task"},"tracegroupgap":0},"margin":{"t":60},"title":{"text":"Vision process","y":0.995},"font":{"size":21},"showlegend":false,"plot_bgcolor":"white","height":1050.0,"width":1500},                        {"responsive": true}                    )                };                            </script>        </div>
</body>
</html>